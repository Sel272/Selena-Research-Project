[
  {
    "objectID": "DimensionalityReduction/dimensionalityreduction_python.html",
    "href": "DimensionalityReduction/dimensionalityreduction_python.html",
    "title": "Research Project",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n\ndf = pd.read_csv('../Data/01-modified-data/cardiovascular_numeric_final.csv')\ndf = df.drop(df.columns[0], axis=1)\ndf.head()\n\n\n\n\n\n\n\n\nid\nage\ngender\nheight\nweight\nap_hi\nap_lo\ncholesterol\ngluc\nsmoke\nalco\nactive\ncardio\nbmi\n\n\n\n\n0\n0\n50\n2\n168\n62\n110\n80\n1\n1\n0\n0\n1\n0\n21.97\n\n\n1\n1\n55\n1\n156\n85\n140\n90\n3\n1\n0\n0\n1\n1\n34.93\n\n\n2\n2\n52\n1\n165\n64\n130\n70\n3\n1\n0\n0\n0\n1\n23.51\n\n\n3\n3\n48\n2\n169\n82\n150\n100\n1\n1\n0\n0\n1\n1\n28.71\n\n\n4\n4\n48\n1\n156\n56\n100\n60\n1\n1\n0\n0\n0\n0\n23.01\n\n\n\n\n\n\n\n\nnumerical_cols = [\"age\", \"height\", \"weight\", \"ap_hi\", \"ap_lo\", \"bmi\"]\nX = df[numerical_cols]\nY = df['cardio']\nprint(X)\n\n       age  height  weight  ap_hi  ap_lo    bmi\n0       50     168      62    110     80  21.97\n1       55     156      85    140     90  34.93\n2       52     165      64    130     70  23.51\n3       48     169      82    150    100  28.71\n4       48     156      56    100     60  23.01\n...    ...     ...     ...    ...    ...    ...\n64801   54     172      70    130     90  23.66\n64802   58     165      80    150     80  29.38\n64803   53     168      76    120     80  26.93\n64804   61     163      72    135     80  27.10\n64805   56     170      72    120     80  24.91\n\n[64806 rows x 6 columns]\n\n\n\n# Standardizing the features\nscaler = StandardScaler()\ndf_std = scaler.fit_transform(X)\n\n\n# Applying PCA to the data\npca = PCA()\npca.fit(df_std)\n\nPCA()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCAPCA()\n\n\n\n# Determine the optimal number of principal components to retain\n# You can use a scree plot and look for the \"elbow\" or use the explained variance ratio\nplt.figure(figsize=(10, 5))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Principal Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.axhline(y=0.95, color='r', linestyle='--')  # for 95% variance\nplt.title('Explained Variance by Different Principal Components')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Choose the number of components such that at least 95% variance is retained\noptimal_components = np.argmax(np.cumsum(pca.explained_variance_ratio_) &gt;= 0.95) + 1\n\n# Reduce the data dimensions\npca_opt = PCA(n_components=optimal_components)\ndf_reduced = pca_opt.fit_transform(df_std)\n\n1st & 2nd\n\n# Visualize the reduced-dimensional data using PCA\nplt.figure(figsize=(10, 5))\nsns.scatterplot(x=df_reduced[:, 0], y=df_reduced[:, 1], hue=Y,color='cyan')\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component')\nplt.title('PCA - Reduced Dimensionality Data Visualization')\nplt.show()\n\n\n\n\n\n\n\n\n2nd & 3\n\n# Visualize the reduced-dimensional data using PCA\nplt.figure(figsize=(10, 5))\nsns.scatterplot(x=df_reduced[:, 1], y=df_reduced[:, 2], hue=Y,color='cyan')\nplt.xlabel('Second Principal Component')\nplt.ylabel('Third Principal Component')\nplt.title('PCA - Reduced Dimensionality Data Visualization')\nplt.show()\n\n\n\n\n\n\n\n\n3rd & 4th\n\n# Visualize the reduced-dimensional data using PCA\nplt.figure(figsize=(10, 5))\nsns.scatterplot(x=df_reduced[:, 2], y=df_reduced[:, 3], hue=Y,color='cyan')\nplt.xlabel('Third Principal Component')\nplt.ylabel('Fourth Principal Component')\nplt.title('PCA - Reduced Dimensionality Data Visualization')\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10, 5))\nsns.scatterplot(x=df_reduced[:, 0], y=df_reduced[:, 2], hue=Y,color='cyan')\nplt.xlabel('First Principal Component')\nplt.ylabel('Third Principal Component')\nplt.title('PCA - Reduced Dimensionality Data Visualization')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Analyze and interpret the results (This is just a placeholder for the analysis)\n# Example: Print the explained variance by each component\nfor i, variance in enumerate(pca_opt.explained_variance_ratio_):\n    print(f\"Principal Component {i+1}: {variance:.2%} of the variance\")\n\n# Print the optimal number of components\nprint(f\"Optimal number of components: {optimal_components}\")\n\nPrincipal Component 1: 38.71% of the variance\nPrincipal Component 2: 22.64% of the variance\nPrincipal Component 3: 19.33% of the variance\nPrincipal Component 4: 14.61% of the variance\nOptimal number of components: 4\n\n\nT-SNE\n\n\nfor i in (1,5,10,30,50):\n    X_embedded = TSNE(n_components=2, learning_rate='auto',init='random', perplexity=i).fit_transform(df_std)\n    plt.scatter(X_embedded[:,0],X_embedded[:,1],c=Y, alpha=0.5,cmap='gist_earth')\n    plt.title(f\"Perplexity={i}\")\n    plt.show()"
  },
  {
    "objectID": "Data/data.html",
    "href": "Data/data.html",
    "title": "Data",
    "section": "",
    "text": "Data is stored in the GitHub repository：https://github.com/Sel272/Selena-Research-Project 🩵\n\n\n\n(Heart Attack Cartoon) source: https://www.nhlbi.nih.gov/health-topics/education-and-awareness/heart-month/animated-gifs\n\n\n\n\n\n(Heart Stress Cartoon) source: https://www.nhlbi.nih.gov/health-topics/education-and-awareness/heart-month/animated-gifs",
    "crumbs": [
      "Data"
    ]
  },
  {
    "objectID": "ARM/arm.html",
    "href": "ARM/arm.html",
    "title": "ARM",
    "section": "",
    "text": "ARM is not being used for this project.😊"
  },
  {
    "objectID": "DataGathering/datagatheringAPI_R.html",
    "href": "DataGathering/datagatheringAPI_R.html",
    "title": "DSAN5000",
    "section": "",
    "text": "library(jsonlite)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()  masks stats::filter()\n✖ purrr::flatten() masks jsonlite::flatten()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nNYTIMES_KEY &lt;- \"Hl58Rk7ILb2Zsy2S8nEe3EaUhCiqw4tV\"\n\n\nterm &lt;- \"Heart+diseases\" # Need to use + to string together separate words\nbegin_date &lt;- \"20000101\"\nend_date &lt;- \"20201231\"\n\nbaseurl &lt;- paste0(\"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=\",term,\n                  \"&begin_date=\",begin_date,\"&end_date=\",end_date,\n                  \"&facet_filter=true&api-key=\",NYTIMES_KEY, sep=\"\")\ninitialQuery &lt;- fromJSON(baseurl)\nmaxPages &lt;- round((initialQuery$response$meta$hits[1] / 10)-1) \n\npages &lt;- list()\nfor(i in 0:9){\n  nytSearch &lt;- fromJSON(paste0(baseurl, \"&page=\", i), flatten = TRUE) %&gt;% data.frame() \n  message(\"Retrieving page \", i)\n  pages[[i+1]] &lt;- nytSearch \n  Sys.sleep(5) \n}\n\n\nallNYTSearch &lt;- rbind_pages(pages)\n\n# Visualize coverage by section\nallNYTSearch %&gt;% \n  group_by(response.docs.type_of_material) %&gt;%\n  summarize(count=n()) %&gt;%\n  mutate(percent = (count / sum(count))*100) %&gt;%\n  ggplot() +\n  geom_bar(aes(y=percent, x=response.docs.type_of_material, fill=response.docs.type_of_material), stat = \"identity\") + coord_flip() + labs(x=\"Sources\",y=\"Percentage\",title=\"Records by Sources\")+theme(legend.position = \"none\")\nallNYTSearch1 = subset(allNYTSearch, select = -c(`response.docs.multimedia`, `response.docs.keywords`,`response.docs.byline.person`))\nfile_path &lt;- \"../../data/00-raw-data/NewYorkTimesAPI.csv\"\nwrite.csv(allNYTSearch1, file = file_path, row.names = FALSE)"
  },
  {
    "objectID": "DataGathering/datagatheringAPI_python.html",
    "href": "DataGathering/datagatheringAPI_python.html",
    "title": "Lab-2.1: Assignment",
    "section": "",
    "text": "Gathering text data with an API\nIMPORTANT: The lab shown here (on the website) is just an HTML version, included for reference. To download the assignment, please navigate to the Labs tab in the Share-point dropdown menu in the website’s navigation bar. The relevant assignment can be determined from the folder’s name, click on the three dots & select download to get the assignment.\nNOTE: It is recommended that you complete this .ipynb file in VS-code.\nSubmission:\n\nExport the completed assignment to HTML or PDF (preferably with Quarto) and upload it to Canvas.\nThe final uploaded version should NOT have any code-errors present\nAll outputs must be visible in the uploaded version, including code-cell outputs, images, graphs, etc\n\n\n\n\nRead over the News-API, Wikipedia API, and Google Scholar API sections in the lab-demonstration section, if you have not done so already.\n\nhttps://jfh.georgetown.domains/dsan5000/\n\nGet an API key for the News-API: see following link\nSubmission: Insert your API key below\n\n\nAPI_KEY='9cf6d393469e41738d84303a9c840fd1'\n\n\n\n\n\nUse the provided News-API code as a starting point\nSelect THREE random topics (e.g. Georgetown, Cats, Clouds) but choose whatever you like\nQuery the API to pull text data and store the results in three different dictionaries\nExtract the title and description text and store for later processing (up to you how you do this)\nClean the text as needed\n\n\n# Import\nimport requests\nimport json\nimport re\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n#Set credentials \nbaseURL = \"https://newsapi.org/v2/everything?\"\ntotal_requests=2\nverbose=True\n\n# THIS CODE WILL NOT WORK UNLESS YOU INSERT YOUR API KEY IN THE NEXT LINE\nAPI_KEY='9cf6d393469e41738d84303a9c840fd1'\n\n\nTOPICS = ['Cardiovascular']\n\n\ndef extract (x):\n\n    URLpost = {'apiKey': API_KEY,\n                'q': '+'+ TOPICS[x],\n                'sortBy': 'relevancy',\n                'totalRequests': 1}\n\n    # print(baseURL)\n    # print(URLpost)\n\n    #GET DATA FROM API\n    response = requests.get(baseURL, URLpost) #request data from the server\n    # print(response.url);  \n    response = response.json() #extract txt data from request into json\n\n    # PRETTY PRINT\n    # https://www.digitalocean.com/community/tutorials/python-pretty-print-json\n\n    # print(json.dumps(response, indent=2))\n\n    # #GET TIMESTAMP FOR PULL REQUEST\n    from datetime import datetime\n    timestamp = datetime.now().strftime(\"%Y-%m-%d-H%H-M%M-S%S\")\n\n    # SAVE TO FILE \n    with open(timestamp+'-newapi-raw-data.json', 'w') as outfile:\n        json.dump(response, outfile, indent=4)\n        \n    return response\n\n\n\nresponse1 = extract(0)\n\n\n# Utility function\n# Function to clean strings\ndef string_cleaner(input_string):\n    try: \n        out=re.sub(r\"\"\"\n                    [,.;@#?!&$-]+  # Accept one or more copies of punctuation\n                    \\ *           # plus zero or more copies of a space,\n                    \"\"\",\n                    \" \",          # and replace it with a single space\n                    input_string, flags=re.VERBOSE)\n\n        #REPLACE SELECT CHARACTERS WITH NOTHING\n        out = re.sub('[’.]+', '', input_string)\n\n        #ELIMINATE DUPLICATE WHITESPACES USING WILDCARDS\n        out = re.sub(r'\\s+', ' ', out)\n\n        #CONVERT TO LOWER CASE\n        out=out.lower()\n    except:\n        print(\"ERROR\")\n        out=''\n    return out\n\n\ndef clean (response):\n    article_list=response['articles']   #list of dictionaries for each article\n    article_keys=article_list[0].keys()\n    # print(\"AVAILABLE KEYS:\")\n    # print(article_keys)\n    index=0\n    cleaned_data=[];  \n    for article in article_list:\n        tmp=[]\n        # if(verbose):\n        #     print(\"#------------------------------------------\")\n        #     print(\"#\",index)\n        #     print(\"#------------------------------------------\")\n\n        for key in article_keys:\n            # if(verbose):\n            #     print(\"----------------\")\n            #     print(key)\n            #     print(article[key])\n            #     print(\"----------------\")\n\n            # if(key=='source'):\n            #     src=string_cleaner(article[key]['name'])\n            #     tmp.append(src) \n\n            # if(key=='author'):\n            #     author=string_cleaner(article[key])\n            #     #ERROR CHECK (SOMETIMES AUTHOR IS SAME AS PUBLICATION)\n            #     if(src in author): \n            #         print(\" AUTHOR ERROR:\",author);author='NA'\n            #     tmp.append(author)\n\n            if(key=='title'):\n                tmp.append(string_cleaner(article[key]))\n                \n            if(key=='description'):\n                tmp.append(string_cleaner(article[key]))\n\n            # if(key=='content'):\n            #     tmp.append(string_cleaner(article[key]))\n\n            # if(key=='publishedAt'):\n            #     #DEFINE DATA PATERN FOR RE TO CHECK  .* --&gt; wildcard\n            #     ref = re.compile('.*-.*-.*T.*:.*:.*Z')\n            #     date=article[key]\n            #     if(not ref.match(date)):\n            #         print(\" DATE ERROR:\",date); date=\"NA\"\n            #     tmp.append(date)\n\n        cleaned_data.append(tmp)\n        index+=1\n\n    return cleaned_data\n\n\n# Create DataFrame\ncleaned_data1 = clean(response1)\ndf1 = pd.DataFrame(cleaned_data1)\n\n\n\n\n\ndf1['t'] = df1[0] + df1[1]\nt1=\"\"\nfor i in range(len(df1)):\n    t1 += df1['t'][i]\n\n\n\n\n\nUse the provided Wikipedia-API code as a starting point\nFor EACH THREE of the random topics, create a word cloud for your cleaned title and description text\n\n\nimport wikipedia\n\ndef generate_word_cloud(my_text):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n    # exit()\n    # Import package\n    # Define a function to plot word cloud\n    def plot_cloud(wordcloud):\n        # Set figure size\n        plt.figure(figsize=(40, 30))\n        # Display image\n        plt.imshow(wordcloud) \n        # No axis details\n        plt.axis(\"off\");\n\n    # Generate word cloud\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000, \n        random_state=1, \n        background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\n\n\ndf1.to_csv('../../data/00-raw-data/cardiorisk.csv')\n\n\ngenerate_word_cloud(t1)"
  },
  {
    "objectID": "DataGathering/datagatheringAPI_python.html#assignment-1",
    "href": "DataGathering/datagatheringAPI_python.html#assignment-1",
    "title": "Lab-2.1: Assignment",
    "section": "",
    "text": "Read over the News-API, Wikipedia API, and Google Scholar API sections in the lab-demonstration section, if you have not done so already.\n\nhttps://jfh.georgetown.domains/dsan5000/\n\nGet an API key for the News-API: see following link\nSubmission: Insert your API key below\n\n\nAPI_KEY='9cf6d393469e41738d84303a9c840fd1'"
  },
  {
    "objectID": "DataGathering/datagatheringAPI_python.html#assignment-2",
    "href": "DataGathering/datagatheringAPI_python.html#assignment-2",
    "title": "Lab-2.1: Assignment",
    "section": "",
    "text": "Use the provided News-API code as a starting point\nSelect THREE random topics (e.g. Georgetown, Cats, Clouds) but choose whatever you like\nQuery the API to pull text data and store the results in three different dictionaries\nExtract the title and description text and store for later processing (up to you how you do this)\nClean the text as needed\n\n\n# Import\nimport requests\nimport json\nimport re\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n#Set credentials \nbaseURL = \"https://newsapi.org/v2/everything?\"\ntotal_requests=2\nverbose=True\n\n# THIS CODE WILL NOT WORK UNLESS YOU INSERT YOUR API KEY IN THE NEXT LINE\nAPI_KEY='9cf6d393469e41738d84303a9c840fd1'\n\n\nTOPICS = ['Cardiovascular']\n\n\ndef extract (x):\n\n    URLpost = {'apiKey': API_KEY,\n                'q': '+'+ TOPICS[x],\n                'sortBy': 'relevancy',\n                'totalRequests': 1}\n\n    # print(baseURL)\n    # print(URLpost)\n\n    #GET DATA FROM API\n    response = requests.get(baseURL, URLpost) #request data from the server\n    # print(response.url);  \n    response = response.json() #extract txt data from request into json\n\n    # PRETTY PRINT\n    # https://www.digitalocean.com/community/tutorials/python-pretty-print-json\n\n    # print(json.dumps(response, indent=2))\n\n    # #GET TIMESTAMP FOR PULL REQUEST\n    from datetime import datetime\n    timestamp = datetime.now().strftime(\"%Y-%m-%d-H%H-M%M-S%S\")\n\n    # SAVE TO FILE \n    with open(timestamp+'-newapi-raw-data.json', 'w') as outfile:\n        json.dump(response, outfile, indent=4)\n        \n    return response\n\n\n\nresponse1 = extract(0)\n\n\n# Utility function\n# Function to clean strings\ndef string_cleaner(input_string):\n    try: \n        out=re.sub(r\"\"\"\n                    [,.;@#?!&$-]+  # Accept one or more copies of punctuation\n                    \\ *           # plus zero or more copies of a space,\n                    \"\"\",\n                    \" \",          # and replace it with a single space\n                    input_string, flags=re.VERBOSE)\n\n        #REPLACE SELECT CHARACTERS WITH NOTHING\n        out = re.sub('[’.]+', '', input_string)\n\n        #ELIMINATE DUPLICATE WHITESPACES USING WILDCARDS\n        out = re.sub(r'\\s+', ' ', out)\n\n        #CONVERT TO LOWER CASE\n        out=out.lower()\n    except:\n        print(\"ERROR\")\n        out=''\n    return out\n\n\ndef clean (response):\n    article_list=response['articles']   #list of dictionaries for each article\n    article_keys=article_list[0].keys()\n    # print(\"AVAILABLE KEYS:\")\n    # print(article_keys)\n    index=0\n    cleaned_data=[];  \n    for article in article_list:\n        tmp=[]\n        # if(verbose):\n        #     print(\"#------------------------------------------\")\n        #     print(\"#\",index)\n        #     print(\"#------------------------------------------\")\n\n        for key in article_keys:\n            # if(verbose):\n            #     print(\"----------------\")\n            #     print(key)\n            #     print(article[key])\n            #     print(\"----------------\")\n\n            # if(key=='source'):\n            #     src=string_cleaner(article[key]['name'])\n            #     tmp.append(src) \n\n            # if(key=='author'):\n            #     author=string_cleaner(article[key])\n            #     #ERROR CHECK (SOMETIMES AUTHOR IS SAME AS PUBLICATION)\n            #     if(src in author): \n            #         print(\" AUTHOR ERROR:\",author);author='NA'\n            #     tmp.append(author)\n\n            if(key=='title'):\n                tmp.append(string_cleaner(article[key]))\n                \n            if(key=='description'):\n                tmp.append(string_cleaner(article[key]))\n\n            # if(key=='content'):\n            #     tmp.append(string_cleaner(article[key]))\n\n            # if(key=='publishedAt'):\n            #     #DEFINE DATA PATERN FOR RE TO CHECK  .* --&gt; wildcard\n            #     ref = re.compile('.*-.*-.*T.*:.*:.*Z')\n            #     date=article[key]\n            #     if(not ref.match(date)):\n            #         print(\" DATE ERROR:\",date); date=\"NA\"\n            #     tmp.append(date)\n\n        cleaned_data.append(tmp)\n        index+=1\n\n    return cleaned_data\n\n\n# Create DataFrame\ncleaned_data1 = clean(response1)\ndf1 = pd.DataFrame(cleaned_data1)\n\n\n\n\n\ndf1['t'] = df1[0] + df1[1]\nt1=\"\"\nfor i in range(len(df1)):\n    t1 += df1['t'][i]"
  },
  {
    "objectID": "DataGathering/datagatheringAPI_python.html#assignment-3",
    "href": "DataGathering/datagatheringAPI_python.html#assignment-3",
    "title": "Lab-2.1: Assignment",
    "section": "",
    "text": "Use the provided Wikipedia-API code as a starting point\nFor EACH THREE of the random topics, create a word cloud for your cleaned title and description text\n\n\nimport wikipedia\n\ndef generate_word_cloud(my_text):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n    # exit()\n    # Import package\n    # Define a function to plot word cloud\n    def plot_cloud(wordcloud):\n        # Set figure size\n        plt.figure(figsize=(40, 30))\n        # Display image\n        plt.imshow(wordcloud) \n        # No axis details\n        plt.axis(\"off\");\n\n    # Generate word cloud\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000, \n        random_state=1, \n        background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\n\n\ndf1.to_csv('../../data/00-raw-data/cardiorisk.csv')\n\n\ngenerate_word_cloud(t1)"
  },
  {
    "objectID": "About/About.html",
    "href": "About/About.html",
    "title": "About Me",
    "section": "",
    "text": "Hi. My name is Xuening(Selena) Yang. I am a recent graduate of George Washington University, where I earned an bachelor degree in Business and Business Analytics with a minor in Statistics. Now, I’m starting an exciting academic journey as a master’s degree in data science and analytics at Georgetown University. My passion for data-driven decision making and desire to excel in the business world led me down this path. I have also been fortunate to broaden my horizons through the University of Edinburgh’s rich exchange programme, exposing me to different cultures and global perspectives. Outside of the classroom, I am an avid traveler, constantly seeking new adventures and cultural experiences. Traveling not only fueled my curiosity, but also enhanced my resilience and critical thinking skills, which are invaluable assets in the ever-growing field of data science and analytics. With my background and current academic pursuits, I am ready to take on the challenge and make a meaningful contribution to this dynamic and exciting field.",
    "crumbs": [
      "About Me"
    ]
  },
  {
    "objectID": "About/About.html#educations",
    "href": "About/About.html#educations",
    "title": "About Me",
    "section": "Educations",
    "text": "Educations\n\nGeorgetown University | August 2023 ~ May 2025\nUniversity of Edinburgh | January 2023 ~ May 2023\nGeorge Washington University | August 2019 ~ May 2023",
    "crumbs": [
      "About Me"
    ]
  },
  {
    "objectID": "About/About.html#professional-eexperience",
    "href": "About/About.html#professional-eexperience",
    "title": "About Me",
    "section": "Professional Eexperience",
    "text": "Professional Eexperience\n\nChoice Hotels International, Data Science Intern | North Bethesda, MD\n\n\nUtilized SQL to query an internal database, performing complex aggregations and joins to transform data from 15+ million observations at the guest level to 5k observations at the property level for analysis\nDesigned 20+ interactive data visualizations, including scatterplots, heat maps, and geospatial maps of hotel locations, using Python in AWS SageMaker to analyze variable relationships and distributions\nPerformed data wrangling and conducted Ridge and Lasso Regression model, optimizing dimensionality reduction and elevating R-squared by 0.2 while preserving feature clarity\nBuilt an Ordinary Least Squares model to quantify the relationship between hotel satisfaction results and repeat guest stay rates, leveraging the results to analyze potential ROI using key strategic levers\nPresented strategic recommendations on hotel booking channels and guest satisfaction, influencing executive decisions and driving a potential $1 million revenue increase\n\n\nMobalytics, Gaming Market Research and Data Analytics Remote Externship | California, US\n\n\nConducted in-depth competitor analysis on 1,000+ companies, creating mind maps, issue trees, and SWOT analyses to deliver strategic improvement recommendations\nTransformed raw data into actionable insights using R and Excel, through statistical analysis and regression modeling\nSynthesized historical data and market trends to present actionable business strategies and data insights to program management teams, driving strategic initiatives\n\n\nChina Galaxy Securities Co., Ltd. Beijing Branch, Investment Banking Division Intern | Beijing, China\n\n\nAnalyze listed corporations refinancing situations of Beijing’s Xicheng and Fengtai Districts, sorted out financial bonds of Xicheng District, including number of issuers and annual number of issues.\nInvestigated the status quo of steel industry and participated in its development cycle and forecasted the saturation point of steel production in China.\nAssisted in modifying the equity financing plan for base construction of Fengtai District in Beijing.",
    "crumbs": [
      "About Me"
    ]
  },
  {
    "objectID": "About/About.html#acadmeic-projects",
    "href": "About/About.html#acadmeic-projects",
    "title": "About Me",
    "section": "Acadmeic Projects",
    "text": "Acadmeic Projects\n\nData Science Portfolio Project: A Narrative Website, GU URL: https://sely27.georgetown.domains/Research_Project | Washington, DC\n\n\nDesigned and launched a comprehensive 13-tab Quarto-based website, effectively narrating a data science project on Cardiovascular Risk, with seamlessly integrated code and hosting on GitHub to enhance accessibility\nProcessed and analyzed large datasets of 60,000+ records in Python, visualizing key insights using Seaborn plots to effectively communicate outcomes\nImplemented supervised machine learning models in Python and R, including Naïve Bayes, Decision Trees, Random Forest, and selected models based on accuracy, precision and recall scores\nOptimized model performance by fine-tuning hyperparameters and addressed potential overfitting concerns using learning curves, resulting in a final test accuracy of up to 85%\n\n\nSarcasm Detection and Translation Model, Natural Language Processing Project in Python, Github: https://github.com/Sel272/Sarcasm-Detection-and-Translation-Model | Washington, DC\n\n\nDeveloped a Python-based Sarcasm Detection Model using Object-Oriented Programming (OOP), incorporating 10 functions from 3 built classes, capable of accurately identifying sarcastic text and translating it into genuine meaning\nLeveraged advanced machine learning techniques, including Logistic Regression, Naïve Bayes, LSTM, and Random Forests, to enhance sarcasm prediction accuracy to 87%\nImplemented a Sarcasm Translation Model employing NLTK package’s sentiment analysis, part-of-speech (POS) tagging, and an antonym word bank to effectively convert sarcastic text into its literal interpretation\n\n\nCapstone Project - Verituity, Graduate-Level Group Project | Washington, DC\n\n\nOptimized LinkedIn’s digital advertising effectiveness by implementing targeted A/B testing strategies\nReduced advertising fatigue by developing predictive machine learning models that forecasted creative wear-out, enabling timely content refreshes and maintaining audience engagement at key stages of the sales cycle\nEngineered a streamlined ETL protocol to enhance B2B ad performance by integrating LinkedIn ROI metrics and Google Analytics",
    "crumbs": [
      "About Me"
    ]
  },
  {
    "objectID": "About/About.html#skills",
    "href": "About/About.html#skills",
    "title": "About Me",
    "section": "Skills",
    "text": "Skills\n\nTechnical Tools: Python, R, MATLAB, SAS, SAS Viya, Tableau, Quarto, Google Analytics, MS Office, SQL, AWS SageMaker, GitHub, Bitbucket, Spark\nLanguage: Native in Mandarin, Fluent in English, Basic in French",
    "crumbs": [
      "About Me"
    ]
  },
  {
    "objectID": "DecisionTrees/randomforest_python.html",
    "href": "DecisionTrees/randomforest_python.html",
    "title": "Research Project",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom sklearn import tree\nfrom IPython.display import Image\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nimport matplotlib.pyplot as plt\n\n\ncardio = pd.read_csv('../Data/01-modified-data/cardiovascular_numeric_final.csv')\ncardio = cardio.drop(cardio.columns[0], axis=1)\ncardio.head()\n\n\n\n\n\n\n\n\nid\nage\ngender\nheight\nweight\nap_hi\nap_lo\ncholesterol\ngluc\nsmoke\nalco\nactive\ncardio\nbmi\n\n\n\n\n0\n0\n50\n2\n168\n62\n110\n80\n1\n1\n0\n0\n1\n0\n21.97\n\n\n1\n1\n55\n1\n156\n85\n140\n90\n3\n1\n0\n0\n1\n1\n34.93\n\n\n2\n2\n52\n1\n165\n64\n130\n70\n3\n1\n0\n0\n0\n1\n23.51\n\n\n3\n3\n48\n2\n169\n82\n150\n100\n1\n1\n0\n0\n1\n1\n28.71\n\n\n4\n4\n48\n1\n156\n56\n100\n60\n1\n1\n0\n0\n0\n0\n23.01\n\n\n\n\n\n\n\n\ncardio_x = cardio.drop(columns=['cardio','id'])\ncardio_y = cardio[['cardio']]\ncardio_y = np.ravel(cardio_y)\n\n\nparam_grid = {\n    'max_features': [5,8,12,15,30],\n    'max_depth': [3, 5, 8],\n    'n_estimators': [100, 200, 250]\n}\n\n\nx_train, x_test, y_train, y_test = train_test_split(cardio_x, cardio_y, test_size=0.2, random_state=12)\n\n\nrf_classifier = RandomForestClassifier()\ngrid_search = GridSearchCV(rf_classifier, param_grid, cv=4, scoring='accuracy')\ngrid_search.fit(x_train, y_train)\nresults = pd.DataFrame(grid_search.cv_results_)\nresults\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_max_depth\nparam_max_features\nparam_n_estimators\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n0\n0.787074\n0.007200\n0.035596\n0.000105\n3\n5\n100\n{'max_depth': 3, 'max_features': 5, 'n_estimat...\n0.720932\n0.722784\n0.724173\n0.718463\n0.721588\n0.002139\n45\n\n\n1\n1.545514\n0.004320\n0.069910\n0.000647\n3\n5\n200\n{'max_depth': 3, 'max_features': 5, 'n_estimat...\n0.720855\n0.723092\n0.725176\n0.718695\n0.721954\n0.002424\n41\n\n\n2\n1.936487\n0.015982\n0.086350\n0.000170\n3\n5\n250\n{'max_depth': 3, 'max_features': 5, 'n_estimat...\n0.720932\n0.722938\n0.724558\n0.719080\n0.721877\n0.002063\n43\n\n\n3\n1.064351\n0.004957\n0.035183\n0.000275\n3\n8\n100\n{'max_depth': 3, 'max_features': 8, 'n_estimat...\n0.720701\n0.723324\n0.725330\n0.718309\n0.721916\n0.002652\n42\n\n\n4\n2.128837\n0.010925\n0.070401\n0.001423\n3\n8\n200\n{'max_depth': 3, 'max_features': 8, 'n_estimat...\n0.720701\n0.723170\n0.725021\n0.717846\n0.721684\n0.002695\n44\n\n\n5\n2.672131\n0.016506\n0.085684\n0.000097\n3\n8\n250\n{'max_depth': 3, 'max_features': 8, 'n_estimat...\n0.720701\n0.725407\n0.725407\n0.717614\n0.722282\n0.003310\n40\n\n\n6\n1.451854\n0.002488\n0.033621\n0.000449\n3\n12\n100\n{'max_depth': 3, 'max_features': 12, 'n_estima...\n0.722475\n0.724327\n0.726410\n0.719620\n0.723208\n0.002496\n31\n\n\n7\n2.903286\n0.002093\n0.065900\n0.000445\n3\n12\n200\n{'max_depth': 3, 'max_features': 12, 'n_estima...\n0.722475\n0.724327\n0.726410\n0.719620\n0.723208\n0.002496\n31\n\n\n8\n3.632230\n0.007120\n0.081526\n0.000799\n3\n12\n250\n{'max_depth': 3, 'max_features': 12, 'n_estima...\n0.722475\n0.724327\n0.726410\n0.719620\n0.723208\n0.002496\n31\n\n\n9\n1.452749\n0.006278\n0.033584\n0.000312\n3\n15\n100\n{'max_depth': 3, 'max_features': 15, 'n_estima...\n0.722475\n0.724327\n0.726410\n0.719620\n0.723208\n0.002496\n31\n\n\n10\n2.906101\n0.005509\n0.065403\n0.000195\n3\n15\n200\n{'max_depth': 3, 'max_features': 15, 'n_estima...\n0.722475\n0.724327\n0.726410\n0.719620\n0.723208\n0.002496\n31\n\n\n11\n3.628169\n0.006186\n0.082443\n0.000961\n3\n15\n250\n{'max_depth': 3, 'max_features': 15, 'n_estima...\n0.722475\n0.724327\n0.726410\n0.719620\n0.723208\n0.002496\n31\n\n\n12\n1.454576\n0.001384\n0.034079\n0.000585\n3\n30\n100\n{'max_depth': 3, 'max_features': 30, 'n_estima...\n0.722475\n0.724327\n0.726410\n0.719620\n0.723208\n0.002496\n31\n\n\n13\n2.930941\n0.036049\n0.066202\n0.000771\n3\n30\n200\n{'max_depth': 3, 'max_features': 30, 'n_estima...\n0.722475\n0.724327\n0.726410\n0.719620\n0.723208\n0.002496\n31\n\n\n14\n3.653693\n0.050620\n0.081528\n0.000866\n3\n30\n250\n{'max_depth': 3, 'max_features': 30, 'n_estima...\n0.722475\n0.724327\n0.726410\n0.719620\n0.723208\n0.002496\n31\n\n\n15\n1.121718\n0.010288\n0.045798\n0.000254\n5\n5\n100\n{'max_depth': 5, 'max_features': 5, 'n_estimat...\n0.727799\n0.728416\n0.731116\n0.723710\n0.727760\n0.002651\n28\n\n\n16\n2.235073\n0.005686\n0.090693\n0.001410\n5\n5\n200\n{'max_depth': 5, 'max_features': 5, 'n_estimat...\n0.726179\n0.729033\n0.731039\n0.723555\n0.727452\n0.002836\n29\n\n\n17\n2.788049\n0.010582\n0.111738\n0.000326\n5\n5\n250\n{'max_depth': 5, 'max_features': 5, 'n_estimat...\n0.725870\n0.727490\n0.731194\n0.722629\n0.726796\n0.003084\n30\n\n\n18\n1.604124\n0.008925\n0.045737\n0.000392\n5\n8\n100\n{'max_depth': 5, 'max_features': 8, 'n_estimat...\n0.730191\n0.731965\n0.733354\n0.724635\n0.730036\n0.003314\n16\n\n\n19\n3.218563\n0.009933\n0.090066\n0.000229\n5\n8\n200\n{'max_depth': 5, 'max_features': 8, 'n_estimat...\n0.728647\n0.728956\n0.732428\n0.724018\n0.728512\n0.002989\n27\n\n\n20\n4.028593\n0.011710\n0.112411\n0.000477\n5\n8\n250\n{'max_depth': 5, 'max_features': 8, 'n_estimat...\n0.730191\n0.732197\n0.733817\n0.723555\n0.729940\n0.003904\n17\n\n\n21\n2.253983\n0.003733\n0.045479\n0.000471\n5\n12\n100\n{'max_depth': 5, 'max_features': 12, 'n_estima...\n0.728956\n0.730653\n0.733740\n0.724790\n0.729535\n0.003232\n22\n\n\n22\n4.501898\n0.004020\n0.089598\n0.000829\n5\n12\n200\n{'max_depth': 5, 'max_features': 12, 'n_estima...\n0.728802\n0.730422\n0.733585\n0.725176\n0.729496\n0.003030\n24\n\n\n23\n5.626368\n0.010844\n0.111776\n0.000983\n5\n12\n250\n{'max_depth': 5, 'max_features': 12, 'n_estima...\n0.728647\n0.730422\n0.733200\n0.725253\n0.729380\n0.002883\n25\n\n\n24\n2.278674\n0.044276\n0.045675\n0.000451\n5\n15\n100\n{'max_depth': 5, 'max_features': 15, 'n_estima...\n0.728570\n0.730036\n0.733817\n0.724250\n0.729168\n0.003425\n26\n\n\n25\n12.831333\n14.452285\n0.089542\n0.000667\n5\n15\n200\n{'max_depth': 5, 'max_features': 15, 'n_estima...\n0.728802\n0.730808\n0.734357\n0.725098\n0.729766\n0.003350\n19\n\n\n26\n5.616060\n0.017365\n0.112423\n0.001731\n5\n15\n250\n{'max_depth': 5, 'max_features': 15, 'n_estima...\n0.728879\n0.730885\n0.733508\n0.725098\n0.729593\n0.003070\n21\n\n\n27\n2.255050\n0.004228\n0.045844\n0.000354\n5\n30\n100\n{'max_depth': 5, 'max_features': 30, 'n_estima...\n0.728647\n0.730653\n0.734280\n0.724481\n0.729515\n0.003539\n23\n\n\n28\n4.509898\n0.007295\n0.089927\n0.001001\n5\n30\n200\n{'max_depth': 5, 'max_features': 30, 'n_estima...\n0.728879\n0.730885\n0.734048\n0.725407\n0.729805\n0.003137\n18\n\n\n29\n5.641994\n0.006853\n0.112222\n0.001534\n5\n30\n250\n{'max_depth': 5, 'max_features': 30, 'n_estima...\n0.728647\n0.731116\n0.733894\n0.725407\n0.729766\n0.003127\n19\n\n\n30\n1.646228\n0.007944\n0.063218\n0.000388\n8\n5\n100\n{'max_depth': 8, 'max_features': 5, 'n_estimat...\n0.732274\n0.731811\n0.735437\n0.726410\n0.731483\n0.003244\n4\n\n\n31\n3.286398\n0.008687\n0.125364\n0.000620\n8\n5\n200\n{'max_depth': 8, 'max_features': 5, 'n_estimat...\n0.731194\n0.732274\n0.737134\n0.726487\n0.731772\n0.003784\n1\n\n\n32\n4.106233\n0.009053\n0.156807\n0.000953\n8\n5\n250\n{'max_depth': 8, 'max_features': 5, 'n_estimat...\n0.730885\n0.732351\n0.736517\n0.726641\n0.731599\n0.003530\n3\n\n\n33\n2.451331\n0.042319\n0.063466\n0.000163\n8\n8\n100\n{'max_depth': 8, 'max_features': 8, 'n_estimat...\n0.729882\n0.732891\n0.736517\n0.725638\n0.731232\n0.003994\n6\n\n\n34\n123.336405\n205.432968\n0.121017\n0.000985\n8\n8\n200\n{'max_depth': 8, 'max_features': 8, 'n_estimat...\n0.730268\n0.731348\n0.735514\n0.725870\n0.730750\n0.003432\n10\n\n\n35\n5.889385\n0.009550\n0.154141\n0.001128\n8\n8\n250\n{'max_depth': 8, 'max_features': 8, 'n_estimat...\n0.730731\n0.733817\n0.736209\n0.725793\n0.731637\n0.003893\n2\n\n\n36\n3.377931\n0.003331\n0.061273\n0.000684\n8\n12\n100\n{'max_depth': 8, 'max_features': 12, 'n_estima...\n0.728725\n0.731734\n0.736209\n0.724095\n0.730191\n0.004413\n15\n\n\n37\n6.813731\n0.021899\n0.122595\n0.001634\n8\n12\n200\n{'max_depth': 8, 'max_features': 12, 'n_estima...\n0.727567\n0.732351\n0.735591\n0.726256\n0.730441\n0.003740\n13\n\n\n38\n8.585203\n0.019440\n0.152713\n0.001746\n8\n12\n250\n{'max_depth': 8, 'max_features': 12, 'n_estima...\n0.728725\n0.733045\n0.736363\n0.724173\n0.730576\n0.004583\n11\n\n\n39\n3.464429\n0.049286\n0.061888\n0.000320\n8\n15\n100\n{'max_depth': 8, 'max_features': 15, 'n_estima...\n0.728493\n0.733045\n0.736286\n0.725330\n0.730789\n0.004195\n9\n\n\n40\n27.938821\n36.788776\n0.120146\n0.001259\n8\n15\n200\n{'max_depth': 8, 'max_features': 15, 'n_estima...\n0.728262\n0.732737\n0.736672\n0.725638\n0.730827\n0.004222\n8\n\n\n41\n8.495591\n0.045663\n0.151279\n0.001280\n8\n15\n250\n{'max_depth': 8, 'max_features': 15, 'n_estima...\n0.728879\n0.732660\n0.736363\n0.725947\n0.730962\n0.003922\n7\n\n\n42\n3.430128\n0.003680\n0.061662\n0.000515\n8\n30\n100\n{'max_depth': 8, 'max_features': 30, 'n_estima...\n0.727722\n0.732582\n0.735900\n0.725098\n0.730326\n0.004191\n14\n\n\n43\n6.864778\n0.008388\n0.121984\n0.000874\n8\n30\n200\n{'max_depth': 8, 'max_features': 30, 'n_estima...\n0.729033\n0.733971\n0.736903\n0.725947\n0.731464\n0.004249\n5\n\n\n44\n8.625078\n0.051382\n0.152690\n0.001218\n8\n30\n250\n{'max_depth': 8, 'max_features': 30, 'n_estima...\n0.727876\n0.733817\n0.735900\n0.724635\n0.730557\n0.004512\n12\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\n\n# Plotting the data\nax.plot(results['param_max_depth'], results['mean_test_score'])  # Line plot\n\n# Adding title and labels\nax.set_title('max_depth random forest')\nax.set_xlabel('max_depth')\nax.set_ylabel('accuracy')\n\nText(0, 0.5, 'accuracy')\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\n\n# Plotting the data\nax.plot(results['param_max_features'], results['mean_test_score'])  # Line plot\n\n# Adding title and labels\nax.set_title('max_features random forest')\nax.set_xlabel('max_features')\nax.set_ylabel('accuracy')\n\nText(0, 0.5, 'accuracy')\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\n\n# Plotting the data\nax.plot(results['param_n_estimators'], results['mean_test_score'])  # Line plot\n\n# Adding title and labels\nax.set_title('n_estimators random forest')\nax.set_xlabel('n_estimators')\nax.set_ylabel('accuracy')\n\nText(0, 0.5, 'accuracy')\n\n\n\n\n\n\n\n\n\n\nbest_params = grid_search.best_params_\nbest_params\n\n{'max_depth': 8, 'max_features': 5, 'n_estimators': 200}\n\n\n\nbest_rf_model = RandomForestClassifier(**best_params)\nbest_rf_model.fit(x_train, y_train)\n\nfeature_importances = best_rf_model.feature_importances_\n\n\nplt.figure(figsize=(8, 6))\nplt.barh(x_train.columns, feature_importances)\nplt.xlabel('Feature Importance')\nplt.title('Variable Importance')\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn import metrics\ndef confusion_plot(y_data,y_pred):\n    accuracy = metrics.accuracy_score(y_data, y_pred)\n    print('ACCURACY:',accuracy)\n    precision_0 = metrics.precision_score(y_data, y_pred,pos_label=0)\n    recall_0 = metrics.recall_score(y_data, y_pred,pos_label=0)\n    print('POSITIVE RECALL (Y=0):',recall_0)\n    print('POSITIVE PRECISION (Y=0):',precision_0)\n    precision_1 = metrics.precision_score(y_data, y_pred,pos_label=1)\n    recall_1 = metrics.recall_score(y_data, y_pred,pos_label=1)\n    print('POSITIVE RECALL (Y=1):',recall_1)\n    print('POSITIVE PRECISION (Y=1):',precision_1)\n    cm = metrics.confusion_matrix(y_data, y_pred, labels=[0,1])\n    print(cm)\n    disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=[0,1])\n    disp.plot()\n    plt.show()\n\n\nyp_train=best_rf_model.predict(x_train)\nyp_test=best_rf_model.predict(x_test)\n\n\nprint(\"------TRAINING------\")\nconfusion_plot(y_train,yp_train)\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)\n\n------TRAINING------\nACCURACY: 0.7427667618239333\nPOSITIVE RECALL (Y=0): 0.8115305932676213\nPOSITIVE PRECISION (Y=0): 0.7251636919599828\nPOSITIVE RECALL (Y=1): 0.6686966625265436\nPOSITIVE PRECISION (Y=1): 0.7671094360435722\n[[21818  5067]\n [ 8269 16690]]\n------TEST------\nACCURACY: 0.7316000617188706\nPOSITIVE RECALL (Y=0): 0.8058876270173342\nPOSITIVE PRECISION (Y=0): 0.7121352172190677\nPOSITIVE RECALL (Y=1): 0.6523125996810207\nPOSITIVE PRECISION (Y=1): 0.7589534236407497\n[[5393 1299]\n [2180 4090]]"
  },
  {
    "objectID": "Clustering/clustering_python.html",
    "href": "Clustering/clustering_python.html",
    "title": "Research Project",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndata=pd.read_csv('../Data/01-modified-data/cardiovascular_numeric_final.csv')\ndata = data.drop(data.columns[0], axis=1)\ndata.head()\n\n\n\n\n\n\n\n\nid\nage\ngender\nheight\nweight\nap_hi\nap_lo\ncholesterol\ngluc\nsmoke\nalco\nactive\ncardio\nbmi\n\n\n\n\n0\n0\n50\n2\n168\n62\n110\n80\n1\n1\n0\n0\n1\n0\n21.97\n\n\n1\n1\n55\n1\n156\n85\n140\n90\n3\n1\n0\n0\n1\n1\n34.93\n\n\n2\n2\n52\n1\n165\n64\n130\n70\n3\n1\n0\n0\n0\n1\n23.51\n\n\n3\n3\n48\n2\n169\n82\n150\n100\n1\n1\n0\n0\n1\n1\n28.71\n\n\n4\n4\n48\n1\n156\n56\n100\n60\n1\n1\n0\n0\n0\n0\n23.01\n\n\n\n\n\n\n\n\nnumerical_cols = [\"age\", \"height\", \"weight\", \"ap_hi\", \"ap_lo\", \"bmi\"]\nX = data[numerical_cols]\nY = data['cardio']\nprint(X)\n\n       age  height  weight  ap_hi  ap_lo    bmi\n0       50     168      62    110     80  21.97\n1       55     156      85    140     90  34.93\n2       52     165      64    130     70  23.51\n3       48     169      82    150    100  28.71\n4       48     156      56    100     60  23.01\n...    ...     ...     ...    ...    ...    ...\n64801   54     172      70    130     90  23.66\n64802   58     165      80    150     80  29.38\n64803   53     168      76    120     80  26.93\n64804   61     163      72    135     80  27.10\n64805   56     170      72    120     80  24.91\n\n[64806 rows x 6 columns]\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Normalize the entire dataset\nscaler = StandardScaler()\nX_normalized = scaler.fit_transform(X)\n\n\n# K-means\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, MeanShift, Birch, DBSCAN\nfrom scipy.spatial.distance import cdist\n\n\nclusters_range = range(1, 11)\ninertia = []\ndistortion = []\n\nfor k in clusters_range:\n    kmeans = KMeans(n_clusters=k, random_state=0).fit(X_normalized)\n    inertia.append(kmeans.inertia_)\n    distortion.append(sum(np.min(cdist(X_normalized, kmeans.cluster_centers_, 'euclidean'), axis=1)) / X_normalized.shape[0])\n\nresults_df = pd.DataFrame({'Clusters': clusters_range, 'Distoration': distortion, 'Inertia': inertia})\nresults_df\n\n/Users/xueningyang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/xueningyang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/xueningyang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/xueningyang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/xueningyang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/xueningyang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/xueningyang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/xueningyang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/xueningyang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/xueningyang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\n\n\n\n\nClusters\nDistoration\nInertia\n\n\n\n\n0\n1\n2.292838\n388836.000000\n\n\n1\n2\n1.993028\n291296.924742\n\n\n2\n3\n1.857978\n252154.144591\n\n\n3\n4\n1.748951\n223794.658809\n\n\n4\n5\n1.654102\n202055.604497\n\n\n5\n6\n1.591076\n186197.578470\n\n\n6\n7\n1.530241\n173906.898109\n\n\n7\n8\n1.483570\n163483.569781\n\n\n8\n9\n1.450806\n156119.165658\n\n\n9\n10\n1.419384\n149598.773806\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5))\n\n# Distortion plot\nax[1].plot(clusters_range, distortion, 'bo-')\nax[1].set_title('Elbow Method For Optimal Cluster (Distortion)')\nax[1].set_xlabel('Number of Clusters')\nax[1].set_ylabel('Distortion')\nax[1].grid(True)\n\n# Inertia plot\nax[0].plot(clusters_range, inertia, 'bo-')\nax[0].set_title('Elbow Method For Optimal Cluster (Inertia)')\nax[0].set_xlabel('Number of Clusters')\nax[0].set_ylabel('Inertia')\nax[0].grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# DBSCAN\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import silhouette_score\n\n\neps_range = (0.1, 0.2,0.3,0.4,0.5)\nmin_samples_range = (100,1000,5000)\n\nbest_eps = 0\nbest_min_samples = 0\nbest_silhouette = -1\nall_scores = []\n\nfor eps in eps_range:\n    for min_samples in min_samples_range:\n        dbscan = DBSCAN(eps=eps, min_samples=min_samples).fit(X_normalized)\n        labels = dbscan.labels_\n        \n        # Ignore noise samples (-1 label)\n        if len(set(labels)) &gt; 1 and -1 in labels:\n            curr_silhouette = silhouette_score(X_normalized, labels)\n            cluster_nums = len(set(labels)) - (1 if -1 in labels else 0)\n            all_scores.append((eps, min_samples, curr_silhouette,cluster_nums))\n            \n            if curr_silhouette &gt; best_silhouette:\n                best_eps, best_min_samples, best_silhouette, cluster_num = eps, min_samples, curr_silhouette, cluster_nums\n\nresults_df = pd.DataFrame(all_scores, columns=['eps', 'min_samples', 'silhouette_score','cluster'])\n\nsns.lineplot(x=results_df['cluster'], y=results_df['silhouette_score'])\nplt.xlabel('Cluster')\nplt.ylabel('Silhouette Score')\nplt.show()\n\nprint(f\"Number of clusters= {cluster_num} with best silhouette score of {best_silhouette:.3f}\")\n\n\n\n\n\n\n\n\nNumber of clusters= 1 with best silhouette score of -0.033\n\n\n\nresults_df #3-optimal \n\n\n\n\n\n\n\n\neps\nmin_samples\nsilhouette_score\ncluster\n\n\n\n\n0\n0.2\n100\n-0.317742\n7\n\n\n1\n0.3\n100\n-0.290760\n3\n\n\n2\n0.4\n100\n-0.033415\n1\n\n\n3\n0.5\n100\n-0.137065\n3\n\n\n4\n0.5\n1000\n-0.234400\n1\n\n\n\n\n\n\n\n\nX_sample = X.sample(n=6000, replace=True)\n# Normalize the entire dataset\nscaler = StandardScaler()\nX_sample_norm = scaler.fit_transform(X_sample)\n\n\nsilhouette_values= []\nn_clusters =[]\nlinkage=[]\nfor i in range(2,11):\n    hierarchical_model = AgglomerativeClustering(n_clusters=i, linkage='ward')\n    cluster_labels = hierarchical_model.fit_predict(X_sample_norm)\n    score  = silhouette_score(X_sample_norm, cluster_labels)\n    silhouette_values.append(score)\n    n_clusters.append(i)\n    linkage.append('Euclidean')\n\n    hierarchical_model = AgglomerativeClustering(n_clusters=i, linkage='single')\n    cluster_labels = hierarchical_model.fit_predict(X_sample_norm)\n    score  = silhouette_score(X_sample_norm, cluster_labels)\n    silhouette_values.append(score)\n    n_clusters.append(i)\n    linkage.append('Manhattan')\n\n    hierarchical_model = AgglomerativeClustering(n_clusters=i, linkage='average')\n    cluster_labels = hierarchical_model.fit_predict(X_sample_norm)\n    score  = silhouette_score(X_sample_norm, cluster_labels)\n    silhouette_values.append(score)\n    n_clusters.append(i)\n    linkage.append('Cosine')\n\n    hierarchical_model = AgglomerativeClustering(n_clusters=i, linkage='complete')\n    cluster_labels = hierarchical_model.fit_predict(X_sample_norm)\n    score  = silhouette_score(X_sample_norm, cluster_labels)\n    silhouette_values.append(score)\n    n_clusters.append(i)\n    linkage.append('Correlation')\n\n\nsns.lineplot(x=n_clusters,y=silhouette_values, hue =linkage)\nplt.xlabel(\"Cluster\")\nplt.ylabel(\"Silhouette Score\")\nplt.title('Hierarchical Clustering')\n\nText(0.5, 1.0, 'Hierarchical Clustering')\n\n\n\n\n\n\n\n\n\n\nimport scipy.cluster.hierarchy as sch\n\n# Plotting a dendrogram\ndendrogram = sch.dendrogram(sch.linkage(X_sample_norm, method='ward'))\nplt.axhline(y=85, color='r', linestyle='--')\nplt.title('Dendrogram')\nplt.xlabel('Data')\nplt.ylabel('Euclidean distances')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n#Final Model\n\n#K-means cluster = 3\nkmeans = KMeans(n_clusters=3, random_state=0).fit(X_normalized)\nkmeans_labels = kmeans.labels_\nkmeans_s = silhouette_score(X_normalized, kmeans_labels)\n\n#DBSCAN cluster = 3\ndbscan = DBSCAN(eps=0.5, min_samples=100).fit(X_normalized)\ndbscan_labels = dbscan.labels_\nDBSCAN_s = silhouette_score(X_normalized, dbscan_labels)\n\n#Hierarchical cluster = 2\nlabels = AgglomerativeClustering(n_clusters=2, linkage='single').fit_predict(X_sample_norm)\nHierarchical_s = silhouette_score(X_sample_norm, labels)\n\n/Users/xueningyang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\nprint(f\"K-means Silhouette Score= {kmeans_s:.4f}\")\nprint(f\"DBSCAN Silhouette Score= {DBSCAN_s:.4f}\")\nprint(f\"Hierarchical Silhouette Score= {Hierarchical_s:.4f}\")\n\nK-means Silhouette Score= 0.2223\nDBSCAN Silhouette Score= -0.1371\nHierarchical Silhouette Score= 0.3090\n\n\n\ndef plot_cluster(X,color_vector,method):\n    fig, ax = plt.subplots()\n    ax.scatter(X[:,3], X[:,5],c=color_vector, alpha=0.5) #, c=y\n    ax.set(xlabel='ap_hi', ylabel='bmi',\n    title=method)\n    ax.grid()\n    # fig.savefig(\"test.png\")\n    plt.show()\n\n\nplot_cluster(X_normalized,kmeans_labels,'K-means')\n\n\n\n\n\n\n\n\n\nplot_cluster(X_normalized,dbscan_labels,'DBSCAN')\n#In the DBSCAN (Density-Based Spatial Clustering of Applications with Noise) clustering model, a label of -1 signifies a noise point. Specifically, it indicates that a data point does not belong to any cluster due to insufficient neighboring points to meet the minimum density requirement (defined by min_samples and eps parameters). Essentially, these are points that are considered outliers within the dataset.\n\n\n\n\n\n\n\n\n\nplot_cluster(X_sample_norm,labels,'Hierarchical')\n\n\n\n\n\n\n\n\nK-means ~ optimal model\n\n# SOURCE: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport numpy as np\n\n# Generating the sample data from make_blobs\n# This particular setting has one distinct cluster and 3 clusters placed close\n# together.\nX = X_normalized\n\nrange_n_clusters = [2, 3]\n\nfor n_clusters in range_n_clusters:\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1 but in this example all\n    # lie within [-0.1, 1]\n    ax1.set_xlim([-0.1, 1])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n    cluster_labels = clusterer.fit_predict(X)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print(\n        \"For n_clusters =\",\n        n_clusters,\n        \"The average silhouette_score is :\",\n        silhouette_avg,\n    )\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) / n_clusters)\n        ax1.fill_betweenx(\n            np.arange(y_lower, y_upper),\n            0,\n            ith_cluster_silhouette_values,\n            facecolor=color,\n            edgecolor=color,\n            alpha=0.7,\n        )\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n    ax2.scatter(\n        X[:, 3], X[:, 5], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n    )\n\n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    # Draw white circles at cluster centers\n    ax2.scatter(\n        centers[:, 3],\n        centers[:, 5],\n        marker=\"o\",\n        c=\"white\",\n        alpha=1,\n        s=200,\n        edgecolor=\"k\",\n    )\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    plt.suptitle(\n        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n        % n_clusters,\n        fontsize=14,\n        fontweight=\"bold\",\n    )\n\nplt.show()\n\n/Users/xueningyang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/xueningyang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\nFor n_clusters = 2 The average silhouette_score is : 0.2362214005967959\nFor n_clusters = 3 The average silhouette_score is : 0.22231647505616087"
  },
  {
    "objectID": "Conclusions/conclusions.html",
    "href": "Conclusions/conclusions.html",
    "title": "Conclusions",
    "section": "",
    "text": "In this project, I engaged in several machine learning steps to address my primary data science research question. I began by defining the problem I aimed to solve: identifying the main risk factors associated with cardiovascular disease. The data was primarily sourced from the Cardiovascular Disease dataset on Kaggle, chosen for its comprehensive range of variables known to affect heart health, including objective features (age, weight, gender), examination features (systolic blood pressure, blood glucose levels), and subjective features (smoking status, alcohol intake, physical activity). This rich dataset allowed for a nuanced analysis of the interplay between various risk factors and their impact on cardiovascular health.",
    "crumbs": [
      "Conclusions"
    ]
  },
  {
    "objectID": "Conclusions/conclusions.html#project-summary",
    "href": "Conclusions/conclusions.html#project-summary",
    "title": "Conclusions",
    "section": "",
    "text": "In this project, I engaged in several machine learning steps to address my primary data science research question. I began by defining the problem I aimed to solve: identifying the main risk factors associated with cardiovascular disease. The data was primarily sourced from the Cardiovascular Disease dataset on Kaggle, chosen for its comprehensive range of variables known to affect heart health, including objective features (age, weight, gender), examination features (systolic blood pressure, blood glucose levels), and subjective features (smoking status, alcohol intake, physical activity). This rich dataset allowed for a nuanced analysis of the interplay between various risk factors and their impact on cardiovascular health.",
    "crumbs": [
      "Conclusions"
    ]
  },
  {
    "objectID": "Conclusions/conclusions.html#description-of-the-data",
    "href": "Conclusions/conclusions.html#description-of-the-data",
    "title": "Conclusions",
    "section": "Description of the Data",
    "text": "Description of the Data\nThe cardiovascular dataset used in this research project is sourced from the Kaggle website.\nThe original dataset comprises 70,000 data points, each with 13 variables. The target label, ‘cardio,’ indicates the presence of cardiovascular disease, with ‘1’ denoting its presence and ‘0’ indicating its absence. A ‘bmi’ variable is created additionally to capture the information for the relationship between height and weight. In terms of volume, this dataset is substantial, providing an ample amount of data for machine learning purposes. However, its size can also pose a challenge, requiring significant processing time during the training process. After addressing missing values and outliers, approximately 64,000 records remain, showcasing a near 50-50 split for the target label.",
    "crumbs": [
      "Conclusions"
    ]
  },
  {
    "objectID": "Conclusions/conclusions.html#key-results-important-findings",
    "href": "Conclusions/conclusions.html#key-results-important-findings",
    "title": "Conclusions",
    "section": "Key Results & Important Findings ✨",
    "text": "Key Results & Important Findings ✨\nDuring the machine learning processes, a total of four models were constructed: a Naive Bayes model using record data, a Naive Bayes model using text data, a decision tree model, and a random forest model. Upon testing these models with training data, the following evaluation metric results were obtained.\n\n\n\n\nTest Accuracy\nTest Precision\nTest Recall\n\n\n\n\nRecord Naïve Bayes\n0.5985\n0.6171\n0.4624\n\n\nText Naïve Bayes\n0.6875\n0.4615\n0.5217\n\n\nDecision Tree\n0.7301\n0.7346\n0.7243\n\n\nRandom Forest\n0.7316\n0.7589\n0.6523\n\n\n\nThe record Naive Bayes model, the Decision Tree model, and the Random Forest model are constructed using the original record data, while the text Naive Bayes model utilizes record data information combined into text strings for text data treatment. Overall, in terms of accuracy, both the Decision Tree and Random Forest outperform the Naive Bayes model. They exhibit higher precision and recall scores, indicating their superiority in this research. Thus, when similar record data is available for future patients, the Decision Tree model and the Random Forest model are the top choices. However, the Text Naive Bayes model holds an advantage in scenarios where record information isn’t accessible. For instance, if all data are converted into text strings from patient forms or other sources, Text Naive Bayes could be valuable for predictions, albeit with less interpretability throughout the process.\nComparing the Decision Tree model and the Random Forest model, the Decision Tree model is preferred due to its ease of interpretation. While both models perform similarly in terms of accuracy, the Decision Tree model’s interpretability and communicability make it stand out. For instance, the interpretation of the leftmost node—where ‘ap_hi’ is less than or equal to 129.5, ‘age’ is less than or equal to 54.5, ‘cholesterol’ is less than or equal to 2.5, and ‘age’ is again less than or equal to 45—predicts the class as 0, indicating non-cardiovascular disease. This logic is reasonable as it suggests that younger individuals with lower levels of cholesterol and blood pressure generally have a lower risk of cardiovascular disease.\nThe decision tree model is selected as the final model for this research project. Its tree diagram effectively communicates crucial characteristics related to cardiovascular disease to the public, aiding in prevention strategies. This model can relay important messages to patients, assisting in the proactive prevention of potential cardiovascular disease in the future.",
    "crumbs": [
      "Conclusions"
    ]
  },
  {
    "objectID": "Conclusions/conclusions.html#real-world-application---how-these-findings-affect-real-life-and-real-people",
    "href": "Conclusions/conclusions.html#real-world-application---how-these-findings-affect-real-life-and-real-people",
    "title": "Conclusions",
    "section": "Real World Application - “How These Findings Affect Real-life and Real people?”",
    "text": "Real World Application - “How These Findings Affect Real-life and Real people?”\nFrom this project, we’ve successfully pinpointed the crucial factors associated with cardiovascular disease: age, cholesterol levels, BMI, blood pressure, and glucose levels. These health metrics are widely recognized as strongly linked to cardiovascular diseases. In practical applications, we could develop a machine learning algorithm that utilizes this information to calculate an individual’s cardiovascular disease risk level during health screenings. Moreover, it could pinpoint the key factors contributing to high risk based on the insights from the tree diagram. This information empowers doctors to engage in meaningful conversations with patients about improving their health based on these results.\nImagine a simple phone app where users input their health metrics to receive an assessment of which indices pose a heightened risk of cardiovascular disease. The model’s practicality and usability in real-life scenarios are evident. By collecting data from users, we could refine and enhance the model’s accuracy, paving the way for more effective cardiovascular disease predictions in the future",
    "crumbs": [
      "Conclusions"
    ]
  },
  {
    "objectID": "Conclusions/conclusions.html#potential-issues-future-improvements",
    "href": "Conclusions/conclusions.html#potential-issues-future-improvements",
    "title": "Conclusions",
    "section": "Potential Issues & Future Improvements 🌟",
    "text": "Potential Issues & Future Improvements 🌟\nIn reflecting on the project’s outcomes, it is clear that while the selected models have provided valuable insights, there is room for enhancement in future iterations. One potential issue is the balance and diversity of the dataset. Although preprocessing efforts have refined the dataset, ensuring a balanced representation of various demographics can improve the model’s predictive power across different population segments.\nFuture improvements could include the integration of additional variables that may influence cardiovascular health, such as genetic factors, more detailed lifestyle choices, and socioeconomic status. Moreover, implementing more sophisticated algorithms like neural networks or ensemble methods could capture complex patterns in the data more effectively.\nAnother avenue for improvement is the model’s applicability in real-world settings. Efforts could be made to streamline the model’s implementation in healthcare systems, allowing for seamless integration into existing workflows. For instance, creating an API that healthcare providers can use to input patient data and receive risk assessments could be a practical next step.\nLastly, with the rapid advancement of wearable health technology, future models could incorporate data from such devices, providing real-time monitoring and potentially more accurate risk assessments. As technologies and methods evolve, so should approaches to cardiovascular risk modeling to ensure findings remain relevant and applicable in improving patient outcomes.",
    "crumbs": [
      "Conclusions"
    ]
  },
  {
    "objectID": "Conclusions/conclusions.html#additionl-information",
    "href": "Conclusions/conclusions.html#additionl-information",
    "title": "Conclusions",
    "section": "Additionl Information",
    "text": "Additionl Information\nThe human heart, roughly the size of a large fist, is a tireless muscle that beats about 100,000 times daily, pumping up to 7,500 liters of blood. This blood circulation, essential for delivering oxygen and nutrients to tissues and removing wastes like carbon dioxide, relies on the collaborative work of the heart, blood, and a network of blood vessels. The heart’s central location between the lungs in the chest cavity underscores its vital role in the body’s circulatory system.",
    "crumbs": [
      "Conclusions"
    ]
  },
  {
    "objectID": "DataExploration/data_exploration.html",
    "href": "DataExploration/data_exploration.html",
    "title": "Research Project",
    "section": "",
    "text": "import pandas  as  pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ncardio = pd.read_csv('../Data/01-modified-data/cardio.csv')\ncardio['bmi'] = round(cardio['weight'] / ((cardio['height']/100) ** 2),2)\ncardio.head()\n\n\n\n\n\n\n\n\nid\nage\ngender\nheight\nweight\nap_hi\nap_lo\ncholesterol\ngluc\nsmoke\nalco\nactive\ncardio\nbmi\n\n\n\n\n0\n0\n50\n2\n168\n62\n110\n80\n1\n1\n0\n0\n1\n0\n21.97\n\n\n1\n1\n55\n1\n156\n85\n140\n90\n3\n1\n0\n0\n1\n1\n34.93\n\n\n2\n2\n52\n1\n165\n64\n130\n70\n3\n1\n0\n0\n0\n1\n23.51\n\n\n3\n3\n48\n2\n169\n82\n150\n100\n1\n1\n0\n0\n1\n1\n28.71\n\n\n4\n4\n48\n1\n156\n56\n100\n60\n1\n1\n0\n0\n0\n0\n23.01\n\n\n\n\n\n\n\n\n# Select numerical columns\nnumerical_cols = [\"age\", \"height\", \"weight\", \"ap_hi\", \"ap_lo\", \"bmi\"]\n\nstats = {\n    'Mean': cardio[numerical_cols].mean(),\n    'Median': cardio[numerical_cols].median(),\n    'Mode': cardio[numerical_cols].mode().iloc[0],\n    'Standard Deviation': cardio[numerical_cols].std(),\n    'Variance': cardio[numerical_cols].var()\n}\n\nstats_df = pd.DataFrame(stats)\nprint(stats_df)\n\n              Mean   Median    Mode  Standard Deviation      Variance\nage      53.338686   54.000   56.00            6.765294     45.769203\nheight  164.359229  165.000  165.00            8.210126     67.406175\nweight   74.205543   72.000   65.00           14.395829    207.239884\nap_hi   128.817286  120.000  120.00          154.011419  23719.517323\nap_lo    96.630414   80.000   80.00          188.472530  35521.894676\nbmi      27.556502   26.375   23.88            6.091795     37.109965\n\n\n\nfor stat_name, values in stats.items():\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=values.index, y=values.values, palette=\"viridis\")\n    plt.title(stat_name)\n    plt.ylabel(stat_name)\n    plt.show()\n\n/var/folders/v5/fcyrhn_x2qx_md06v54z9x200000gn/T/ipykernel_8894/4009188356.py:3: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x=values.index, y=values.values, palette=\"viridis\")\n/var/folders/v5/fcyrhn_x2qx_md06v54z9x200000gn/T/ipykernel_8894/4009188356.py:3: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x=values.index, y=values.values, palette=\"viridis\")\n/var/folders/v5/fcyrhn_x2qx_md06v54z9x200000gn/T/ipykernel_8894/4009188356.py:3: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x=values.index, y=values.values, palette=\"viridis\")\n/var/folders/v5/fcyrhn_x2qx_md06v54z9x200000gn/T/ipykernel_8894/4009188356.py:3: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x=values.index, y=values.values, palette=\"viridis\")\n/var/folders/v5/fcyrhn_x2qx_md06v54z9x200000gn/T/ipykernel_8894/4009188356.py:3: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x=values.index, y=values.values, palette=\"viridis\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncategorical_vars = [\"gender\", \"cholesterol\", \"gluc\", \"smoke\", \"alco\", \"active\", \"cardio\"]\n\nfor var in categorical_vars:\n    print(f\"Frequency distribution for {var}:\\n\")\n    print(cardio[var].value_counts())\n    print(\"\\n\" + \"=\"*50 + \"\\n\")\n\nFrequency distribution for gender:\n\ngender\n1    45530\n2    24470\nName: count, dtype: int64\n\n==================================================\n\nFrequency distribution for cholesterol:\n\ncholesterol\n1    52385\n2     9549\n3     8066\nName: count, dtype: int64\n\n==================================================\n\nFrequency distribution for gluc:\n\ngluc\n1    59479\n3     5331\n2     5190\nName: count, dtype: int64\n\n==================================================\n\nFrequency distribution for smoke:\n\nsmoke\n0    63831\n1     6169\nName: count, dtype: int64\n\n==================================================\n\nFrequency distribution for alco:\n\nalco\n0    66236\n1     3764\nName: count, dtype: int64\n\n==================================================\n\nFrequency distribution for active:\n\nactive\n1    56261\n0    13739\nName: count, dtype: int64\n\n==================================================\n\nFrequency distribution for cardio:\n\ncardio\n0    35021\n1    34979\nName: count, dtype: int64\n\n==================================================\n\n\n\n\ncategorical_vars = [\"gender\", \"cholesterol\", \"gluc\", \"smoke\", \"alco\", \"active\", \"cardio\"]\n\nfor var in categorical_vars:\n    plt.figure(figsize=(8, 6))\n    sns.countplot(data=cardio, x=var, palette=\"viridis\")\n    plt.title(f'Bar Chart for {var}')\n    plt.ylabel('Frequency')\n    plt.show()\n\n/var/folders/v5/fcyrhn_x2qx_md06v54z9x200000gn/T/ipykernel_4320/2267353753.py:5: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.countplot(data=cardio, x=var, palette=\"viridis\")\n/var/folders/v5/fcyrhn_x2qx_md06v54z9x200000gn/T/ipykernel_4320/2267353753.py:5: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.countplot(data=cardio, x=var, palette=\"viridis\")\n/var/folders/v5/fcyrhn_x2qx_md06v54z9x200000gn/T/ipykernel_4320/2267353753.py:5: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.countplot(data=cardio, x=var, palette=\"viridis\")\n/var/folders/v5/fcyrhn_x2qx_md06v54z9x200000gn/T/ipykernel_4320/2267353753.py:5: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.countplot(data=cardio, x=var, palette=\"viridis\")\n/var/folders/v5/fcyrhn_x2qx_md06v54z9x200000gn/T/ipykernel_4320/2267353753.py:5: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.countplot(data=cardio, x=var, palette=\"viridis\")\n/var/folders/v5/fcyrhn_x2qx_md06v54z9x200000gn/T/ipykernel_4320/2267353753.py:5: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.countplot(data=cardio, x=var, palette=\"viridis\")\n/var/folders/v5/fcyrhn_x2qx_md06v54z9x200000gn/T/ipykernel_4320/2267353753.py:5: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.countplot(data=cardio, x=var, palette=\"viridis\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncorrelation = cardio.corr()\nplt.figure(figsize=(12, 8))\nsns.heatmap(correlation, annot=True, cmap='viridis', vmin=-1, vmax=1)\nplt.title('Correlation Matrix')\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(\"----------------------\")\nprint(\"PEARSON CORRELATION MATRIX:\")\nprint(\"----------------------\")\nprint(cardio.corr(method='pearson',numeric_only=True))\n\nsns.set_theme(style=\"white\")\ncorr = cardio.corr(numeric_only=True)  #Compute the correlation matrix\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool)) \nf, ax = plt.subplots(figsize=(7, 5)) #initialize figure\n\ncmap = sns.diverging_palette(230, 20, as_cmap=True) #custom diverging colormap\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\n----------------------\nPEARSON CORRELATION MATRIX:\n----------------------\n                   id       age    gender    height    weight     ap_hi  \\\nid           1.000000  0.003050  0.003502 -0.003038 -0.001831  0.003356   \nage          0.003050  1.000000 -0.023017 -0.081456  0.053659  0.020793   \ngender       0.003502 -0.023017  1.000000  0.499033  0.155412  0.006005   \nheight      -0.003038 -0.081456  0.499033  1.000000  0.290966  0.005488   \nweight      -0.001831  0.053659  0.155412  0.290966  1.000000  0.030702   \nap_hi        0.003356  0.020793  0.006005  0.005488  0.030702  1.000000   \nap_lo       -0.002529  0.017754  0.015254  0.006150  0.043711  0.016086   \ncholesterol  0.006106  0.154386 -0.035821 -0.050226  0.141763  0.023778   \ngluc         0.002467  0.098596 -0.020491 -0.018595  0.106859  0.011841   \nsmoke       -0.003699 -0.047884  0.338135  0.187989  0.067779 -0.000922   \nalco         0.001210 -0.029918  0.170966  0.094419  0.067115  0.001408   \nactive       0.003755 -0.009819  0.005866 -0.006570 -0.016865 -0.000033   \ncardio       0.003799  0.237749  0.008109 -0.010821  0.181656  0.054475   \nbmi         -0.001375  0.085412 -0.096522 -0.290653  0.761964  0.024850   \n\n                ap_lo  cholesterol      gluc     smoke      alco    active  \\\nid          -0.002529     0.006106  0.002467 -0.003699  0.001210  0.003755   \nage          0.017754     0.154386  0.098596 -0.047884 -0.029918 -0.009819   \ngender       0.015254    -0.035821 -0.020491  0.338135  0.170966  0.005866   \nheight       0.006150    -0.050226 -0.018595  0.187989  0.094419 -0.006570   \nweight       0.043711     0.141763  0.106859  0.067779  0.067115 -0.016865   \nap_hi        0.016086     0.023778  0.011841 -0.000922  0.001408 -0.000033   \nap_lo        1.000000     0.024019  0.010806  0.005186  0.010601  0.004780   \ncholesterol  0.024019     1.000000  0.451578  0.010354  0.035760  0.009911   \ngluc         0.010806     0.451578  1.000000 -0.004756  0.011246 -0.006770   \nsmoke        0.005186     0.010354 -0.004756  1.000000  0.340094  0.025858   \nalco         0.010601     0.035760  0.011246  0.340094  1.000000  0.025476   \nactive       0.004780     0.009911 -0.006770  0.025858  0.025476  1.000000   \ncardio       0.065719     0.221147  0.089307 -0.015486 -0.007330 -0.035653   \nbmi          0.035344     0.146254  0.101387 -0.027219  0.014390 -0.014268   \n\n               cardio       bmi  \nid           0.003799 -0.001375  \nage          0.237749  0.085412  \ngender       0.008109 -0.096522  \nheight      -0.010821 -0.290653  \nweight       0.181656  0.761964  \nap_hi        0.054475  0.024850  \nap_lo        0.065719  0.035344  \ncholesterol  0.221147  0.146254  \ngluc         0.089307  0.101387  \nsmoke       -0.015486 -0.027219  \nalco        -0.007330  0.014390  \nactive      -0.035653 -0.014268  \ncardio       1.000000  0.165617  \nbmi          0.165617  1.000000  \n\n\n\n\n\n\n\n\n\n\nnumerical_cols1 = [\"age\", \"height\", \"weight\"]\n\n\nfor col in numerical_cols1:\n    plt.figure(figsize=(8, 6))\n    sns.displot(data=cardio, x= col, kde=True, stat='density', hue='cardio')\n    plt.title(f'Density Plot for {col}')\n    plt.ylabel('Density')\n    plt.show()\n\n&lt;Figure size 800x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 800x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 800x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8, 6))\nsns.displot(data=cardio, x= \"bmi\", kde=True, stat='density', hue='cardio')\nplt.title(f'Density Plot for bmi')\nplt.ylabel('Density')\nplt.show()\n\n&lt;Figure size 800x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8, 6))\nsns.displot(data=cardio, x= \"bmi\", kde=True, stat='density', hue='cardio')\nplt.title(f'Density Plot for bmi')\nplt.ylabel('Density')\nplt.xlim(0,50)\nplt.show()\n\n&lt;Figure size 800x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8, 6))\nsns.displot(data=cardio, x= \"height\", kde=True, stat='density', hue='gender')\nplt.title(f'Density Plot for height')\nplt.ylabel('Density')\nplt.show()\n\n&lt;Figure size 800x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n# INSERT CODE TO RE-CREATE THE FOLLOWING PLOTS \n# SEE IF THERE IS ANY CORRELATION BETWEEN THE CONTINOUS VARIABLES\nplt.figure(figsize=(8, 6))\nsns.lmplot(\n    data=cardio, \n    x=\"bmi\",\n    y=\"ap_hi\",\n    hue = \"cardio\"\n)\nplt.show()\n\n&lt;Figure size 800x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n#Identify Outlier\n#Apply two sigma rules to search for outliers\ndef outlier_search(df,column_name):\n    mean = np.mean(df[column_name])\n    std = np.std(df[column_name])\n    lower_bound = mean - 3 * std\n    upper_bound = mean + 3 * std\n    outliers = df[(df[column_name] &lt; lower_bound) | (df[column_name] &gt; upper_bound)]\n    return (column_name, len(outliers[column_name]))\ncardio_numeric = cardio[numerical_cols]\ncardio_numeric.shape\n\n(70000, 6)\n\n\n\ncardio_numeric.hist(figsize=(10, 8))  # You can adjust the size with figsize\nfor ax in plt.gcf().axes:  # gcf() gets the current figure. This loops through each subplot.\n    ax.set_ylim([0, 100])\nplt.tight_layout()  # Adjusts subplots for better layout\nplt.show()\n\n\n\n\n\n\n\n\n\n#original outliers, except for age\noutlier_list = []\nfor i in range(len(numerical_cols)-1):\n    name = cardio_numeric.columns[i+1] \n    outlier_list.append(outlier_search(cardio_numeric,name))\noutlier_list\n\n[('height', 293), ('weight', 702), ('ap_hi', 38), ('ap_lo', 951), ('bmi', 536)]\n\n\n\ndef outlier_range(df,column_name):\n    mean = np.mean(df[column_name])\n    std = np.std(df[column_name])\n    lower_bound = mean - 3 * std\n    upper_bound = mean + 3 * std\n    return lower_bound,upper_bound\n\n\n#clean once\ncardio_temp = cardio\nfor i in range(1):\n    cardio_edit = cardio_temp\n    cardio_numeric_edit = cardio_edit[numerical_cols]\n    for k in range(len(numerical_cols)-1):\n        name = cardio_numeric.columns[k+1]\n        lower_bound,upper_bound = outlier_range(cardio_numeric_edit,name)\n        cardio_edit = cardio_edit[(cardio_edit[name] &gt;= lower_bound) & (cardio_edit[name] &lt;= upper_bound)]\n    \n    cardio_temp = cardio_edit\n\n    cardio_numeric_edit = cardio_temp[numerical_cols]\n    outlier_list = []\n    for k in range(len(numerical_cols)-1):\n        name = cardio_numeric_edit.columns[k+1] \n        outlier_list.append(outlier_search(cardio_numeric_edit,name))\n\noutlier_list\n\n[('height', 102),\n ('weight', 317),\n ('ap_hi', 1038),\n ('ap_lo', 434),\n ('bmi', 570)]\n\n\n\ncardio_temp.shape\n\n(67958, 14)\n\n\n\ncardio_temp = cardio\nfor i in range(7):\n    cardio_edit = cardio_temp\n    cardio_numeric_edit = cardio_edit[numerical_cols]\n    for k in range(len(numerical_cols)-1):\n        name = cardio_numeric.columns[k+1]\n        lower_bound,upper_bound = outlier_range(cardio_numeric_edit,name)\n        cardio_edit = cardio_edit[(cardio_edit[name] &gt;= lower_bound) & (cardio_edit[name] &lt;= upper_bound)]\n    \n    cardio_temp = cardio_edit\n\n    cardio_numeric_edit = cardio_temp[numerical_cols]\n    outlier_list = []\n    for k in range(len(numerical_cols)-1):\n        name = cardio_numeric_edit.columns[k+1] \n        outlier_list.append(outlier_search(cardio_numeric_edit,name))\n\noutlier_list\n\n[('height', 0), ('weight', 0), ('ap_hi', 0), ('ap_lo', 0), ('bmi', 0)]\n\n\n\ncardio_temp.shape\n\n(64806, 14)\n\n\n\ncardio_numeric_clean = cardio_temp[numerical_cols]\ncardio_numeric_clean.hist(figsize=(10, 8))  # You can adjust the size with figsize\nplt.tight_layout()  # Adjusts subplots for better layout\nplt.show()\n\n\n\n\n\n\n\n\n\ncardio_temp.to_csv('../data/01-modified-data/cardiovascular_numeric_final.csv')"
  },
  {
    "objectID": "Introduction/Introduction.html",
    "href": "Introduction/Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Cardiovascular risk analysis is an essential component of modern healthcare and has profound consequences for individuals and society. By assessing an individual’s risk factors for heart disease and related conditions, healthcare professionals can take proactive steps to prevent or mitigate these potentially life-threatening problems. This risk analysis is critical for a number of reasons.\nFirst, it can prevent cardiovascular disease and promote better health and quality of life through lifestyle changes and medication. Secondly, it can effectively allocate medical resources, focus on high-risk groups, and reduce overall medical costs. Personalized medicine makes it possible to tailor treatments to individual risk profiles, thereby increasing treatment effectiveness while minimizing side effects.\nOn a broader scale, public health programs benefit from cardiovascular risk data to guide community-wide interventions and health education efforts. Research and development efforts benefit from a deeper understanding of the mechanisms of heart disease, leading to innovative treatment and prevention strategies. Cardiovascular risk analysis also reveals health disparities and contributes to equitable health care access and outcomes.\nAdditionally, early detection is key as it allows for timely intervention before symptoms appear. Patient education is another important outcome, allowing individuals to make informed choices about their health. Ultimately, by analyzing and addressing cardiovascular risk factors, we can enhance individual well-being and the overall health of society.",
    "crumbs": [
      "Project Introduction"
    ]
  },
  {
    "objectID": "Introduction/Introduction.html#what-is-cardiovascular-risk-and-why-we-want-to-analyze-it",
    "href": "Introduction/Introduction.html#what-is-cardiovascular-risk-and-why-we-want-to-analyze-it",
    "title": "Introduction",
    "section": "",
    "text": "Cardiovascular risk analysis is an essential component of modern healthcare and has profound consequences for individuals and society. By assessing an individual’s risk factors for heart disease and related conditions, healthcare professionals can take proactive steps to prevent or mitigate these potentially life-threatening problems. This risk analysis is critical for a number of reasons.\nFirst, it can prevent cardiovascular disease and promote better health and quality of life through lifestyle changes and medication. Secondly, it can effectively allocate medical resources, focus on high-risk groups, and reduce overall medical costs. Personalized medicine makes it possible to tailor treatments to individual risk profiles, thereby increasing treatment effectiveness while minimizing side effects.\nOn a broader scale, public health programs benefit from cardiovascular risk data to guide community-wide interventions and health education efforts. Research and development efforts benefit from a deeper understanding of the mechanisms of heart disease, leading to innovative treatment and prevention strategies. Cardiovascular risk analysis also reveals health disparities and contributes to equitable health care access and outcomes.\nAdditionally, early detection is key as it allows for timely intervention before symptoms appear. Patient education is another important outcome, allowing individuals to make informed choices about their health. Ultimately, by analyzing and addressing cardiovascular risk factors, we can enhance individual well-being and the overall health of society.",
    "crumbs": [
      "Project Introduction"
    ]
  },
  {
    "objectID": "Introduction/Introduction.html#potential-factors",
    "href": "Introduction/Introduction.html#potential-factors",
    "title": "Introduction",
    "section": "Potential Factors 🫀",
    "text": "Potential Factors 🫀\nCardiovascular risk prediction is a critical aspect of preventing heart disease and related conditions. It involves assessing an individual’s likelihood of developing cardiovascular issues, such as heart attacks or strokes, based on various risk factors. These risk factors can include age, gender, family history, smoking status, physical activity, diet, blood pressure, cholesterol levels, diabetes, and body weight.\n\n10 Questions:\n\nAge: How old are you? (Cardiovascular risk generally increases with age.)\nGender: Are you male or female? (Certain risk factors may vary by gender.)\nFamily History: Does your family have a history of heart disease or related conditions?\nSmoking: Do you currently smoke, or have you ever smoked in the past?\nPhysical Activity: How often do you engage in physical activity or exercise?\nDiet: What is your typical diet like? (Consider factors like consumption of fruits, vegetables, and processed foods.)\nBlood Pressure: What is your current blood pressure, and have you been diagnosed with hypertension (high blood pressure)?\nCholesterol Levels: Do you know your cholesterol levels, including LDL (low-density lipoprotein) and HDL (high-density lipoprotein) cholesterol?\nDiabetes: Have you been diagnosed with diabetes, or do you have elevated blood sugar levels?\nWeight and Body Mass Index (BMI): What is your current weight, and do you know your BMI? (Obesity is a significant cardiovascular risk factor.)\n\n\n\n\n\n\n\nUnlocking Additional Factors Contributing to Cardiovascular Diseases\n\n\n\n\n\n\nMental Health: Mental health conditions, especially stress, depression, and anxiety, have been shown to affect heart health, potentially due to hormonal changes and unhealthy coping behaviors like smoking and poor diet.\nSleep Patterns: Both insufficient and excessive sleep have been associated with an increased risk of cardiovascular disease, likely related to disruptions in biological processes.\nAlcohol Consumption: Moderate alcohol intake may have some protective effects on the heart, but excessive alcohol use is a risk factor for cardiovascular disease.\nEnvironmental Factors: Exposure to pollution and living in areas with little opportunity for physical activity or access to healthy foods can increase cardiovascular risk.\nEthnicity: Certain ethnic groups may have a higher predisposition to cardiovascular diseases due to both genetic factors and social determinants of health.\nSocioeconomic Status: Lower socioeconomic status has been linked with higher cardiovascular risk due to factors like limited access to healthcare, higher stress levels, and differences in lifestyle habits.",
    "crumbs": [
      "Project Introduction"
    ]
  },
  {
    "objectID": "Introduction/Introduction.html#scholarly-articles",
    "href": "Introduction/Introduction.html#scholarly-articles",
    "title": "Introduction",
    "section": "Scholarly Articles",
    "text": "Scholarly Articles\n\nArticle 1\nThe article “Cardiovascular Risk Prediction” by Donald M. Lloyd-Jones discusses the significant attention that risk prediction has garnered in the cardiovascular literature over the past five years. Risk assessment plays a crucial role in various aspects of cardiovascular disease (CVD) research, including the identification of risk factors, the discovery of new risk markers, the evaluation of potential therapeutic targets, and the cost-effective implementation of preventive measures for both primary and secondary CVD prevention. The importance of risk prediction in the field of CVD was underscored by the publication of the Third Report of the National Cholesterol Education Program’s Adult Treatment Panel (ATP-III) in 2001, along with similar guidelines from other national and international organizations. These guidelines elevated risk prediction to a central position in the study and management of cardiovascular diseases.\n\n\nArticle 2\nThe study titled “Cardiovascular Risk Prediction in Men and Women Aged Under 50 Years Using Routine Care Data” by Hendrikus J. A. van Os and colleagues aimed to develop prediction models for the risk of first-ever cardiovascular events in individuals aged 30 to 49 years. Traditional cardiovascular risk prediction models often exclude young adults, and it is known that cardiovascular risk factors vary between men and women.\nThe researchers analyzed data from a Dutch routine care database, which included individuals without prior cardiovascular disease. They used Cox proportional hazards models, specific to gender, based on traditional cardiovascular risk factors as reference models. These were then compared to models using subsets of predictors, including the 20 or 50 most significant predictors identified through the Cox elastic net model regularization coefficients. The performance of these models was assessed using the C-index and calibration curve slopes over a 10-year follow-up period. The analyses were further stratified by age groups of 30-39 years and 40-49 years.\nThe study included 542,141 patients, with an average age of 39.7 years, and 51% were women. Over the follow-up period, 10,767 cardiovascular events occurred. The reference models, which included traditional predictors, showed moderate discrimination in both women (C-index 0.648) and men (C-index 0.661). However, the Cox proportional hazard models incorporating the 50 most important predictors led to a modest increase in the C-index (0.030 for women and 0.012 for men) and a net correct reclassification of 3.7% of events in women and 1.2% in men compared to the reference models.\nIn conclusion, the study suggests that electronic health record-derived prediction models for first-ever cardiovascular events in individuals under 50 years of age have moderate discriminatory performance. However, the inclusion of nontraditional cardiovascular predictors identified through data-driven methods can slightly improve the predictive performance of these models for both men and women in this age group.\nBoth (Os et al. (2023), Lloyd-Jones (2010)) scholarly articles emphasize the need for increased attention and further research into cardiovascular disease, particularly regarding its potential effects and contributing factors. The two datasets 12 are going to be used in the further research.",
    "crumbs": [
      "Project Introduction"
    ]
  },
  {
    "objectID": "Introduction/Introduction.html#footnotes",
    "href": "Introduction/Introduction.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPrediction of cardiovascular disease risk based on major contributing features↩︎\nCVD Risk Prediction Synthetic Dataset: 2. fullPatientData.csv - csv file with multiple covariates z↩︎",
    "crumbs": [
      "Project Introduction"
    ]
  },
  {
    "objectID": "NaiveBayes/NB_Record.html",
    "href": "NaiveBayes/NB_Record.html",
    "title": "Research Project",
    "section": "",
    "text": "import numpy as np \nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy\nimport sklearn \n\n\ndef merit(x,y,correlation=\"pearson\"):\n    # x=matrix of features \n    # y=matrix (or vector) of targets \n    # correlation=\"pearson\" or \"spearman\"\n    r_xx=0\n    r_xy=0\n    for i in range(x.shape[1]):\n        for v in range(i+1,x.shape[1]):\n            xi = x.iloc[:,i]\n            xv = x.iloc[:,v]\n            if correlation==\"pearson\": \n                correlation_coefficient, p_value = scipy.stats.pearsonr(xi,xv)\n                r_xx += correlation_coefficient\n            if correlation==\"spearman\": \n                correlation_coefficient, p_value = scipy.stats.spearmanr(xi,xv)\n                r_xx += correlation_coefficient\n\n    for i in range(x.shape[1]):\n        xi = x.iloc[:,i]\n        yi = y.iloc[:,0]\n        if correlation==\"pearson\": \n            correlation_coefficient, p_value = scipy.stats.pearsonr(xi,yi)\n            r_xy += correlation_coefficient\n        if correlation==\"spearman\": \n            correlation_coefficient, p_value = scipy.stats.spearmanr(xi,yi)\n            r_xy += correlation_coefficient\n            \n    k = x.shape[1]\n    if k == 1:\n        r_xx_avg =0\n        r_xy_avg = r_xy/(k)\n    else:\n        r_xx_avg = r_xx/((k-1)*k/2)\n        r_xy_avg = r_xy/(k)\n\n    return (k*np.absolute(r_xy_avg)/(np.sqrt(k+k*(k-1)*np.absolute(r_xx_avg))))\n\n\ncardio = pd.read_csv(\"../Data/01-modified-data/cardiovascular_numeric_final.csv\")\n\ncardio = cardio.drop(columns=['Unnamed: 0', 'id'])\ncardio.head()\n\n\n\n\n\n\n\n\nage\ngender\nheight\nweight\nap_hi\nap_lo\ncholesterol\ngluc\nsmoke\nalco\nactive\ncardio\nbmi\n\n\n\n\n0\n50\n2\n168\n62\n110\n80\n1\n1\n0\n0\n1\n0\n21.97\n\n\n1\n55\n1\n156\n85\n140\n90\n3\n1\n0\n0\n1\n1\n34.93\n\n\n2\n52\n1\n165\n64\n130\n70\n3\n1\n0\n0\n0\n1\n23.51\n\n\n3\n48\n2\n169\n82\n150\n100\n1\n1\n0\n0\n1\n1\n28.71\n\n\n4\n48\n1\n156\n56\n100\n60\n1\n1\n0\n0\n0\n0\n23.01\n\n\n\n\n\n\n\n\nimport itertools\n\ndef maximize_CFS(x,y):\n    list1 = [*range(1,x.shape[1]+1)]; #print(list1)\n    m = float('-inf')\n    for L in range(1,len(list1)):\n        for subset in itertools.combinations(list1, L):\n            subset_l = list(subset)\n            subset_l = [x - 1 for x in subset_l]\n            x_subset = x.iloc[:,subset_l]\n            m_temp = merit(x_subset,y,correlation=\"spearman\")\n            if m_temp &gt; m:\n                print(\"found new max: \", m_temp)\n                print(\"optimal features = \", subset_l)\n                m= m_temp\n                opt_set = x_subset\n\n    return opt_set\n\ndef maximize_CFS_2(x,y):\n    list1 = [*range(1,x.shape[1]+1)]; #print(list1)\n    m = float('-inf')\n    for L in range(1,len(list1)):\n        for subset in itertools.combinations(list1, L):\n            subset_l = list(subset)\n            subset_l = [x - 1 for x in subset_l]\n            x_subset = x.iloc[:,subset_l]\n            m_temp = merit(x_subset,y,correlation=\"spearman\")\n            if m_temp &gt; m:\n                print(\"found new max: \", m_temp)\n                subset_l_print = [x + 6 for x in subset_l]\n                print(\"optimal features = \", subset_l_print)\n                m= m_temp\n                opt_set = x_subset\n\n    return opt_set\n\n\ndef explore_data(x,y):\n\n    #PRINT SHAPE\n    print(x.shape)\n    print(y.shape)\n\n    #COMPUTE MERIT \n    # print(\"merit =\",merit(x,y,correlation=\"spearman\")); \n    print(\"merit =\",merit(x,y,correlation=\"spearman\"))\n\n    # #PLOT\n    # if (iplot):\n    #     sns.pairplot(pd.DataFrame(np.hstack((x,y.reshape(y.shape[0],1)))))\n    #     plt.show()\n\n\ncardio_x = cardio.drop(columns=['cardio'])\ncardio_x_1 = cardio_x.iloc[:,0:6]\ncardio_x_2 = cardio_x.iloc[:,6:12]\ncardio_y = cardio[['cardio']]\n\n\nexplore_data(cardio_x,cardio_y)\n\n(64806, 12)\n(64806, 1)\nmerit = 0.3148619450034282\n\n\n\nx_opt_1=maximize_CFS(cardio_x_1,cardio_y) #using merit find the most revelent columns (1st 6)\n\nfound new max:  0.24118366917520198\noptimal features =  [0]\nfound new max:  0.44305965281952503\noptimal features =  [4]\nfound new max:  0.4496408170195329\noptimal features =  [0, 4, 5]\n\n\n\nx_opt_2=maximize_CFS_2(cardio_x_2,cardio_y) #rest of (6)\n\nfound new max:  0.2107372715369121\noptimal features =  [6]\nfound new max:  0.2586790918689096\noptimal features =  [6, 11]\n\n\n\ncardio_x_new = cardio_x.iloc[:,[0,4,5,6,11]] #best subset\nexplore_data(cardio_x_new,cardio_y)\n\n(64806, 5)\n(64806, 1)\nmerit = 0.4562044278371533\n\n\n\nfrom sklearn import metrics\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n\nX_train, X_test, y_train, y_test = train_test_split(cardio_x_new, cardio_y, test_size=0.2, random_state=20)\nnb_classifier = MultinomialNB()\nnb_classifier.fit(X_train,y_train)\ny_pred_test = nb_classifier.predict(X_test)\ny_pred_train = nb_classifier.predict(X_train)\n\n/Users/xueningyang/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\n\ndef get_metrics(true, preds):\n    \"\"\"\n    Takes gold labels and predictions to compute performance metrics\n    :param true: array-like object\n    :param preds: array-like object\n    :return: a tuple of various performance metrics\n    \"\"\"\n    accuracy = metrics.accuracy_score(true['cardio'],preds)\n    f1_score = metrics.f1_score(true['cardio'],preds)\n    conf_matrix = metrics.confusion_matrix(true['cardio'],preds)\n\n    return accuracy, f1_score, conf_matrix\n\n\ndef plot_confusion_matrix(conf_matrix_data, labels, data=\"train\"):\n    \"\"\"\n    Takes as input confusion matrix data from get_metrics() and prints out a\n    confusion matrix\n    :param conf_matrix_data:\n    :return: None\n    \"\"\"\n    if data == \"train\":\n        plt.title(\"Training Confusion Matrix\")\n    else:\n        plt.title(\"Test Confusion Matrix\")\n    axis = sns.heatmap(conf_matrix_data,annot=True,fmt=\"d\")\n    axis.set_xticklabels(labels)\n    axis.set_yticklabels(labels)\n    axis.set_xlabel(\"predicted\")\n    axis.set_ylabel(\"true\")\n    plt.show()\n    return\n\n\nacc_train, f1_train, conf_train = get_metrics(y_train, y_pred_train)\nacc_test, f1_test, conf_test = get_metrics(y_test, y_pred_test)\nprint(\"Training Accuracy:\", acc_train)\nprint(\"Test Accuracy:\", acc_test)\nprint(\"Training F1 Score:\", f1_train)\nprint(\"Test F1 Score:\", f1_test)\n\nTraining Accuracy: 0.6054123910192115\nTest Accuracy: 0.598518747106928\nTraining F1 Score: 0.5276719540070651\nTest F1 Score: 0.5287085672885347\n\n\n\nplot_confusion_matrix(conf_train, [0, 1],data=\"train\")\n\n\n\n\n\n\n\n\n\nplot_confusion_matrix(conf_test, [0, 1],data=\"test\")"
  },
  {
    "objectID": "DataCleaning/datacleaning_python.html",
    "href": "DataCleaning/datacleaning_python.html",
    "title": "Research Project",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\n\ndf = pd.read_csv(\"../../data/00-raw-data/cardiorisk.csv\")\n\n\ndf1 = df.rename(columns={'0': 'title', '1': 'description'})\ndf2 = df1[['title', 'description']]\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n# Vectorize the texts using CountVectorizer\nvectorizer = CountVectorizer()\nX_title= vectorizer.fit_transform(df2['title'])\n\n# Calculate word frequencies\nword_frequencies_title = X_title.sum(axis=0)\nword_frequencies_title = np.array(word_frequencies_title).flatten()\n\n# Create a mapping of words to their frequencies\nword_to_frequency_title = dict(zip(vectorizer.get_feature_names_out(), word_frequencies_title))\n\nX_des= vectorizer.fit_transform(df2['description'])\n\n# Calculate word frequencies\nword_frequencies_des = X_des.sum(axis=0)\nword_frequencies_des = np.array(word_frequencies_des).flatten()\n\n# Create a mapping of words to their frequencies\nword_frequencies_des = dict(zip(vectorizer.get_feature_names_out(), word_frequencies_des))\n\n\n\n\nword_to_frequency_title_fil = {k: v for k, v in word_to_frequency_title.items() if v &gt; 8}\nword_frequencies_des_fil = {k: v for k, v in word_frequencies_des.items() if v &gt; 8}\nword_to_frequency_title_final = dict(sorted(word_to_frequency_title_fil.items(),  key=lambda item: item[1],reverse=True))\nword_to_frequency_des_final = dict(sorted(word_frequencies_des_fil.items(),  key=lambda item: item[1],reverse=True))\n\n\nword_to_frequency_title_final\n\n{'to': 26,\n 'and': 22,\n 'of': 21,\n 'for': 18,\n 'the': 17,\n 'in': 16,\n 'study': 12,\n 'heart': 10,\n 'is': 10,\n 'cardiovascular': 9,\n 'health': 9,\n 'risk': 9}\n\n\n\nword_to_frequency_des_final\n\n{'the': 78,\n 'of': 65,\n 'and': 62,\n 'to': 48,\n 'in': 38,\n 'for': 26,\n 'that': 26,\n 'health': 23,\n 'with': 23,\n 'is': 22,\n 'are': 21,\n 'heart': 18,\n 'new': 18,\n 'risk': 17,\n 'as': 16,\n 'can': 16,\n 'be': 15,\n 'have': 15,\n 'but': 14,\n 'de': 14,\n 'on': 14,\n 'you': 14,\n 'more': 13,\n 'this': 13,\n 'has': 12,\n 'study': 12,\n 'by': 11,\n 'disease': 11,\n 'high': 11,\n 'it': 11,\n 'been': 10,\n 'cardiovascular': 10,\n 'from': 10,\n 'que': 10,\n 'weight': 10,\n 'your': 10,\n 'at': 9,\n 'la': 9,\n 'than': 9,\n 'these': 9,\n 'was': 9,\n 'who': 9}\n\n\n\ndf2.to_csv('../../data/01-modified-data/cardioriskapi.csv')"
  },
  {
    "objectID": "DataCleaning/Cardio1.html",
    "href": "DataCleaning/Cardio1.html",
    "title": "Cardio1",
    "section": "",
    "text": "library(\"readxl\")\nlibrary(skimr)\n\n\ncardio &lt;- read_excel(\"../Data/00-raw-data/cardio_train.xlsx\")\ncardio$age &lt;- round(cardio$age / 365)\nhead(cardio)\n\n# A tibble: 6 × 13\n     id   age gender height weight ap_hi ap_lo cholesterol  gluc smoke  alco\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0    50      2    168     62   110    80           1     1     0     0\n2     1    55      1    156     85   140    90           3     1     0     0\n3     2    52      1    165     64   130    70           3     1     0     0\n4     3    48      2    169     82   150   100           1     1     0     0\n5     4    48      1    156     56   100    60           1     1     0     0\n6     8    60      1    151     67   120    80           2     2     0     0\n# ℹ 2 more variables: active &lt;dbl&gt;, cardio &lt;dbl&gt;\n\nsummary(cardio)\n\n       id             age            gender         height     \n Min.   :    0   Min.   :30.00   Min.   :1.00   Min.   : 55.0  \n 1st Qu.:25007   1st Qu.:48.00   1st Qu.:1.00   1st Qu.:159.0  \n Median :50002   Median :54.00   Median :1.00   Median :165.0  \n Mean   :49972   Mean   :53.34   Mean   :1.35   Mean   :164.4  \n 3rd Qu.:74889   3rd Qu.:58.00   3rd Qu.:2.00   3rd Qu.:170.0  \n Max.   :99999   Max.   :65.00   Max.   :2.00   Max.   :250.0  \n     weight           ap_hi             ap_lo           cholesterol   \n Min.   : 10.00   Min.   : -150.0   Min.   :  -70.00   Min.   :1.000  \n 1st Qu.: 65.00   1st Qu.:  120.0   1st Qu.:   80.00   1st Qu.:1.000  \n Median : 72.00   Median :  120.0   Median :   80.00   Median :1.000  \n Mean   : 74.21   Mean   :  128.8   Mean   :   96.63   Mean   :1.367  \n 3rd Qu.: 82.00   3rd Qu.:  140.0   3rd Qu.:   90.00   3rd Qu.:2.000  \n Max.   :200.00   Max.   :16020.0   Max.   :11000.00   Max.   :3.000  \n      gluc           smoke              alco             active      \n Min.   :1.000   Min.   :0.00000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:1.000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:1.0000  \n Median :1.000   Median :0.00000   Median :0.00000   Median :1.0000  \n Mean   :1.226   Mean   :0.08813   Mean   :0.05377   Mean   :0.8037  \n 3rd Qu.:1.000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:1.0000  \n Max.   :3.000   Max.   :1.00000   Max.   :1.00000   Max.   :1.0000  \n     cardio      \n Min.   :0.0000  \n 1st Qu.:0.0000  \n Median :0.0000  \n Mean   :0.4997  \n 3rd Qu.:1.0000  \n Max.   :1.0000  \n\n\n\n#Analyzing the excel File\nprint(is.data.frame(cardio))\n\n[1] TRUE\n\nprint(ncol(cardio))\n\n[1] 13\n\nprint(nrow(cardio))\n\n[1] 70000\n\n#70000 obs, 13 variables\n\n\n#checking for any NA value\ncolSums(is.na(cardio)) #No NA\n\n         id         age      gender      height      weight       ap_hi \n          0           0           0           0           0           0 \n      ap_lo cholesterol        gluc       smoke        alco      active \n          0           0           0           0           0           0 \n     cardio \n          0 \n\n\n\n# Compute the correlation matrix\ncor_matrix &lt;- cor(cardio)  # I'm using the built-in mtcars dataset as an example\n\n# Create a heatmap\nheatmap(cor_matrix, main=\"Correlation Heatmap\", symm=TRUE, trace=\"none\")\n\nWarning in plot.window(...): \"trace\" is not a graphical parameter\n\n\nWarning in plot.xy(xy, type, ...): \"trace\" is not a graphical parameter\n\n\nWarning in title(...): \"trace\" is not a graphical parameter\n\n\n\n\n\n\n\n\n\n\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\n#compute the correlation matrix\ncor_matrix &lt;- cor(cardio)  \n#correlation plot\ncorrplot(cor_matrix, method=\"circle\")\n\n\n\n\n\n\n\n\n\nwrite.csv(cardio, file = \"../Data/01-modified-data/cardio.csv\", row.names = FALSE)"
  },
  {
    "objectID": "DataCleaning/datagcleaningAPI_R.html",
    "href": "DataCleaning/datagcleaningAPI_R.html",
    "title": "DSAN5000",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\ndf &lt;- read_csv(\"../Data/00-raw-data/NewYorkTimesAPI.csv\")\n\nRows: 40 Columns: 30\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (19): status, copyright, response.docs.abstract, response.docs.web_url,...\ndbl   (5): response.docs.word_count, response.docs.print_page, response.meta...\nlgl   (5): response.docs.headline.content_kicker, response.docs.headline.nam...\ndttm  (1): response.docs.pub_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ncolnames(df)\n\n [1] \"status\"                               \n [2] \"copyright\"                            \n [3] \"response.docs.abstract\"               \n [4] \"response.docs.web_url\"                \n [5] \"response.docs.snippet\"                \n [6] \"response.docs.lead_paragraph\"         \n [7] \"response.docs.source\"                 \n [8] \"response.docs.pub_date\"               \n [9] \"response.docs.document_type\"          \n[10] \"response.docs.news_desk\"              \n[11] \"response.docs.section_name\"           \n[12] \"response.docs.type_of_material\"       \n[13] \"response.docs._id\"                    \n[14] \"response.docs.word_count\"             \n[15] \"response.docs.uri\"                    \n[16] \"response.docs.subsection_name\"        \n[17] \"response.docs.print_section\"          \n[18] \"response.docs.print_page\"             \n[19] \"response.docs.headline.main\"          \n[20] \"response.docs.headline.kicker\"        \n[21] \"response.docs.headline.content_kicker\"\n[22] \"response.docs.headline.print_headline\"\n[23] \"response.docs.headline.name\"          \n[24] \"response.docs.headline.seo\"           \n[25] \"response.docs.headline.sub\"           \n[26] \"response.docs.byline.original\"        \n[27] \"response.docs.byline.organization\"    \n[28] \"response.meta.hits\"                   \n[29] \"response.meta.offset\"                 \n[30] \"response.meta.time\"                   \n\n\n\ndf1 = subset(df, select = c(`response.docs.abstract`, `response.docs.lead_paragraph`))\n\n\nwrite.csv(df1, file = \"../Data/01-modified-data/NewYorkTimesAPI.csv\", row.names = FALSE)"
  },
  {
    "objectID": "DataCleaning/datacleaning.html",
    "href": "DataCleaning/datacleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Data cleaning is an essential step in the data analysis process, vital for ensuring the accuracy and reliability of conclusions derived from the data. It entails the meticulous task of correcting or removing data that is erroneous, corrupted, incomplete, or irrelevant. This process is especially critical in handling large datasets, which often originate from various sources and are prone to a range of inconsistencies, including duplicate records, misaligned formats, or missing values.\nThe importance of clean data cannot be overstated, as it is fundamental to making informed decisions. Inaccurate data can lead to erroneous conclusions, significantly impacting the outcome of an analysis. This is particularly crucial for machine learning models, which depend heavily on the quality of their input data for reliable predictions and analysis.\nMoreover, data cleaning plays a critical role in identifying and addressing potential issues within the dataset, enabling a more profound understanding of the underlying patterns and trends. These might otherwise remain hidden amidst the noise inherent in unclean data. In summary, data cleaning is not just a preparatory step but a crucial aspect of the data analysis process, underpinning the integrity and usefulness of the resulting insights.",
    "crumbs": [
      "Data Cleaning"
    ]
  },
  {
    "objectID": "DataCleaning/datacleaning.html#why-is-data-cleaning-necessary",
    "href": "DataCleaning/datacleaning.html#why-is-data-cleaning-necessary",
    "title": "Data Cleaning",
    "section": "",
    "text": "Data cleaning is an essential step in the data analysis process, vital for ensuring the accuracy and reliability of conclusions derived from the data. It entails the meticulous task of correcting or removing data that is erroneous, corrupted, incomplete, or irrelevant. This process is especially critical in handling large datasets, which often originate from various sources and are prone to a range of inconsistencies, including duplicate records, misaligned formats, or missing values.\nThe importance of clean data cannot be overstated, as it is fundamental to making informed decisions. Inaccurate data can lead to erroneous conclusions, significantly impacting the outcome of an analysis. This is particularly crucial for machine learning models, which depend heavily on the quality of their input data for reliable predictions and analysis.\nMoreover, data cleaning plays a critical role in identifying and addressing potential issues within the dataset, enabling a more profound understanding of the underlying patterns and trends. These might otherwise remain hidden amidst the noise inherent in unclean data. In summary, data cleaning is not just a preparatory step but a crucial aspect of the data analysis process, underpinning the integrity and usefulness of the resulting insights.",
    "crumbs": [
      "Data Cleaning"
    ]
  },
  {
    "objectID": "DataCleaning/datacleaning.html#data-cleaning-for-cardio_train.xlsx",
    "href": "DataCleaning/datacleaning.html#data-cleaning-for-cardio_train.xlsx",
    "title": "Data Cleaning",
    "section": "1. Data Cleaning for cardio_train.xlsx",
    "text": "1. Data Cleaning for cardio_train.xlsx\n\n\n\n\n\n\nLinks\n\n\n\n\nRaw Data: https://github.com/anly501/dsan-5000-project-Sel272/blob/main/DSAN5000project/Data/00-raw-data/cardio_train.xlsx.\nCleaned Data: https://github.com/anly501/dsan-5000-project-Sel272/blob/main/DSAN5000project/Data/01-modified-data/cardio.csv.\nCode: https://github.com/anly501/dsan-5000-project-Sel272/blob/main/DSAN5000project/DataCleaning/Cardio1.Rmd.\n\n\n\nIn the process of preparing our dataset for analysis, we observed the following:\n\nNo Missing Values: The dataset was meticulously examined for any missing or ‘NA’ values. We can confidently confirm the absence of such values, making it seamless for further analytical processes.\nCategorical to Numerical Transformation: One essential step in data preprocessing, especially for machine learning models, is the conversion of categorical data into a numerical format. In our case, this transformation was already accomplished. Every categorical variable in our dataset has been effectively converted to a numerical representation, ensuring compatibility with most analytical tools and algorithms.\n\nOne of the columns I manipulated is the “age” column. Originally, the “age” column was measured in days. I divided the values in this column by 365 to convert them from days to years.\n\nCleanliness: Drawing the dataset from Kaggle, a platform renowned for its comprehensive datasets, we benefited from the intrinsic cleanliness of the data. The dataset required no additional cleaning, modification, or transformation, making it ready for immediate analysis.\nOutlier Detection: In the traditional data cleaning process, identifying and cleaning outliers is usually conducted as part of data cleaning. However, in the workflow of this project, this step was carried out during the Data Exploration section. For more details and visualizations, please refer to that section. Below is a quick summary of the outlier detection performed. “I used the 3-Sigma rule to identify outliers. Any data record that falls outside the range of mean plus or minus 3 standard deviations is defined as an outlier. I removed those data points and recalculated the 3-Sigma range. This process was repeated until no outliers were detected. By visualization, It’s evident that there are numerous outliers for ap_hi, ap_lo, and bmi, where the values are significantly outside the standard range. For other variables, like weight and height, there are also some values that defy common sense. For age, we opted to retain the lower age variables, even though they are considered outliers based on the 3-Sigma rule.”\n\nIn summary, the dataset’s pristine condition significantly simplifies the preprocessing phase, allowing us to proceed directly to exploratory data analysis or modeling without additional data wrangling steps. Upon examining the correlation matrix for all the variables, cholesterol, age, and weight seem to have a moderately strong correlational relationship with my target variable, ‘cardio’.\n\n\n\nCorrelation Matrix",
    "crumbs": [
      "Data Cleaning"
    ]
  },
  {
    "objectID": "DataCleaning/datacleaning.html#data-cleaning-for-api-using-python-r",
    "href": "DataCleaning/datacleaning.html#data-cleaning-for-api-using-python-r",
    "title": "Data Cleaning",
    "section": "2. Data Cleaning for API Using Python & R",
    "text": "2. Data Cleaning for API Using Python & R\n\n\n\n\n\n\nLinks\n\n\n\n\nPython\n\nRaw Data: https://github.com/anly501/dsan-5000-project-Sel272/blob/main/DSAN5000project/Data/00-raw-data/cardiorisk.csv.\nCleaned Data: https://github.com/anly501/dsan-5000-project-Sel272/blob/main/DSAN5000project/Data/01-modified-data/cardioriskapi.csv.\nCode: https://github.com/anly501/dsan-5000-project-Sel272/blob/main/DSAN5000project/DataCleaning/datacleaning_python.ipynb.\n\nR\n\nRaw Data: https://github.com/anly501/dsan-5000-project-Sel272/blob/main/DSAN5000project/Data/00-raw-data/NewYorkTimesAPI.csv.\nCleaned Data: https://github.com/anly501/dsan-5000-project-Sel272/blob/main/DSAN5000project/Data/01-modified-data/NewYorkTimesAPI.csv.\nCode: https://github.com/anly501/dsan-5000-project-Sel272/blob/main/DSAN5000project/DataCleaning/datagcleaningAPI_R.Rmd.\n\n\n\n\nTo clean and preprocess data using Python, start by creating a datacleaning_python and datacleaningAPI_R file. Within this script, load your dataset into a pandas DataFrame, standardize column names to lowercase and replace spaces with underscores for uniformity. If the data includes text columns, use the CountVectorizer from scikit-learn to convert this text into numerical format. Finally, save the cleaned and transformed data back to a CSV file, ensuring it’s primed for subsequent analysis or modeling.\n\nPython API Word Frequency in Text Description\n\n\n\n\ndescription_freq\n\n\n\nPython API Word Frequency in Text Title\n\n\n\n\ntitle_freq",
    "crumbs": [
      "Data Cleaning"
    ]
  },
  {
    "objectID": "NaiveBayes/naivebayes.html",
    "href": "NaiveBayes/naivebayes.html",
    "title": "Naïve Bayes",
    "section": "",
    "text": "Naive Bayes is a probabilistic classification algorithm anchored in Bayes’ theorem. It calculates the likelihood of a class based on given features, operating under the simple yet often effective assumption that all features are independent. This “naive” approach aims to determine the most probable category for a data instance, leveraging the theorem’s mathematical principles for both its simplicity and accuracy.\nIn this project, the primary objective is to leverage the Naive Bayes classification method to discern patterns related to cardiovascular risk. Given its foundation in Bayes’ theorem, the algorithm will provide a probabilistic understanding of the data, potentially offering a unique perspective on the complex interplay of factors leading to cardiovascular ailments. By analyzing the individual and combined effects of various health metrics and lifestyle choices, we aim to develop a robust model that can predict cardiovascular risk with a high degree of accuracy.\nVariants of Naive Bayes\nThere are several flavors of Naive Bayes, tailored for different types of data:\n\nGaussian Naive Bayes: Ideal for handling continuous data, this variant assumes that data for each class is distributed according to a Gaussian (normal) distribution. For data that appears to follow a normal distribution, Gaussian Naive Bayes is typically preferred.\nMultinomial Naive Bayes: Best suited for features representing counts or discrete frequency data, it’s often the go-to for text classification tasks. When working with count data, such as word frequencies in text, Multinomial Naive Bayes shines.\nBernoulli Naive Bayes: Tailored for binary data, this approach models features using the Bernoulli distribution. For binary or Boolean features, Bernoulli Naive Bayes is often the best choice.",
    "crumbs": [
      "Naïve Bayes"
    ]
  },
  {
    "objectID": "NaiveBayes/naivebayes.html#introduction-to-naive-bayes",
    "href": "NaiveBayes/naivebayes.html#introduction-to-naive-bayes",
    "title": "Naïve Bayes",
    "section": "",
    "text": "Naive Bayes is a probabilistic classification algorithm anchored in Bayes’ theorem. It calculates the likelihood of a class based on given features, operating under the simple yet often effective assumption that all features are independent. This “naive” approach aims to determine the most probable category for a data instance, leveraging the theorem’s mathematical principles for both its simplicity and accuracy.\nIn this project, the primary objective is to leverage the Naive Bayes classification method to discern patterns related to cardiovascular risk. Given its foundation in Bayes’ theorem, the algorithm will provide a probabilistic understanding of the data, potentially offering a unique perspective on the complex interplay of factors leading to cardiovascular ailments. By analyzing the individual and combined effects of various health metrics and lifestyle choices, we aim to develop a robust model that can predict cardiovascular risk with a high degree of accuracy.\nVariants of Naive Bayes\nThere are several flavors of Naive Bayes, tailored for different types of data:\n\nGaussian Naive Bayes: Ideal for handling continuous data, this variant assumes that data for each class is distributed according to a Gaussian (normal) distribution. For data that appears to follow a normal distribution, Gaussian Naive Bayes is typically preferred.\nMultinomial Naive Bayes: Best suited for features representing counts or discrete frequency data, it’s often the go-to for text classification tasks. When working with count data, such as word frequencies in text, Multinomial Naive Bayes shines.\nBernoulli Naive Bayes: Tailored for binary data, this approach models features using the Bernoulli distribution. For binary or Boolean features, Bernoulli Naive Bayes is often the best choice.",
    "crumbs": [
      "Naïve Bayes"
    ]
  },
  {
    "objectID": "NaiveBayes/naivebayes.html#prepare-your-data-for-naive-bayes",
    "href": "NaiveBayes/naivebayes.html#prepare-your-data-for-naive-bayes",
    "title": "Naïve Bayes",
    "section": "Prepare your Data for Naive Bayes",
    "text": "Prepare your Data for Naive Bayes\nTo successfully implement Naive Bayes classification, labeled text or recorded data relevant to the project is crucial. After obtaining and carefully cleaning this data set, it must be divided into training, validation, and test subsets. This policy partitioning plays a fundamental role in machine learning. The training set is used to build the model, the validation set aids in refining the model through hyperparameter tuning, and the test set, serving as an unseen dataset, assesses its effectiveness by comparing predicted outcomes with actual results. Such partitioning ensures that the model actually recognizes patterns and does not just memorize the data, or called overfitting. This approach enhances the model’s resilience and credibility when new, unfamiliar data is introduced.",
    "crumbs": [
      "Naïve Bayes"
    ]
  },
  {
    "objectID": "NaiveBayes/naivebayes.html#feature-selection",
    "href": "NaiveBayes/naivebayes.html#feature-selection",
    "title": "Naïve Bayes",
    "section": "Feature selection",
    "text": "Feature selection\nFeature selection is vital for the success of a data science project, aiming to pinpoint the most relevant dataset attributes. Proper feature selection boosts model performance, minimizes overfitting, and clarifies result interpretation.\n\nRecord Data\nFor record data we define the following metric for the feature selection process.\n\\[ \\mathrm {Merit} _{S_{k}}={\\frac {k|{\\overline {r_{cf}}|}}{\\sqrt {k+k(k-1)|{\\overline {r_{ff}}}|}}}  \\]\n\\(\\overline{r_{xy}}\\) is the average value of all input-output (feature-target) Spearman correlations\n\\(\\overline{r_{xx}}\\) is the average value of all input-input (feature-feature) Spearman correlations\nNote: The vertical lines denote absolute values\nMerit refers to a score or measure that quantifies the quality or importance of a subset of features. It’s a way to evaluate how well a particular subset of features might perform in a predictive model.\nI iterated through all possible subsets of feature variables and calculated the Merit information for each one. The subset with the highest Merit was then selected. Given the relatively high dimension of features, considering all potential subset combinations in one loop would be cumbersome. As a solution, I divided the search into two parts: first, iterating through the initial six features, and then separately looping through the remaining six features.\n\n\nCode\nimport pandas as pd\ncardio = pd.read_csv(\"../Data/01-modified-data/cardiovascular_numeric_final.csv\")\n\ncardio = cardio.drop(columns=['Unnamed: 0', 'id','cardio'])\ncardio.head()\n\n\n\n\n\n\n\n\n\nage\ngender\nheight\nweight\nap_hi\nap_lo\ncholesterol\ngluc\nsmoke\nalco\nactive\nbmi\n\n\n\n\n0\n50\n2\n168\n62\n110\n80\n1\n1\n0\n0\n1\n21.97\n\n\n1\n55\n1\n156\n85\n140\n90\n3\n1\n0\n0\n1\n34.93\n\n\n2\n52\n1\n165\n64\n130\n70\n3\n1\n0\n0\n0\n23.51\n\n\n3\n48\n2\n169\n82\n150\n100\n1\n1\n0\n0\n1\n28.71\n\n\n4\n48\n1\n156\n56\n100\n60\n1\n1\n0\n0\n0\n23.01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 1\n\n\n\n\n\n\n\nPart 2\n\n\n\n\n\nBased on the graphs presented earlier, features 1, 5, 6, 7, and 12 have been selected according to the Merit selection results. These features correspond to the subsequent variables.\n\n\nCode\nimport pandas as pd\ncardio = pd.read_csv(\"../Data/01-modified-data/cardiovascular_numeric_final.csv\")\n\ncardio = cardio.drop(columns=['Unnamed: 0', 'id','cardio'])\ncardio = cardio.iloc[:,[0,4,5,6,11]]\ncardio.head()\n\n\n\n\n\n\n\n\n\nage\nap_hi\nap_lo\ncholesterol\nbmi\n\n\n\n\n0\n50\n110\n80\n1\n21.97\n\n\n1\n55\n140\n90\n3\n34.93\n\n\n2\n52\n130\n70\n3\n23.51\n\n\n3\n48\n150\n100\n1\n28.71\n\n\n4\n48\n100\n60\n1\n23.01\n\n\n\n\n\n\n\n\n\nText Data\n\n\n\n\n\n\n\n\n\nAccuracy vs Number of features\n\n\n\n\n\n\n\nRuntime vs Number of features\n\n\n\n\n\n\n\n\n\nRuntime vs Test Accuracies\n\n\n\n\n\n\n\nTrain-Test Accuracies vs Number of features\n\n\n\n\n\nIn analyzing the text data, I begin by using a whitespace tokenizer to segment the text into distinct words and then convert them to lowercase to ensure uniformity during training. The corpus contains 3622 unique words. The training matrix for the x variable is converted into a binary format with dimensions (400, 3622). Here, each column signifies a distinct word, with its values—0 or 1—indicating absence or presence, respectively.\nDuring the feature selection process, I iterated over the training matrix, examining all words across 100 iterations, each encompassing up to 1000 features. Although the total features count to 3622, I capped the maximum at 1000 for feature selection purposes. My selection focused on the features delivering the highest test accuracy. Settling on the top 100 features, I achieved a test accuracy of 76.25%. In the subsequent sections detailing model prediction results, these 100 features will be the basis for training.\n\n\n\nText World Cloud",
    "crumbs": [
      "Naïve Bayes"
    ]
  },
  {
    "objectID": "NaiveBayes/naivebayes.html#naïve-bayes-nb-with-labeled-record-data",
    "href": "NaiveBayes/naivebayes.html#naïve-bayes-nb-with-labeled-record-data",
    "title": "Naïve Bayes",
    "section": "Naïve Bayes (NB) with Labeled Record Data",
    "text": "Naïve Bayes (NB) with Labeled Record Data\nThe dataset is divided into a training set and a test set using an 80/20 split. This division helps mitigate the risk of overfitting by not training the model on the entire dataset and provides a separate unseen test set to evaluate model performance. Since there were no hyperparameter tuning steps for my Naive Bayes Model, a validation dataset was not created during the data splitting step.\nOverfitting and underfitting are prevalent challenges in the model training process, especially in supervised learning. Underfitting occurs when a model is too simplistic, failing to capture significant patterns within the dataset. This results in high bias and low variance, typically leading to subpar accuracy on both training and test data. Conversely, overfitting arises when the model is excessively complex, perhaps perfectly fitting the training data but faltering on new, unseen data. Such models often exhibit high training accuracy but poor test accuracy. From a bias-variance tradeoff perspective, overfit models exhibit low bias but high variance, as they may not generalize well to different datasets.\nIn my model, I carried out feature selection to mitigate the issues of overfitting and underfitting. I chose the features that offer the most information related to the response variable “cardio”. The optimal features selected are “age”, “ap_hi”, “ap_lo”, “cholesterol”, and “bmi”.\nThe Multinomial Naive Bayes model was utilized for this analysis. I trained the model using the training dataset and subsequently made predictions on the test set. These predictions were then compared with the actual labels to derive performance metrics, including accuracy and F1-score.\nI achieved the following accuracy and F1-score values for the training and test data.\n\n\n\nAccuracy & F1-score\n\n\nBoth the training and test accuracies are modest, averaging around 60%. For a binary classification, even a random guess could achieve about 50% accuracy, highlighting the limited performance of the Naive Bayes model on this dataset. The F1-scores for the training and test sets hover around 0.528, a value considerably distant from the ideal score of 1 for high-performing models. Given the modest accuracy observed for both training and test data, it’s unlikely that the model experienced either overfitting or underfitting during the training process.\nHere are the confusion matrices derived from both the training and test sets. 1 is defined as positive.\n\n\n\n\n\n\n\n\n\nConfusion Matrix for training dataset\n\n\n\n\n\n\n\nConfusion Matrix for test dataset\n\n\n\n\n\nFrom the training confusion matrix,\n\\(precision = \\frac{TP}{TP + FP} = \\frac{11427}{11427 + 6967} = 0.6212\\)\n\\(recall = \\frac{TP}{TP + FN} = \\frac{11427}{11427 + 13490} = 0.4586\\)\nFrom the test confusion matrix,\n\\(precision = \\frac{TP}{TP + FP} = \\frac{2919}{2919 + 1811} = 0.6171\\)\n\\(recall = \\frac{TP}{TP + FN} = \\frac{2919}{2919 + 3393} = 0.4624\\)\nPrecision quantifies the number of correctly predicted positive instances relative to the total number of predicted positives. Recall, alternatively termed Sensitivity or True Positive Rate, quantifies how many of the true positive instances were successfully predicted. In the context of my research, the primary aim is to predict cardiovascular diseases. Consequently, it’s crucial to minimize False Positives, representing individuals diagnosed with cardiovascular diseases but incorrectly predicted otherwise. The recall values for both training and test datasets hover around 0.46, suggesting that nearly half of the true positives are overlooked.\nIn summary, the performance of the Naive Bayes model on the record data falls short, particularly in terms of recall and accuracy. The selected features - “age,” “ap_hi,” “ap_lo,” “cholesterol,” and “bmi” - appear inadequate for effective cardiovascular disease prediction. Given this model’s underwhelming metrics, exploring alternative machine learning techniques for better model construction is recommended. While we will retain the Naive Bayes results for benchmarking purposes, they will likely be referenced in the Naive Bayes section of any forthcoming research papers. Ultimately, a comparative table will feature these metrics alongside those of other models, enabling a more comprehensive assessment.",
    "crumbs": [
      "Naïve Bayes"
    ]
  },
  {
    "objectID": "NaiveBayes/naivebayes.html#naïve-bayes-nb-with-labeled-text-data",
    "href": "NaiveBayes/naivebayes.html#naïve-bayes-nb-with-labeled-text-data",
    "title": "Naïve Bayes",
    "section": "Naïve Bayes (NB) with Labeled Text Data",
    "text": "Naïve Bayes (NB) with Labeled Text Data\nFor the text data, I possess a sample of 400 records. I’ve partitioned this dataset into training and test sets, maintaining an 80/20 split, akin to the record data. The model, trained on this training set, is tasked with predicting whether the topic pertains to cardiovascular disease. I’ve opted for the Multinomial Naive Bayes model given its proven prowess in text classification tasks. The selected features consist of the first 100 words from the entire corpus of 3,622 words.\nI achieved the following accuracy and F1-score values for the training and test data.\n\n\n\nAccuracy & F1-score\n\n\nThe model achieved a training accuracy of 0.91875 and a test accuracy of 0.6875. The notable disparity between these accuracies suggests that the model may be overfitting the training data. This overfitting could be attributed to the diverse nature of the text data and the limited dataset size. Increasing the number of data records in the future could mitigate this issue.\nComparatively, the test accuracy for text data is nearly 70%, markedly higher than that of the record data. This could highlight the inherent strength of the Multinomial Naive Bayes model in text classification tasks. However, the F1-scores provide a more nuanced view: with a training F1-score of 0.8375 and a test F1-score of 0.49, the model’s performance still remains suboptimal, especially considering that the test F1-score is even lower than that for record data.\n\n\n\n\n\n\n\n\n\nConfusion Matrix for training dataset\n\n\n\n\n\n\n\nConfusion Matrix for test dataset\n\n\n\n\n\nFrom the training confusion matrix,\n\\(precision = \\frac{TP}{TP + FP} = \\frac{67}{67 + 16} = 0.8072\\)\n\\(recall = \\frac{TP}{TP + FN} = \\frac{67}{67 + 10} = 0.8701\\)\nFrom the test confusion matrix,\n\\(precision = \\frac{TP}{TP + FP} = \\frac{12}{12 + 14} = 0.4615\\)\n\\(recall = \\frac{TP}{TP + FN} = \\frac{12}{12 + 11} = 0.5217\\)\nUpon analyzing the precision and recall scores, it’s evident that the scores for the training data substantially outpace those for the test data. This further highlights the previously noted overfitting issue. Specifically, the test data’s precision is 0.4615, falling below the halfway mark. This implies that a significant portion of the positives predicted are erroneous. Although the recall score fares slightly better at 0.5217, both these scores emphasize the model’s limited efficacy on the test data. Notably, the recall consistently surpasses the precision for both datasets, aligning with our research’s central objective of precisely identifying all true positives.\nGiven the nature of our research, the emphasis will remain on record data, relegating text data to a secondary role. The inherent structure of text data makes its results more challenging to decipher. Any significant findings pertaining to this will be documented in the Naive Bayes section, should this research culminate in a publication.\nTo sum up, while the Naive Bayes model for text data displays a commendable test accuracy, it grapples with overfitting, as evidenced by the stark performance disparity between training and test datasets. Examining the precision and recall metrics—both hovering between 50% and 60%—underscores the model’s underwhelming performance. Since the datasets for text and record data are intrinsically distinct, direct comparisons might not yield substantial insights. Moving forward, our research will primarily concentrate on the record data.",
    "crumbs": [
      "Naïve Bayes"
    ]
  },
  {
    "objectID": "NaiveBayes/NB_Text.html",
    "href": "NaiveBayes/NB_Text.html",
    "title": "Research Project",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport os\nimport shutil\n\n#OUTPUT FOLDER: START FRESH (DELETE OLD ONE IF EXISTS)\noutput_dir = \"../Image/Naive_Bayes_Text/\"\nif os.path.exists(output_dir) and os.path.isdir(output_dir):\n    shutil.rmtree(output_dir)\nos.mkdir(output_dir)\n\n\ndf=pd.read_csv(\"../Data/01-modified-data/cardiovascular_text_final.csv\")\ndf = df[['text','label']]\nprint(df.shape)\nprint(df.columns)\n\n(400, 2)\nIndex(['text', 'label'], dtype='object')\n\n\n\ntext=[]\ny=[]\ndf['text'] = df['text'].astype(str)\n#ITERATE OVER ROWS\n# for i in range(0,10):  \nfor i in range(0,df.shape[0]):\n    # QUICKLY CLEAN TEXT\n    keep=\"abcdefghijklmnopqrstuvwxyz0123456789 \"\n    replace=\".,!;\"\n    tmp=\"\"\n    for char in df['text'][i].replace(\"&lt;br /&gt;\",\"\").lower():\n        if char in replace:\n            tmp+=\" \"\n        if char in keep:\n            tmp+=char\n    tmp=\" \".join(tmp.split())\n    text.append(tmp)\n    # CONVERT STRINGS TO INT TAGS\n    if(df[\"label\"][i]==1):\n        y.append(1)\n    if(df[\"label\"][i]==0):\n        y.append(0)\n\n    #PRINT FIRST COUPLE REVIEWS\n    if(i&lt;3):\n        print(i)\n        print(df[\"text\"][i].replace(\"&lt;br /&gt;\",\"\"),'\\n')\n        print(tmp)\n        print(df[\"label\"][i],y[i])\n\n0\nresearchers increasingly find that the effects of infection by sars-cov-2 extend to the cardiovascular system \n\nresearchers increasingly find that the effects of infection by sarscov2 extend to the cardiovascular system\n1 1\n1\ntogether, ai imaging and ai genetic analysis may be able to help doctors rapidly pinpoint a diagnosis and create a highly personalized treatment plan \n\ntogether ai imaging and ai genetic analysis may be able to help doctors rapidly pinpoint a diagnosis and create a highly personalized treatment plan\n1 1\n2\nlas enfermedades relacionadas con nuestro sistema cardiovascular se han convertido en el mayor riesgo para nuestra salud, especialmente conforme nuestra edad avanza monitorizar esta salud implica vigilar muchos factores, pero mantener alguno de estos factore… \n\nlas enfermedades relacionadas con nuestro sistema cardiovascular se han convertido en el mayor riesgo para nuestra salud especialmente conforme nuestra edad avanza monitorizar esta salud implica vigilar muchos factores pero mantener alguno de estos factore\n1 1\n\n\n\n# CONVERT Y TO NUMPY ARRAY\ny=np.array(y)\n\n\n#DOUBLE CHECK SIZE\nprint(len(text),len(y))\n\n400 400\n\n\n\n# PARAMETERS TO CONTROL SIZE OF FEATURE SPACE WITH COUNT-VECTORIZER\n# minDF = 0.01 means \"ignore terms that appear in less than 1% of the documents\". \n# minDF = 5 means \"ignore terms that appear in less than 5 documents\".\n# max_features=int, default=None\n#   If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef vectorize(corpus,MAX_FEATURES):\n    vectorizer=CountVectorizer(max_features=MAX_FEATURES,stop_words=\"english\")   \n    # RUN COUNT VECTORIZER ON OUR COURPUS \n    Xs  =  vectorizer.fit_transform(corpus)   \n    X=np.array(Xs.todense())\n    #CONVERT TO ONE-HOT VECTORS (can also be done with binary=true in CountVectorizer)\n    maxs=np.max(X,axis=0)\n    return (np.ceil(X/maxs),vectorizer.vocabulary_)\n\n(x,vocab0)=vectorize(text,MAX_FEATURES=10000)\n\n\n# DOUBLE CHECK SHAPES\nprint(x.shape,y.shape)\n\n(400, 3622) (400,)\n\n\n\n#swap keys and values (value --&gt; ley)\nvocab1 = dict([(value, key) for key, value in vocab0.items()])\n\n\n# CHECK VOCAB KEY-VALUE PAIRS\nprint(list(vocab1.keys())[0:10])\nprint(list(vocab1.values())[0:10])\n\n[2732, 1638, 1058, 1649, 2826, 1219, 522, 172, 1607, 1393]\n['researchers', 'increasingly', 'effects', 'infection', 'sarscov2', 'extend', 'cardiovascular', 'ai', 'imaging', 'genetic']\n\n\n\n# CHECK TO SEE IF COUNT-VECT COLUMNS ARE SORTED BY OCCURRENCE \nprint(x.sum(axis=0))\n\n[5. 1. 4. ... 1. 1. 1.]\n\n\n\n#RE-ORDER COLUMN SO IT IS SORTED FROM HIGH FREQ TERMS TO LOW \n# https://stackoverflow.com/questions/60758625/sort-pandas-dataframe-by-sum-of-columns\ndf2=pd.DataFrame(x)\ns = df2.sum(axis=0)\ndf2=df2[s.sort_values(ascending=False).index[:]]\nprint(df2.head())\n\n   497   914   2179  1510  3119  1798  3130  522   1094  2604  ...  1498  \\\n0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   \n1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   0.0  ...   0.0   \n3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   \n4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n\n   126   1490  1491  1492  1493  1494  1496  1497  3621  \n0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n\n[5 rows x 3622 columns]\n\n\n\n# REMAP DICTIONARY TO CORRESPOND TO NEW COLUMN NUMBERS\nprint()\ni1=0\nvocab2={}\nfor i2 in list(df2.columns):\n    # print(i2)\n    vocab2[i1]=vocab1[int(i2)]\n    i1+=1\n\n#DOUBLE CHECK \nprint(vocab2[0],vocab1[497])\nprint(vocab2[1],vocab1[914])\n\n\ncancer cancer\ndiabetes diabetes\n\n\n\n# RENAME COLUMNS 0,1,2,3 .. \ndf2.columns = range(df2.columns.size)\nprint(df2.head())\nprint(df2.sum(axis=0))\nx=df2.to_numpy()\n\n   0     1     2     3     4     5     6     7     8     9     ...  3612  \\\n0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   \n1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   0.0  ...   0.0   \n3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   \n4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n\n   3613  3614  3615  3616  3617  3618  3619  3620  3621  \n0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n\n[5 rows x 3622 columns]\n0       59.0\n1       46.0\n2       37.0\n3       37.0\n4       30.0\n        ... \n3617     1.0\n3618     1.0\n3619     1.0\n3620     1.0\n3621     1.0\nLength: 3622, dtype: float64\n\n\n\n# DOUBLE CHECK \nprint(x.shape,y.shape)\n\n(400, 3622) (400,)\n\n\n\nimport random\nrandom.seed(123)\nN=x.shape[0]\nl = [*range(N)]     # indices\ncut = int(0.8 * N) #80% of the list\nrandom.shuffle(l)   # randomize\ntrain_index = l[:cut] # first 80% of shuffled list\ntest_index = l[cut:] # last 20% of shuffled list\n\nprint(train_index[0:10])\nprint(test_index[0:10])\n\n[179, 121, 270, 53, 166, 146, 162, 25, 85, 108]\n[37, 200, 283, 383, 171, 87, 134, 249, 42, 92]\n\n\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport time\n\ndef train_MNB_model(X,Y,i_print=False):\n\n    if(i_print):\n        print(X.shape,Y.shape)\n\n    #SPLIT\n    x_train=X[train_index]\n    y_train=Y[train_index].flatten()\n\n    x_test=X[test_index]\n    y_test=Y[test_index].flatten()\n\n    # INITIALIZE MODEL \n    model = MultinomialNB()\n\n    # TRAIN MODEL \n    start = time.process_time()\n    model.fit(x_train,y_train)\n    time_train=time.process_time() - start\n\n    # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n    start = time.process_time()\n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n    time_eval=time.process_time() - start\n\n    acc_train= accuracy_score(y_train, yp_train)*100\n    acc_test= accuracy_score(y_test, yp_test)*100\n\n    if(i_print):\n        print(acc_train,acc_test,time_train,time_eval)\n\n    return (acc_train,acc_test,time_train,time_eval)\n\n\n#TEST\nprint(type(x),type(y))\nprint(x.shape,y.shape)\n(acc_train,acc_test,time_train,time_eval)=train_MNB_model(x,y,i_print=True)\n\n&lt;class 'numpy.ndarray'&gt; &lt;class 'numpy.ndarray'&gt;\n(400, 3622) (400,)\n(400, 3622) (400,)\n95.3125 55.00000000000001 0.011404000000005965 0.003239000000000658\n\n\n\n##UTILITY FUNCTION TO INITIALIZE RELEVANT ARRAYS\ndef initialize_arrays():\n    global num_features,train_accuracies\n    global test_accuracies,train_time,eval_time\n    num_features=[]\n    train_accuracies=[]\n    test_accuracies=[]\n    train_time=[]\n    eval_time=[]\n\n\n# INITIALIZE ARRAYS\ninitialize_arrays()\n\n# DEFINE SEARCH FUNCTION\ndef partial_grid_search(num_runs, min_index, max_index):\n    for i in range(1, num_runs+1):\n        # SUBSET FEATURES \n        upper_index=min_index+i*int((max_index-min_index)/num_runs)\n        xtmp=x[:,0:upper_index]\n\n        #TRAIN \n        (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtmp,y,i_print=False)\n\n        if(i%5==0):\n            print(i,upper_index,xtmp.shape[1],acc_train,acc_test)\n            \n        #RECORD \n        num_features.append(xtmp.shape[1])\n        train_accuracies.append(acc_train)\n        test_accuracies.append(acc_test)\n        train_time.append(time_train)\n        eval_time.append(time_eval)\n\n# DENSE SEARCH (SMALL NUMBER OF FEATURES (FAST))\npartial_grid_search(num_runs=100, min_index=0, max_index=1000)\n\n# SPARSE SEARCH (LARGE NUMBER OF FEATURES (SLOWER))\n# partial_grid_search(num_runs=20, min_index=1000, max_index=10000)\n\n5 50 50 81.875 75.0\n10 100 100 83.4375 76.25\n15 150 150 85.9375 72.5\n20 200 200 84.6875 71.25\n25 250 250 86.5625 70.0\n30 300 300 86.5625 67.5\n35 350 350 88.4375 67.5\n40 400 400 88.75 68.75\n45 450 450 89.0625 67.5\n50 500 500 88.75 68.75\n55 550 550 89.6875 65.0\n60 600 600 89.6875 68.75\n65 650 650 90.3125 67.5\n70 700 700 90.3125 67.5\n75 750 750 90.9375 70.0\n80 800 800 91.5625 72.5\n85 850 850 92.1875 70.0\n90 900 900 91.875 70.0\n95 950 950 91.875 68.75\n100 1000 1000 91.875 68.75\n\n\n\n#UTILITY FUNCTION TO SAVE RESULTS\ndef save_results(path_root):\n    out=np.transpose(np.array([num_features,train_accuracies,test_accuracies,train_time,eval_time])) \n    out=pd.DataFrame(out)\n    out.to_csv(path_root+\".csv\")\n\n\n#UTILITY FUNCTION TO PLOT RESULTS\ndef plot_results(path_root):\n\n    #PLOT-1\n    plt.plot(num_features,train_accuracies,'-or')\n    plt.plot(num_features,test_accuracies,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('ACCURACY: Training (red) and Test (blue)')\n    plt.savefig(path_root+'-1.png')\n    plt.show()\n\n    # #PLOT-2\n    plt.plot(num_features,train_time,'-or')\n    plt.plot(num_features,eval_time,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('Runtime: training time (red) and evaluation time(blue)')\n    plt.savefig(path_root+'-2.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(np.array(test_accuracies),train_time,'-or')\n    plt.plot(np.array(test_accuracies),eval_time,'-ob')\n    plt.xlabel('test_accuracies')\n    plt.ylabel('Runtime: training time (red) and evaluation time (blue)')\n    plt.savefig(path_root+'-3.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(num_features,np.array(train_accuracies)-np.array(test_accuracies),'-or')\n    plt.xlabel('Number of features')\n    plt.ylabel('train_accuracies-test_accuracies')\n    plt.savefig(path_root+'-4.png')\n    plt.show()\n\n\nsave_results(output_dir+\"/partial_grid_search\")\nplot_results(output_dir+\"/partial_grid_search\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#Select the highest test accuracy\nmax_index = test_accuracies.index(max(test_accuracies))\nnum_features[max_index]\n\n100\n\n\n\nxtmp=x[:,0:1000]\nX_train = xtmp[train_index]\ny_train = y[train_index].flatten()\nX_test = xtmp[test_index]\ny_test = y[test_index].flatten()\n\n\ndef train_MNB_model_pred(X,Y,i_print=False):\n\n    if(i_print):\n        print(X.shape,Y.shape)\n\n    #SPLIT\n    x_train=X[train_index]\n    y_train=Y[train_index].flatten()\n\n    x_test=X[test_index]\n    y_test=Y[test_index].flatten()\n\n    # INITIALIZE MODEL \n    model = MultinomialNB()\n\n    # TRAIN MODEL \n    model.fit(x_train,y_train)\n\n    # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n\n    return (yp_train,yp_test)\n\n\nfrom sklearn import metrics\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\ndef get_metrics(true, preds):\n    \"\"\"\n    Takes gold labels and predictions to compute performance metrics\n    :param true: array-like object\n    :param preds: array-like object\n    :return: a tuple of various performance metrics\n    \"\"\"\n    accuracy = metrics.accuracy_score(true,preds)\n    f1_score = metrics.f1_score(true,preds)\n    conf_matrix = metrics.confusion_matrix(true,preds)\n\n    return accuracy, f1_score, conf_matrix\n\n\nimport seaborn as sns\ndef plot_confusion_matrix(conf_matrix_data, labels, data=\"train\"):\n    \"\"\"\n    Takes as input confusion matrix data from get_metrics() and prints out a\n    confusion matrix\n    :param conf_matrix_data:\n    :return: None\n    \"\"\"\n    if data == \"train\":\n        plt.title(\"Training Confusion Matrix\")\n    else:\n        plt.title(\"Test Confusion Matrix\")\n    axis = sns.heatmap(conf_matrix_data,annot=True,fmt=\"d\")\n    axis.set_xticklabels(labels)\n    axis.set_yticklabels(labels)\n    axis.set_xlabel(\"predicted\")\n    axis.set_ylabel(\"true\")\n    plt.show()\n    return\n\n\n\ny_pred_train,y_pred_test = train_MNB_model_pred(xtmp, y)\nacc_train, f1_train, conf_train = get_metrics(y_train, y_pred_train)\nacc_test, f1_test, conf_test = get_metrics(y_test, y_pred_test)\nprint(\"Training Accuracy:\", acc_train)\nprint(\"Test Accuracy:\", acc_test)\nprint(\"Training F1 Score:\", f1_train)\nprint(\"Test F1 Score:\", f1_test)\n\nTraining Accuracy: 0.91875\nTest Accuracy: 0.6875\nTraining F1 Score: 0.8375\nTest F1 Score: 0.4897959183673469\n\n\n\nplot_confusion_matrix(conf_train, [0, 1],data=\"train\")\n\n\n\n\n\n\n\n\n\nplot_confusion_matrix(conf_test, [0, 1],data=\"test\")"
  },
  {
    "objectID": "DataExploration/dataexploration.html",
    "href": "DataExploration/dataexploration.html",
    "title": "Data Exploration",
    "section": "",
    "text": "Data understanding is a fundamental and critical step in the field of data science and analytics. It serves as the foundation upon which all subsequent data-related activities are built. This process involves the exploration, examination, and comprehension of the data at hand, enabling organizations and individuals to derive valuable insights, make informed decisions, and solve complex problems. Moreover, data understanding allows us to uncover patterns, trends, and relationships.\nThe original dataset comprises 70,000 patient records, containing 11 predictive variables (features) and 1 response variable (target). Body Mass Index (BMI) has been included in the dataset due to its significance as a factor that can be used to estimate an individual’s body fat and, by extension, assess their health in relation to their weight. It serves as a valuable tool for quickly evaluating health risks associated with weight. Therefore, we now have 12 features.\n\nAge | Objective Feature | age | int (days)\nHeight | Objective Feature | height | int (cm) |\nWeight | Objective Feature | weight | float (kg) |\nGender | Objective Feature | gender | categorical code |\nSystolic blood pressure | Examination Feature | ap_hi | int |\nDiastolic blood pressure | Examination Feature | ap_lo | int |\nCholesterol | Examination Feature | cholesterol | 1: normal, 2: above normal, 3: well above normal |\nGlucose | Examination Feature | gluc | 1: normal, 2: above normal, 3: well above normal |\nSmoking | Subjective Feature | smoke | binary |\nAlcohol intake | Subjective Feature | alco | binary |\nPhysical activity | Subjective Feature | active | binary |\nBody Mass Index | Objective Feature | bmi | float (kg/m²)|\nPresence or absence of cardiovascular disease | Target Variable | cardio | binary |\n\nThe problem I am addressing involves understanding the potential risk factors associated with cardiovascular diseases.\nIn the dataset, there are six numerical features: Age, Height, Weight, Systolic blood pressure (ap_hi), Diastolic blood pressure (ap_lo), and Body Mass Index (BMI). Additionally, there are seven categorical features: Gender, Cholesterol, Glucose, Smoking, Alcohol intake, Physical activity, and the presence or absence of cardiovascular disease (the Target Variable).\nIdentifying potential relationships and their relevance to the project’s objectives is a crucial step in data analysis and modeling. These relationships will be explored through visualization methods, such as correlation matrices, histograms, scatterplots, etc.\n\n\nCode\nimport pandas  as  pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ncardio = pd.read_csv('../Data/01-modified-data/cardio.csv')\ncardio['bmi'] = round(cardio['weight'] / ((cardio['height']/100) ** 2),2)\ncardio.head()\n\n\n\n\n\n\n\n\n\nid\nage\ngender\nheight\nweight\nap_hi\nap_lo\ncholesterol\ngluc\nsmoke\nalco\nactive\ncardio\nbmi\n\n\n\n\n0\n0\n50\n2\n168\n62\n110\n80\n1\n1\n0\n0\n1\n0\n21.97\n\n\n1\n1\n55\n1\n156\n85\n140\n90\n3\n1\n0\n0\n1\n1\n34.93\n\n\n2\n2\n52\n1\n165\n64\n130\n70\n3\n1\n0\n0\n0\n1\n23.51\n\n\n3\n3\n48\n2\n169\n82\n150\n100\n1\n1\n0\n0\n1\n1\n28.71\n\n\n4\n4\n48\n1\n156\n56\n100\n60\n1\n1\n0\n0\n0\n0\n23.01\n\n\n\n\n\n\n\n\n\n\nDescriptive statistics serve as a fundamental tool for understanding and summarizing data, offering valuable insights into the characteristics of a dataset. In this analysis, the focus is on both numerical and categorical variables, each demanding specific methods to reveal crucial details.\nThe statistical summary report for numerical variables:\n\n\nCode\nimport pandas  as  pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ncardio = pd.read_csv('../Data/01-modified-data/cardio.csv')\ncardio['bmi'] = round(cardio['weight'] / ((cardio['height']/100) ** 2),2)\ncardio.head()\n\n# Select numerical columns\nnumerical_cols = [\"age\", \"height\", \"weight\", \"ap_hi\", \"ap_lo\", \"bmi\"]\n\nstats = {\n    'Mean': cardio[numerical_cols].mean(),\n    'Median': cardio[numerical_cols].median(),\n    'Mode': cardio[numerical_cols].mode().iloc[0],\n    'Standard Deviation': cardio[numerical_cols].std(),\n    'Variance': cardio[numerical_cols].var()\n}\n\nstats_df = pd.DataFrame(stats)\nprint(stats_df)\n\n\n              Mean   Median    Mode  Standard Deviation      Variance\nage      53.338686   54.000   56.00            6.765294     45.769203\nheight  164.359229  165.000  165.00            8.210126     67.406175\nweight   74.205543   72.000   65.00           14.395829    207.239884\nap_hi   128.817286  120.000  120.00          154.011419  23719.517323\nap_lo    96.630414   80.000   80.00          188.472530  35521.894676\nbmi      27.556502   26.375   23.88            6.091795     37.109965\n\n\n\n\n\n\n\n\n\n\n\nMean\n\n\n\n\n\n\n\nMedian\n\n\n\n\n\n\n\nMode\n\n\n\n\n\n\n\n\n\nStandard Deviation\n\n\n\n\n\n\n\nVariance\n\n\n\n\n\nFor categorical variables, frequency distributions and bar charts are used to visualize data distribution:\n\n\nCode\nimport pandas  as  pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ncardio = pd.read_csv('../Data/01-modified-data/cardio.csv')\ncardio['bmi'] = round(cardio['weight'] / ((cardio['height']/100) ** 2),2)\ncardio.head()\n\ncategorical_vars = [\"gender\", \"cholesterol\", \"gluc\", \"smoke\", \"alco\", \"active\", \"cardio\"]\n\nfor var in categorical_vars:\n    print(f\"Frequency distribution for {var}:\\n\")\n    print(cardio[var].value_counts())\n    print(\"\\n\" + \"=\"*50 + \"\\n\")\n\n\nFrequency distribution for gender:\n\ngender\n1    45530\n2    24470\nName: count, dtype: int64\n\n==================================================\n\nFrequency distribution for cholesterol:\n\ncholesterol\n1    52385\n2     9549\n3     8066\nName: count, dtype: int64\n\n==================================================\n\nFrequency distribution for gluc:\n\ngluc\n1    59479\n3     5331\n2     5190\nName: count, dtype: int64\n\n==================================================\n\nFrequency distribution for smoke:\n\nsmoke\n0    63831\n1     6169\nName: count, dtype: int64\n\n==================================================\n\nFrequency distribution for alco:\n\nalco\n0    66236\n1     3764\nName: count, dtype: int64\n\n==================================================\n\nFrequency distribution for active:\n\nactive\n1    56261\n0    13739\nName: count, dtype: int64\n\n==================================================\n\nFrequency distribution for cardio:\n\ncardio\n0    35021\n1    34979\nName: count, dtype: int64\n\n==================================================\n\n\n\n\n\n\n\n\n\n\n\n\nGender\n\n\n\n\n\n\n\nCholesterol\n\n\n\n\n\n\n\nGluc\n\n\n\n\n\n\n\n\n\nSmoke\n\n\n\n\n\n\n\nAlco\n\n\n\n\n\n\n\nActive\n\n\n\n\n\n\n\n\n\nCardio\n\n\n\n\n\n\n\n\nVisualizations, such as histograms, box plots, scatter plots, and heatmaps, serve as invaluable tools for unveiling data’s distribution, identifying relationships between variables, and detecting potential trends. These graphical representations simplify complex data, offering a clear and efficient means of interpretation, ultimately enhancing our ability to understand and leverage the insights hidden within the dataset.\n\n\n\nAge\n\n\nA. The density plot for age suggests that a higher age of an individual is likely to be associated with cardiovascular dieases.\n\n\n\nWeight\n\n\nB. From the weight density plot in relation to cardio, the density curve for those with cardiovascular diseases is higher than that for those without, particularly for individuals with larger weights. This suggests that individuals with higher weights are more likely to have cardiovascular diseases.\n\n\n\n(i) Height\n\n\n\n\n\n(ii) Height\n\n\nC. (i) The first density plot indicates there is rarely an assciation between height and cardiovasucler dieases. (ii) The second density plot tells there is a relationsip between gender and height, but such variance does not result in any different on the response variable cardio.\n\n\n\nExamining the correlations between variables with correlation matrices provides a better understanding of the relationships within the data.\n\n\n\nCorrelation Matrix\n\n\nBased on the correlation matrix, there is a relatively higher correlation between ‘height’ and ‘gender’, ‘gluc’ and ‘cholesterol’, and ‘alco’ and ‘smoke’.\nExamining our response variable, ‘cardio’, we find that the variables with the highest correlations are ‘age’ (0.24), ‘weight’ (0.18), ‘cholesterol’ (0.22), and ‘bmi’ (0.17). These variables could be crucial features during the model training process.\n\n\n\n\nQ1 Age: How old are you? (Cardiovascular risk generally increases with age.)\n\nAnswer: Age is a key factor in predicting cardiovascular diseases. Observing the age density plot, we see that a higher age tends to be associated with a higher probability of cardiovascular diseases.\n\nQ2 Gender: Are you male or female? (Certain risk factors may vary by gender.)\n\nAnswer: Gender doesn’t seem to be a significant feature. There’s scarcely any difference in cardiovascular disease rates between males and females.\n\nQ3 Family History: Does your family have a history of heart disease or related conditions?\n\nAnswer: It’s challenging to find datasets with family history, primarily due to the vast variations in individual family histories and the availability of such data.\n\nQ4 Smoking: Do you currently smoke, or have you ever smoked in the past?\n\nAnswer: Based on the correlation matrix, smoking doesn’t appear to be an important feature in the current Exploratory Data Analysis (EDA) results.\n\nQ5 Physical Activity: How often do you engage in physical activity or exercise?\n\nAnswer: Physical activity is a feature in the dataset. Based on the correlation matrix, its correlation with other variables is almost zero. Given its binary nature, it’s a more subjective feature, making it challenging to incorporate into the model-building process.\n\nQ6 Diet: What is your typical diet like? (Consider factors like consumption of fruits, vegetables, and processed foods.)\n\nAnswer: Diet isn’t a feature in our dataset. Given that it’s likely a categorical variable with numerous levels, its utility in the model-building process remains uncertain.\n\nQ7 Blood Pressure: What is your current blood pressure, and have you been diagnosed with hypertension (high blood pressure)?\n\nAnswer: Our dataset includes systolic blood pressure (ap_hi) and diastolic blood pressure (ap_lo). However, they don’t seem to be significant cardiovascular risk factors based on our EDA results.\n\nQ8 Cholesterol Levels: Do you know your cholesterol levels, including LDL (low-density lipoprotein) and HDL (high-density lipoprotein) cholesterol? Q10 Weight and Body Mass Index (BMI): What is your current weight, and do you know your BMI? (Obesity is a significant cardiovascular risk factor.)\n\nAnswer: BMI and Cholesterol Levels exhibit a higher correlation with our response variable, cardio. They are significant cardiovascular risk factors.\n\n\n\nGiven the nature of the independent features and the low dimensionality of the levels for all categorical variables, no data grouping or segmentation has been performed on the dataset.\nThe sole additional column introduced is “bmi”, which more effectively represents the relationship between weight and height. It seems to be a crucial factor during the model training process.\n\n\n\nI used the 3-Sigma rule to identify outliers. Any data record that falls outside the range of mean plus or minus 3 standard deviations is defined as an outlier. I removed those data points and recalculated the 3-Sigma range. This process was repeated until no outliers were detected.\nThe following graph depicts the distributions before cleaning.\n\n\n\nOutlier Before\n\n\nIt’s evident that there are numerous outliers for ap_hi, ap_lo, and bmi, where the values are significantly outside the standard range. For other variables, like weight and height, there are also some values that defy common sense. For age, we opted to retain the lower age variables, even though they are considered outliers based on the 3-Sigma rule. The subsequent visuals illustrate the distributions post-cleaning.\n\n\n\nOutlier After\n\n\nNo outliers were detected using the 3-Sigma rule, and all the distributions appear reasonable.\n\n\n\nFrom the correlation matrix, we can identify the key features predicting cardiovascular diseases, which include ‘age,’ ‘weight,’ ‘cholesterol,’ and ‘bmi.’ These are the potential variables that will be selected during the feature selection process. However, due to the low number of features in this dataset, all the features will be considered in the future modeling processes.\nAll the exploratory data analysis (EDA) results align with our expectations. I am able to answer my research questions and generate hypotheses based on the relationships between different variables and the response variable ‘cardio.’ The patterns presented in the analysis are reasonable and make sense in the real world.\nOne of the advantages of this dataset is the large number of data records. After the outlier cleaning steps, we still have more than 60,000 records in the dataset. This ensures that the model has sufficient data for training and learning.\n\n\n\nAll the codes were complied in python using ipynb file. The detailed code is also saved online in the following location.\nhttps://github.com/anly501/dsan-5000-project-Sel272/blob/main/DSAN5000project/DataExploration/data_exploration.ipynb",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "DataExploration/dataexploration.html#data-understanding",
    "href": "DataExploration/dataexploration.html#data-understanding",
    "title": "Data Exploration",
    "section": "",
    "text": "Data understanding is a fundamental and critical step in the field of data science and analytics. It serves as the foundation upon which all subsequent data-related activities are built. This process involves the exploration, examination, and comprehension of the data at hand, enabling organizations and individuals to derive valuable insights, make informed decisions, and solve complex problems. Moreover, data understanding allows us to uncover patterns, trends, and relationships.\nThe original dataset comprises 70,000 patient records, containing 11 predictive variables (features) and 1 response variable (target). Body Mass Index (BMI) has been included in the dataset due to its significance as a factor that can be used to estimate an individual’s body fat and, by extension, assess their health in relation to their weight. It serves as a valuable tool for quickly evaluating health risks associated with weight. Therefore, we now have 12 features.\n\nAge | Objective Feature | age | int (days)\nHeight | Objective Feature | height | int (cm) |\nWeight | Objective Feature | weight | float (kg) |\nGender | Objective Feature | gender | categorical code |\nSystolic blood pressure | Examination Feature | ap_hi | int |\nDiastolic blood pressure | Examination Feature | ap_lo | int |\nCholesterol | Examination Feature | cholesterol | 1: normal, 2: above normal, 3: well above normal |\nGlucose | Examination Feature | gluc | 1: normal, 2: above normal, 3: well above normal |\nSmoking | Subjective Feature | smoke | binary |\nAlcohol intake | Subjective Feature | alco | binary |\nPhysical activity | Subjective Feature | active | binary |\nBody Mass Index | Objective Feature | bmi | float (kg/m²)|\nPresence or absence of cardiovascular disease | Target Variable | cardio | binary |\n\nThe problem I am addressing involves understanding the potential risk factors associated with cardiovascular diseases.\nIn the dataset, there are six numerical features: Age, Height, Weight, Systolic blood pressure (ap_hi), Diastolic blood pressure (ap_lo), and Body Mass Index (BMI). Additionally, there are seven categorical features: Gender, Cholesterol, Glucose, Smoking, Alcohol intake, Physical activity, and the presence or absence of cardiovascular disease (the Target Variable).\nIdentifying potential relationships and their relevance to the project’s objectives is a crucial step in data analysis and modeling. These relationships will be explored through visualization methods, such as correlation matrices, histograms, scatterplots, etc.\n\n\nCode\nimport pandas  as  pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ncardio = pd.read_csv('../Data/01-modified-data/cardio.csv')\ncardio['bmi'] = round(cardio['weight'] / ((cardio['height']/100) ** 2),2)\ncardio.head()\n\n\n\n\n\n\n\n\n\nid\nage\ngender\nheight\nweight\nap_hi\nap_lo\ncholesterol\ngluc\nsmoke\nalco\nactive\ncardio\nbmi\n\n\n\n\n0\n0\n50\n2\n168\n62\n110\n80\n1\n1\n0\n0\n1\n0\n21.97\n\n\n1\n1\n55\n1\n156\n85\n140\n90\n3\n1\n0\n0\n1\n1\n34.93\n\n\n2\n2\n52\n1\n165\n64\n130\n70\n3\n1\n0\n0\n0\n1\n23.51\n\n\n3\n3\n48\n2\n169\n82\n150\n100\n1\n1\n0\n0\n1\n1\n28.71\n\n\n4\n4\n48\n1\n156\n56\n100\n60\n1\n1\n0\n0\n0\n0\n23.01",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "DataExploration/dataexploration.html#descriptive-statstics",
    "href": "DataExploration/dataexploration.html#descriptive-statstics",
    "title": "Data Exploration",
    "section": "",
    "text": "Descriptive statistics serve as a fundamental tool for understanding and summarizing data, offering valuable insights into the characteristics of a dataset. In this analysis, the focus is on both numerical and categorical variables, each demanding specific methods to reveal crucial details.\nThe statistical summary report for numerical variables:\n\n\nCode\nimport pandas  as  pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ncardio = pd.read_csv('../Data/01-modified-data/cardio.csv')\ncardio['bmi'] = round(cardio['weight'] / ((cardio['height']/100) ** 2),2)\ncardio.head()\n\n# Select numerical columns\nnumerical_cols = [\"age\", \"height\", \"weight\", \"ap_hi\", \"ap_lo\", \"bmi\"]\n\nstats = {\n    'Mean': cardio[numerical_cols].mean(),\n    'Median': cardio[numerical_cols].median(),\n    'Mode': cardio[numerical_cols].mode().iloc[0],\n    'Standard Deviation': cardio[numerical_cols].std(),\n    'Variance': cardio[numerical_cols].var()\n}\n\nstats_df = pd.DataFrame(stats)\nprint(stats_df)\n\n\n              Mean   Median    Mode  Standard Deviation      Variance\nage      53.338686   54.000   56.00            6.765294     45.769203\nheight  164.359229  165.000  165.00            8.210126     67.406175\nweight   74.205543   72.000   65.00           14.395829    207.239884\nap_hi   128.817286  120.000  120.00          154.011419  23719.517323\nap_lo    96.630414   80.000   80.00          188.472530  35521.894676\nbmi      27.556502   26.375   23.88            6.091795     37.109965\n\n\n\n\n\n\n\n\n\n\n\nMean\n\n\n\n\n\n\n\nMedian\n\n\n\n\n\n\n\nMode\n\n\n\n\n\n\n\n\n\nStandard Deviation\n\n\n\n\n\n\n\nVariance\n\n\n\n\n\nFor categorical variables, frequency distributions and bar charts are used to visualize data distribution:\n\n\nCode\nimport pandas  as  pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ncardio = pd.read_csv('../Data/01-modified-data/cardio.csv')\ncardio['bmi'] = round(cardio['weight'] / ((cardio['height']/100) ** 2),2)\ncardio.head()\n\ncategorical_vars = [\"gender\", \"cholesterol\", \"gluc\", \"smoke\", \"alco\", \"active\", \"cardio\"]\n\nfor var in categorical_vars:\n    print(f\"Frequency distribution for {var}:\\n\")\n    print(cardio[var].value_counts())\n    print(\"\\n\" + \"=\"*50 + \"\\n\")\n\n\nFrequency distribution for gender:\n\ngender\n1    45530\n2    24470\nName: count, dtype: int64\n\n==================================================\n\nFrequency distribution for cholesterol:\n\ncholesterol\n1    52385\n2     9549\n3     8066\nName: count, dtype: int64\n\n==================================================\n\nFrequency distribution for gluc:\n\ngluc\n1    59479\n3     5331\n2     5190\nName: count, dtype: int64\n\n==================================================\n\nFrequency distribution for smoke:\n\nsmoke\n0    63831\n1     6169\nName: count, dtype: int64\n\n==================================================\n\nFrequency distribution for alco:\n\nalco\n0    66236\n1     3764\nName: count, dtype: int64\n\n==================================================\n\nFrequency distribution for active:\n\nactive\n1    56261\n0    13739\nName: count, dtype: int64\n\n==================================================\n\nFrequency distribution for cardio:\n\ncardio\n0    35021\n1    34979\nName: count, dtype: int64\n\n==================================================\n\n\n\n\n\n\n\n\n\n\n\n\nGender\n\n\n\n\n\n\n\nCholesterol\n\n\n\n\n\n\n\nGluc\n\n\n\n\n\n\n\n\n\nSmoke\n\n\n\n\n\n\n\nAlco\n\n\n\n\n\n\n\nActive\n\n\n\n\n\n\n\n\n\nCardio",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "DataExploration/dataexploration.html#data-visualization",
    "href": "DataExploration/dataexploration.html#data-visualization",
    "title": "Data Exploration",
    "section": "",
    "text": "Visualizations, such as histograms, box plots, scatter plots, and heatmaps, serve as invaluable tools for unveiling data’s distribution, identifying relationships between variables, and detecting potential trends. These graphical representations simplify complex data, offering a clear and efficient means of interpretation, ultimately enhancing our ability to understand and leverage the insights hidden within the dataset.\n\n\n\nAge\n\n\nA. The density plot for age suggests that a higher age of an individual is likely to be associated with cardiovascular dieases.\n\n\n\nWeight\n\n\nB. From the weight density plot in relation to cardio, the density curve for those with cardiovascular diseases is higher than that for those without, particularly for individuals with larger weights. This suggests that individuals with higher weights are more likely to have cardiovascular diseases.\n\n\n\n(i) Height\n\n\n\n\n\n(ii) Height\n\n\nC. (i) The first density plot indicates there is rarely an assciation between height and cardiovasucler dieases. (ii) The second density plot tells there is a relationsip between gender and height, but such variance does not result in any different on the response variable cardio.",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "DataExploration/dataexploration.html#correlation-analysis",
    "href": "DataExploration/dataexploration.html#correlation-analysis",
    "title": "Data Exploration",
    "section": "",
    "text": "Examining the correlations between variables with correlation matrices provides a better understanding of the relationships within the data.\n\n\n\nCorrelation Matrix\n\n\nBased on the correlation matrix, there is a relatively higher correlation between ‘height’ and ‘gender’, ‘gluc’ and ‘cholesterol’, and ‘alco’ and ‘smoke’.\nExamining our response variable, ‘cardio’, we find that the variables with the highest correlations are ‘age’ (0.24), ‘weight’ (0.18), ‘cholesterol’ (0.22), and ‘bmi’ (0.17). These variables could be crucial features during the model training process.",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "DataExploration/dataexploration.html#hypothesis-generation",
    "href": "DataExploration/dataexploration.html#hypothesis-generation",
    "title": "Data Exploration",
    "section": "",
    "text": "Q1 Age: How old are you? (Cardiovascular risk generally increases with age.)\n\nAnswer: Age is a key factor in predicting cardiovascular diseases. Observing the age density plot, we see that a higher age tends to be associated with a higher probability of cardiovascular diseases.\n\nQ2 Gender: Are you male or female? (Certain risk factors may vary by gender.)\n\nAnswer: Gender doesn’t seem to be a significant feature. There’s scarcely any difference in cardiovascular disease rates between males and females.\n\nQ3 Family History: Does your family have a history of heart disease or related conditions?\n\nAnswer: It’s challenging to find datasets with family history, primarily due to the vast variations in individual family histories and the availability of such data.\n\nQ4 Smoking: Do you currently smoke, or have you ever smoked in the past?\n\nAnswer: Based on the correlation matrix, smoking doesn’t appear to be an important feature in the current Exploratory Data Analysis (EDA) results.\n\nQ5 Physical Activity: How often do you engage in physical activity or exercise?\n\nAnswer: Physical activity is a feature in the dataset. Based on the correlation matrix, its correlation with other variables is almost zero. Given its binary nature, it’s a more subjective feature, making it challenging to incorporate into the model-building process.\n\nQ6 Diet: What is your typical diet like? (Consider factors like consumption of fruits, vegetables, and processed foods.)\n\nAnswer: Diet isn’t a feature in our dataset. Given that it’s likely a categorical variable with numerous levels, its utility in the model-building process remains uncertain.\n\nQ7 Blood Pressure: What is your current blood pressure, and have you been diagnosed with hypertension (high blood pressure)?\n\nAnswer: Our dataset includes systolic blood pressure (ap_hi) and diastolic blood pressure (ap_lo). However, they don’t seem to be significant cardiovascular risk factors based on our EDA results.\n\nQ8 Cholesterol Levels: Do you know your cholesterol levels, including LDL (low-density lipoprotein) and HDL (high-density lipoprotein) cholesterol? Q10 Weight and Body Mass Index (BMI): What is your current weight, and do you know your BMI? (Obesity is a significant cardiovascular risk factor.)\n\nAnswer: BMI and Cholesterol Levels exhibit a higher correlation with our response variable, cardio. They are significant cardiovascular risk factors.",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "DataExploration/dataexploration.html#data-grouping-and-segmentation",
    "href": "DataExploration/dataexploration.html#data-grouping-and-segmentation",
    "title": "Data Exploration",
    "section": "",
    "text": "Given the nature of the independent features and the low dimensionality of the levels for all categorical variables, no data grouping or segmentation has been performed on the dataset.\nThe sole additional column introduced is “bmi”, which more effectively represents the relationship between weight and height. It seems to be a crucial factor during the model training process.",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "DataExploration/dataexploration.html#identifying-outliers",
    "href": "DataExploration/dataexploration.html#identifying-outliers",
    "title": "Data Exploration",
    "section": "",
    "text": "I used the 3-Sigma rule to identify outliers. Any data record that falls outside the range of mean plus or minus 3 standard deviations is defined as an outlier. I removed those data points and recalculated the 3-Sigma range. This process was repeated until no outliers were detected.\nThe following graph depicts the distributions before cleaning.\n\n\n\nOutlier Before\n\n\nIt’s evident that there are numerous outliers for ap_hi, ap_lo, and bmi, where the values are significantly outside the standard range. For other variables, like weight and height, there are also some values that defy common sense. For age, we opted to retain the lower age variables, even though they are considered outliers based on the 3-Sigma rule. The subsequent visuals illustrate the distributions post-cleaning.\n\n\n\nOutlier After\n\n\nNo outliers were detected using the 3-Sigma rule, and all the distributions appear reasonable.",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "DataExploration/dataexploration.html#report-and-discuss-your-methods-and-findings",
    "href": "DataExploration/dataexploration.html#report-and-discuss-your-methods-and-findings",
    "title": "Data Exploration",
    "section": "",
    "text": "From the correlation matrix, we can identify the key features predicting cardiovascular diseases, which include ‘age,’ ‘weight,’ ‘cholesterol,’ and ‘bmi.’ These are the potential variables that will be selected during the feature selection process. However, due to the low number of features in this dataset, all the features will be considered in the future modeling processes.\nAll the exploratory data analysis (EDA) results align with our expectations. I am able to answer my research questions and generate hypotheses based on the relationships between different variables and the response variable ‘cardio.’ The patterns presented in the analysis are reasonable and make sense in the real world.\nOne of the advantages of this dataset is the large number of data records. After the outlier cleaning steps, we still have more than 60,000 records in the dataset. This ensures that the model has sufficient data for training and learning.",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "DataExploration/dataexploration.html#tools-and-software",
    "href": "DataExploration/dataexploration.html#tools-and-software",
    "title": "Data Exploration",
    "section": "",
    "text": "All the codes were complied in python using ipynb file. The detailed code is also saved online in the following location.\nhttps://github.com/anly501/dsan-5000-project-Sel272/blob/main/DSAN5000project/DataExploration/data_exploration.ipynb",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "DataExploration/dataexploration.html#data-understanding-1",
    "href": "DataExploration/dataexploration.html#data-understanding-1",
    "title": "Data Exploration",
    "section": "Data Understanding",
    "text": "Data Understanding\nFor text data, I used the News API to retrieve text data from the server. I identified four topics: ‘Cardiovascular,’ ‘Cancer,’ ‘Stroke,’ and ‘Diabetes.’ All the text data retrieved using the ‘Cardiovascular’ topic is labeled with ‘1,’ indicating its relevance to our research goals, while data from the other topics is labeled as ‘0.’\nThe dataset consists of 400 records with 2 columns. One column contains the text data retrieved using the News API, and the other column contains the labels as described above. Additionally, I removed stopwords to enhance the text data modeling training process, allowing the algorithm to focus more on content words rather than function words. Below is a screenshot of the text data obtained.\n\n\n\nText Data",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "DataExploration/dataexploration.html#data-visualization-1",
    "href": "DataExploration/dataexploration.html#data-visualization-1",
    "title": "Data Exploration",
    "section": "Data Visualization",
    "text": "Data Visualization\nFor the text data, I created two word clouds to visualize the high-frequency words in the ‘Cardiovascular’ topic and the other topics. I also generated word frequency charts to delve into further details.\nThere are no significant differences between the words across different topics, except for the topic words themselves. Additionally, the word frequency chart includes many Spanish words, making it challenging to identify any meaningful patterns.\n\n\n\n\n\n\n\n\n\nCardio-Related\n\n\n\n\n\n\n\nNon Cardio-Related\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCardio-Related\n\n\n\n\n\n\n\nNon Cardio-Related",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "DataExploration/dataexploration.html#report-and-discuss-your-methods-and-findings-1",
    "href": "DataExploration/dataexploration.html#report-and-discuss-your-methods-and-findings-1",
    "title": "Data Exploration",
    "section": "Report and discuss your methods and findings",
    "text": "Report and discuss your methods and findings\nIn summary, when compared to record data, visualizing and identifying useful patterns in text data is much more challenging due to its unstructured nature. In the Naive Bayes process, we will explore the text data further to determine if we can gain better insights through modeling, training, and prediction results.",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "DataExploration/dataexploration.html#tools-and-software-1",
    "href": "DataExploration/dataexploration.html#tools-and-software-1",
    "title": "Data Exploration",
    "section": "Tools and Software",
    "text": "Tools and Software\nAll the codes were complied in python using ipynb file.",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "DataExploration/data_exploration_text.html",
    "href": "DataExploration/data_exploration_text.html",
    "title": "Research Project",
    "section": "",
    "text": "import wikipedia\nimport pandas as pd\n\n/Users/xueningyang/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n  warnings.warn(\n\n\n\ndf=pd.read_csv(\"../Data/01-modified-data/cardiovascular_text_final.csv\")\ndf = df[['text','label']]\ndf['text'] = df['text'].astype(str)\n\n\nimport nltk\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/xueningyang/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n\n\nTrue\n\n\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\ndef remove_stopwords(text):\n    # Tokenize the text\n    words = text.split()\n\n    # Get the list of stopwords in English (you can change the language if needed)\n    stop_words = set(stopwords.words('english'))\n\n    # Remove the stopwords from the tokenized words\n    filtered_words = [word for word in words if word.lower() not in stop_words]\n\n    # Convert the filtered words back to a string\n    return \" \".join(filtered_words)\n\n\ndf['text'] = df['text'].apply(remove_stopwords)\ndf\n\n\n\n\n\n\n\n\ntext\nlabel\n\n\n\n\n0\nresearchers increasingly find effects infectio...\n1\n\n\n1\ntogether, ai imaging ai genetic analysis may a...\n1\n\n\n2\nlas enfermedades relacionadas con nuestro sist...\n1\n\n\n3\nmonthly dose vitamin found lower incidence car...\n1\n\n\n4\nauthor cat bohannon says there's \"male norm\" s...\n1\n\n\n...\n...\n...\n\n\n395\nhere's take medical community leverage healthi...\n0\n\n\n396\nwoot! men's nylon ratchet belts sale prices li...\n0\n\n\n397\nwalmart, nations largest private employer, exp...\n0\n\n\n398\npeople diabetes, essential inject insulin regu...\n0\n\n\n399\nscientists found success treating 'neglected' ...\n0\n\n\n\n\n400 rows × 2 columns\n\n\n\n\ndf1 = df[df['label'] == 1]\ndf2 = df[df['label'] == 0]\ntext_1=\"\"\nfor i in range(len(df1)):\n    text_1 += df1['text'][i]\ntext_2=\"\"\nfor i in range(len(df2)):\n    text_2 += df2['text'][i+100]\n\n\ndef generate_word_cloud(my_text):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n    # exit()\n    # Import package\n    # Define a function to plot word cloud\n    def plot_cloud(wordcloud):\n        # Set figure size\n        plt.figure(figsize=(40, 30))\n        # Display image\n        plt.imshow(wordcloud) \n        # No axis details\n        plt.axis(\"off\");\n\n    # Generate word cloud\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000, \n        random_state=1, \n        background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\n\n\ngenerate_word_cloud(text_1)\n\n\n\n\n\n\n\n\n\ngenerate_word_cloud(text_2)\n\n\n\n\n\n\n\n\n\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n\ndef plot_word_frequency(text):\n    # Convert text to lowercase\n    text = text.lower()\n\n    # Tokenize the text using regex to split by non-alphanumeric characters\n    words = re.findall(r'\\w+', text)\n\n    # Count the occurrences of each word\n    word_counts = Counter(words)\n\n    # Sort words by their count\n    sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n\n    # Split the words and their counts into separate lists\n    labels, values = zip(*sorted_word_counts[:20])  # Take top 20 words\n\n    # Plotting the word frequency chart\n    plt.figure(figsize=(10, 5))\n    plt.bar(labels, values)\n    plt.xlabel('Words')\n    plt.ylabel('Frequency')\n    plt.title('Top 20 Word Frequencies')\n    plt.xticks(rotation=90)\n    plt.show()\n\n\n\n\n\nplot_word_frequency(text_1)\n\n\n\n\n\n\n\n\n\nplot_word_frequency(text_2)"
  },
  {
    "objectID": "Clustering/clustering.html",
    "href": "Clustering/clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "The ‘Cardio’ dataset comprises 12 features; however, for clustering purposes, only the numerical variables are pertinent. Therefore, I extracted the continuous variables to form a subset, denoted as the feature dataset X, from the original data. This curated dataset is utilized in the current project.\nThe objective is to employ clustering techniques to segment the dataset into distinct clusters, each representing unique cardiovascular traits. Each applied method will aim to maximize the difference between clusters. The resultant cluster groups will serve as a basis for analyzing varying levels of cardiovascular risk.\n\n\nCode\nimport pandas as pd\n\ndata=pd.read_csv('../Data/01-modified-data/cardiovascular_numeric_final.csv')\ndata = data.drop(data.columns[0], axis=1)\n\nnumerical_cols = [\"age\", \"height\", \"weight\", \"ap_hi\", \"ap_lo\", \"bmi\"]\nX = data[numerical_cols]\nY = data['cardio']\nX.columns\n\n\nIndex(['age', 'height', 'weight', 'ap_hi', 'ap_lo', 'bmi'], dtype='object')",
    "crumbs": [
      "Clustering"
    ]
  },
  {
    "objectID": "Clustering/clustering.html#introduction",
    "href": "Clustering/clustering.html#introduction",
    "title": "Clustering",
    "section": "",
    "text": "The ‘Cardio’ dataset comprises 12 features; however, for clustering purposes, only the numerical variables are pertinent. Therefore, I extracted the continuous variables to form a subset, denoted as the feature dataset X, from the original data. This curated dataset is utilized in the current project.\nThe objective is to employ clustering techniques to segment the dataset into distinct clusters, each representing unique cardiovascular traits. Each applied method will aim to maximize the difference between clusters. The resultant cluster groups will serve as a basis for analyzing varying levels of cardiovascular risk.\n\n\nCode\nimport pandas as pd\n\ndata=pd.read_csv('../Data/01-modified-data/cardiovascular_numeric_final.csv')\ndata = data.drop(data.columns[0], axis=1)\n\nnumerical_cols = [\"age\", \"height\", \"weight\", \"ap_hi\", \"ap_lo\", \"bmi\"]\nX = data[numerical_cols]\nY = data['cardio']\nX.columns\n\n\nIndex(['age', 'height', 'weight', 'ap_hi', 'ap_lo', 'bmi'], dtype='object')",
    "crumbs": [
      "Clustering"
    ]
  },
  {
    "objectID": "Clustering/clustering.html#theories",
    "href": "Clustering/clustering.html#theories",
    "title": "Clustering",
    "section": "Theories",
    "text": "Theories\n\nK-Means Clustering\nK-means is a commonly used clustering algorithm based on Euclidean distance and is a typical example of an unsupervised learning technique that does not require a labeled response. When using the K-Means algorithm, it should be noted that the number of clusters, k, is a hyperparameter and requires human input to determine. The main purpose is to find k optimal centroids based on the set K and to assign the data points closest to these centroids to the clusters represented by these centroids.\nK-means clustering is widely and popularly used for cluster analysis because it is easy to interpret and implement, and the classification is effective. Even with a large number of variables, K-means can be computationally faster if a small k value is chosen. However, its accuracy may not be as high as that of supervised learning techniques, and it is also more sensitive to the choice of the k value. Moreover, it is not suitable for non-linear data.\n\n\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise)\nThe Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is one of the most common clustering algorithms that identifies clusters based on the density of data points in a space. It operates by pinpointing areas where data points are closely grouped, thereby indicating that they are neighbors. Additionally, it can be used to identify non-linearly separable clusters and to mark outlier points that are situated alone in low-density regions.\nCompared to the traditional K-Means algorithm, the most significant difference with DBSCAN is that it does not require the input of the number of clusters, k. It can also detect noise, effectively separating it from the clusters. However, a disadvantage of DBSCAN is its sensitivity to the choice of the ‘Eps’ and ‘MinPts’ parameters.\n\n\nHierarchical Clustering\nHierarchical clustering is a technique in cluster analysis designed to create a nested series of clusters organized hierarchically. Compared to the other two methods, Hierarchical clustering can generate a dendrogram, which allows for the visualization of different clustering results and the similarity relationships among data points.\nAgglomerative clustering is the most widely used type of Hierarchical clustering for grouping objects based on their similarity. This process begins with a single cluster encompassing all data points and gradually divides it into distinct clusters using various linkage methods. The process can be visualized through a dendrogram, and the optimal number of clusters can be chosen based on these visual results.\nLike DBSCAN, Hierarchical clustering does not require a predetermined number of clusters. Its advantage lies in the use of a dendrogram for enhanced visualization.\n\n\nModel Selection Methods\n\nElbow Method: This method is used to find the optimal k value by plotting the within-cluster sum of squares (WCSS) against the number of clusters on a graph. The point where the WCSS begins to diminish, known as the ‘elbow,’ is considered an indicator of the appropriate number of clusters. When k equals 1, the WCSS value is at its largest. This method is mostly used in K-means clustering.\nSilhouette Score: This metric evaluates the distance of a point to its own cluster in comparison to its distance to other clusters. The higher the score, the greater the separation between different clusters. Values range from -1 to 1, indicating poor, uncertain, and good clustering results, respectively. In essence, this measure ensures that each cluster is distinct and its members are more closely related to their own cluster than to others. The silhouette score, or silhouette coefficient, plays a crucial role in fine-tuning the hyperparameters in clustering algorithms such as DBSCAN and hierarchical clustering.",
    "crumbs": [
      "Clustering"
    ]
  },
  {
    "objectID": "Clustering/clustering.html#methods",
    "href": "Clustering/clustering.html#methods",
    "title": "Clustering",
    "section": "Methods",
    "text": "Methods\n\nData Selection\nThe labels (targets) Y is not used when I did the data selection for the clustering.\n\n\nCode\nimport pandas as pd\n\ndata=pd.read_csv('../Data/01-modified-data/cardiovascular_numeric_final.csv')\ndata = data.drop(data.columns[0], axis=1)\n\nnumerical_cols = [\"age\", \"height\", \"weight\", \"ap_hi\", \"ap_lo\", \"bmi\"]\nX = data[numerical_cols]\nY = data['cardio']\nprint(X)\n\n\n       age  height  weight  ap_hi  ap_lo    bmi\n0       50     168      62    110     80  21.97\n1       55     156      85    140     90  34.93\n2       52     165      64    130     70  23.51\n3       48     169      82    150    100  28.71\n4       48     156      56    100     60  23.01\n...    ...     ...     ...    ...    ...    ...\n64801   54     172      70    130     90  23.66\n64802   58     165      80    150     80  29.38\n64803   53     168      76    120     80  26.93\n64804   61     163      72    135     80  27.10\n64805   56     170      72    120     80  24.91\n\n[64806 rows x 6 columns]\n\n\n\n\nHyper-parameter tuning\nHyperparameter tuning is a crucial step in optimizing clustering algorithms to achieve the best performance. The following hyperparameters are used in the project:\n\nK-Means\nIn K-means clustering analysis, the key parameter to adjust is the number of clusters, denoted as ‘k.’ One of the primary methods for tuning this parameter is the elbow method. This involves plotting ‘k’ against measures of Inertia and Distortion to visually determine the optimal number of clusters. The ‘elbow’ point—where the rate of decrease in both Inertia and Distortion sharply changes—indicates the most appropriate value for ‘k.’ This insight is typically gained during the hyper-parameter tuning stage.\n\n\n\nK-Means\n\n\nThe graph indicates that the optimal number of clusters is three. This conclusion is drawn from the observation that the reduction in Inertia and Distortion is less pronounced when moving from one to two clusters than it is from two to three clusters. According to the elbow method’s criteria, this suggests a ‘k’ value of three, where the rate of decrease in these metrics noticeably slows, signifying the most suitable cluster count.\n\n\nDBSCAN\nIn DBSCAN modeling, the parameters ‘eps’ and ‘min_samples’ require careful tuning. The ‘eps’ refers to epsilon, a critical parameter defining the maximum distance between two points for them to be categorized as part of the same neighborhood. On the other hand, ‘min_samples’ represents the minimum number of samples required for a cluster to be defined. For ‘eps,’ the tested range includes values such as 0.1, 0.2, 0.3, 0.4, and 0.5. For ‘min_samples,’ the range spans from 100, 1000, and 5000. Given the extensive size of the dataset, exhaustive tuning across a broad spectrum of values is impractical due to the significant time investment it would require. Consequently, a narrower selection of candidate values is explored. The silhouette score, a measure of cluster cohesion and separation, guides the tuning of DBSCAN parameters. The optimal parameters are identified by the peak silhouette score. The ensuing graph showcases the results from this parameter tuning exercise.\n\n\n\nDBSCAN\n\n\nFrom the graph above, the highest silhouette score occurs when the number of clusters is 1. However, a cluster number of one implies that all data points are in a single cluster, which is not desirable. Therefore, the optimal number of clusters is identified as three, which yields the second-highest silhouette score. The accompanying table, detailing metrics for each ‘eps’ and ‘min_samples’ configuration, indicates that the optimal ‘eps’ is 0.5 and the optimal ‘min_samples’ is 100. This combination results in an ideal cluster count of three.\n\n\n\nDBSCAN Table\n\n\n\n\nHierarchical Clustering\nThe parameters to tune include the optimal number of clusters used to split the dendrogram and the linkage criterion, which defines the rule for measuring the distance between clusters. The dendrogram and silhouette score are common methods used in hierarchical clustering.\nLinkage Criterion or Distance Metrics\nDifferent clustering algorithms can use various distance metrics, and the choice of metric can significantly influence the clusters that are formed. For example,\n\n\n\nHierarchical Clustering\n\n\n\nEuclidean Distance measures the straight-line distance between two points in multidimensional space, calculated as the square root of the sum of the squared differences between the points’ coordinates.\nManhattan Distance calculates the distance between two points by summing the absolute differences of their Cartesian coordinates.\nCosine similarity gauges the similarity of two points, focusing on direction rather than magnitude, useful in text analysis and high-dimensional spaces.\nIn Hierarchical clustering, the correlation distance (one minus the Pearson correlation coefficient) can be used to determine the similarity between two points.\n\nBy carefully tuning parameters and choosing the right distance metric, one can greatly enhance the quality of clustering results. The Hierarchical clustering plot suggests choosing Manhattan distance as the optimal metric because it delivers the highest value among all considered metrics. However, the Manhattan distance does not produce clear results when visualized with a dendrogram. Thus, I have ultimately selected the Euclidean distance for the dendrogram, while Manhattan distance was used to achieve optimal Hierarchical clustering. The ideal number of clusters, determined by the highest silhouette value, is 2.\n\n\n\nDendrogram\n\n\n\n\n\nFinal Results\nIn summary, I have reconstructed all the models, choosing 3 clusters for K-means clustering, 3 for DBSCAN, and 2 for Hierarchical clustering. This decision was based on the elbow method previously used for K-means and silhouette analysis for DBSCAN and Hierarchical clustering.",
    "crumbs": [
      "Clustering"
    ]
  },
  {
    "objectID": "Clustering/clustering.html#results",
    "href": "Clustering/clustering.html#results",
    "title": "Clustering",
    "section": "Results",
    "text": "Results\n\n\n\n\nSilhouette Score\n\n\n\n\nK-Means Clustering\n0.2223\n\n\nDBSCAN\n-0.1371\n\n\nHierarchical Clustering\n0.3090\n\n\n\nAccording to the final results table, the silhouette score for Hierarchical clustering is the highest at 0.3090, compared to the other two methods. The higher the silhouette score, the better the clustering results. However, since the value is still relatively low, it does not provide substantial insight into the analysis. Further investigation is required. The number of optimal clusters is close among the three methods, with K-means at 3, DBSCAN at 3, and Hierarchical clustering at 2. Due to the different algorithms and the nature of each method, there will be a small variance in the optimal cluster numbers. Overall, the results align with one another.\n\n\n\n\n\n\n\n\n\n2D Visualization\n\n\n\n\n\n\n\n\n\n\n\nBased on the analysis of the three plots, the K-means clustering plot appears to be the most effective.Overall, the number of clusters matches between different methods, but the data points within each group vary significantly from one method to another. The scatter plot clearly visualizes the data points and their assigned cluster labels, showcasing distinct grouping patterns and separation between clusters, allowing for the identification of three distinct groups.\nIn contrast, the silhouette score for DBSCAN is negative, indicating it is an unsuitable choice. Furthermore, the scatter plot for DBSCAN divides the data into three clusters and one noise cluster, offering no clear visual conclusions. In the Hierarchical clustering plot, the data points are divided into two groups, but one group contains only a single value, rendering it indistinct in the visualization. This essentially equates to dividing the entire dataset into a single cluster, which does not meet our expectations, despite it having the highest silhouette score among the three.\nTherefore, K-means emerges as the more reasonable option, with a clearer division of clusters and a comparatively high silhouette score, making it the optimal model. Additionally, the labels could reflect three categories—high, medium, and low cardiovascular risk—rather than a binary distinction of cardiovascular disease presence.",
    "crumbs": [
      "Clustering"
    ]
  },
  {
    "objectID": "Clustering/clustering.html#conclusions",
    "href": "Clustering/clustering.html#conclusions",
    "title": "Clustering",
    "section": "Conclusions",
    "text": "Conclusions\n\n\nIn conclusion, the optimal clustering algorithm for the dataset appears to be the K-means method. It achieves relatively better scoring measures compared to others and effectively divides the data into distinct groups based on visualization.\nA crucial aspect of using K-means or any other clustering method is determining the true labels for each cluster group. Since clustering is an unsupervised method, true labels are not known for unseen testing data. Even though true labels are available in this dataset and can be used to evaluate the results, it is still impractical to assign specific labels such as “cardiovascular” or “non-cardiovascular” to any particular cluster. Moreover, the accompanying graph suggests a pattern that may justify dividing the dataset into three risk categories—high, medium, and low—when setting the number of clusters to three. This introduces an additional layer of complexity in defining group characteristics as the number of clusters increases. Additional data would be required for more accurate labeling of each group.\nFurthermore, the measurement scores are not high for any of the three clustering methods, indicating limited practical utility for these clustering techniques. Future research may benefit from a supervised approach to directly predict labels, which could be a more effective modeling method for the current research objectives.\n\n\n\n\n\n\nNote\n\n\n\nIn the cluster analysis of the ‘cardio’ dataset, I chose to work with 10% of the full dataset for Hierarchical clustering due to the algorithm’s time-intensive nature, which might introduce a selection bias. In the final results, ‘bmi’ and ‘ap_hi’ were selected as variables for visualizing the 2D clustering. ‘Bmi’ effectively encapsulates information on both weight and height, while ‘ap_hi’ serves as a reliable indicator of systolic blood pressure, making them informative dimensions for interpretation.\n\n\n\nReferences\n\n\nDSAN 5000 class notes and sample codes\nhttps://www.displayr.com/understanding-cluster-analysis-a-comprehensive-guide/#common-mistakes-and-disadvantages-with-cluster-analysis",
    "crumbs": [
      "Clustering"
    ]
  },
  {
    "objectID": "DecisionTrees/decisiontree_python.html",
    "href": "DecisionTrees/decisiontree_python.html",
    "title": "Research Project",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom sklearn import tree\nfrom IPython.display import Image\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\n\n\nimport pandas as pd\ncardio = pd.read_csv('../Data/01-modified-data/cardio.csv')\ncardio['bmi'] = round(cardio['weight'] / ((cardio['height']/100) ** 2),2)\ncardio.head()\n\n\n\n\n\n\n\n\nid\nage\ngender\nheight\nweight\nap_hi\nap_lo\ncholesterol\ngluc\nsmoke\nalco\nactive\ncardio\nbmi\n\n\n\n\n0\n0\n50\n2\n168\n62\n110\n80\n1\n1\n0\n0\n1\n0\n21.97\n\n\n1\n1\n55\n1\n156\n85\n140\n90\n3\n1\n0\n0\n1\n1\n34.93\n\n\n2\n2\n52\n1\n165\n64\n130\n70\n3\n1\n0\n0\n0\n1\n23.51\n\n\n3\n3\n48\n2\n169\n82\n150\n100\n1\n1\n0\n0\n1\n1\n28.71\n\n\n4\n4\n48\n1\n156\n56\n100\n60\n1\n1\n0\n0\n0\n0\n23.01\n\n\n\n\n\n\n\n\n# LOOK AT FIRST ROW/Dataframe\nprint(cardio.iloc[0])\n\nid               0.00\nage             50.00\ngender           2.00\nheight         168.00\nweight          62.00\nap_hi          110.00\nap_lo           80.00\ncholesterol      1.00\ngluc             1.00\nsmoke            0.00\nalco             0.00\nactive           1.00\ncardio           0.00\nbmi             21.97\nName: 0, dtype: float64\n\n\n\n#PRINT THE FOLLOWING DATA-FRAME WHICH SUMMARIZES EACH COLUMN\nsummary_cardio = cardio.describe().T[['min', 'mean', 'max']]\nsummary_cardio['dtypes'] = cardio.dtypes\nsummary_cardio = summary_cardio[['dtypes', 'min', 'mean', 'max']]\nsummary_cardio\n\n\n\n\n\n\n\n\ndtypes\nmin\nmean\nmax\n\n\n\n\nid\nint64\n0.00\n49947.926488\n99999.00\n\n\nage\nint64\n30.00\n53.278878\n65.00\n\n\ngender\nint64\n1.00\n1.348317\n2.00\n\n\nheight\nint64\n142.00\n164.452952\n187.00\n\n\nweight\nint64\n37.00\n72.708283\n109.00\n\n\nap_hi\nint64\n85.00\n125.447212\n170.00\n\n\nap_lo\nint64\n55.00\n80.773617\n106.00\n\n\ncholesterol\nint64\n1.00\n1.349921\n3.00\n\n\ngluc\nint64\n1.00\n1.217434\n3.00\n\n\nsmoke\nint64\n0.00\n0.086967\n1.00\n\n\nalco\nint64\n0.00\n0.052079\n1.00\n\n\nactive\nint64\n0.00\n0.804555\n1.00\n\n\ncardio\nint64\n0.00\n0.481884\n1.00\n\n\nbmi\nfloat64\n14.48\n26.908029\n40.01\n\n\n\n\n\n\n\n\n# INSERT CODE TO EXPLORE THE LOAD BALANCE AND COUNT THE NUMBER OF SAMPLES FOR EACH CARDIO (THEN PRINT THE RESULT)\ncount0 = len(cardio[cardio['cardio'] == 0])\ncount1 = len(cardio[cardio['cardio'] == 1])\ntotal = count0  + count1  \nprop1 = count0  / total\nprop2 = count1 / total\n\nprint(\"Number of points with cardio=0:\", count0, prop1)\nprint(\"Number of points with cardio=1:\", count1, prop2)\n\nNumber of points with cardio=0: 33577 0.5181156065796377\nNumber of points with cardio=1: 31229 0.4818843934203623\n\n\n\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n# RUN THE FOLLOWING CODE TO SHOW THE HEAT-MAP FOR THE CORRELATION MATRIX\ncorr = cardio.corr();  #print(corr)                 #COMPUTE CORRELATION OF FEATER MATRIX\nprint(corr.shape)\nsns.set_theme(style=\"white\")\nf, ax = plt.subplots(figsize=(20, 20))  # Set up the matplotlib figure\ncmap = sns.diverging_palette(230, 20, as_cmap=True)     # Generate a custom diverging colormap\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr,  cmap=cmap, vmin=-1, vmax=1, center=0,\n        square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show();\n\n(14, 14)\n\n\n\n\n\n\n\n\n\n\n# INSERT CODE TO MAKE DATA-FRAMES (or numpy arrays) (X,Y) WHERE Y=\"target\" COLUMN and X=\"everything else\"\nX = cardio.drop('cardio', axis=1) \nY = cardio['cardio']\n\n\n# INSERT CODE TO PARTITION THE DATASET INTO TRAINING AND TEST SETS\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=12)\n\n\n#Baseline Random Classifier\nimport numpy as np\nimport random\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\n# RANDOM CLASSIFIER \n\nnp.random.seed(12)\n\ndef random_classifier(y_data):\n    ypred=[];\n    max_label=np.max(y_data); #print(max_label)\n    for i in range(0,len(y_data)):\n        ypred.append(int(np.floor((max_label+1)*np.random.uniform(0,1))))\n\n    print(\"-----RANDOM CLASSIFIER-----\")\n    print(\"count of prediction:\",Counter(ypred).values()) # counts the elements' frequency\n    print(\"probability of prediction:\",np.fromiter(Counter(ypred).values(), dtype=float)/len(y_data)) # counts the elements' frequency\n    print(\"accuracy\",accuracy_score(y_data, ypred))\n    print(\"percision, recall, fscore,\",precision_recall_fscore_support(y_data, ypred))\n\nprint(\"\\nBINARY CLASS: UNIFORM LOAD\")\ny=Y\nrandom_classifier(y)\n\nprint(\"\\nBINARY CLASS: NON UNIFORM LOAD\")\ny=Y\nrandom_classifier(y)\n\n\nBINARY CLASS: UNIFORM LOAD\n-----RANDOM CLASSIFIER-----\ncount of prediction: dict_values([35071, 34929])\nprobability of prediction: [0.50101429 0.49898571]\naccuracy 0.4986\npercision, recall, fscore, (array([0.49890223, 0.49829654]), array([0.49961452, 0.49758426]), array([0.49925812, 0.49794015]), array([35021, 34979]))\n\nBINARY CLASS: NON UNIFORM LOAD\n-----RANDOM CLASSIFIER-----\ncount of prediction: dict_values([35055, 34945])\nprobability of prediction: [0.50078571 0.49921429]\naccuracy 0.5006857142857143\npercision, recall, fscore, (array([0.50098727, 0.50038511]), array([0.49990006, 0.50147231]), array([0.50044307, 0.50092812]), array([35021, 34979]))\n\n\n\n\n#### INSERT CODE BELOW TO TRAIN A SKLEARN DECISION TREE MODEL ON x_train,y_train \nfrom sklearn import tree\ntest_results=[]\ntrain_results=[]\n\nfor num_layer in range(1,20):\n    model = tree.DecisionTreeClassifier(max_depth=num_layer)\n    model = model.fit(x_train,y_train)\n\n    yp_train=model.predict(x_train)\n    yp_test=model.predict(x_test)\n\n    # print(y_pred.shape)\n    test_results.append([num_layer,accuracy_score(y_test, yp_test),precision_score(y_test, yp_test,pos_label=1),recall_score(y_test, yp_test,pos_label=1)])\n    train_results.append([num_layer,accuracy_score(y_train, yp_train),precision_score(y_train, yp_train,pos_label=1),recall_score(y_train, yp_train,pos_label=1)])\n\n\n\n# INSERT CODE TO GENERATE THE THREE PLOTS BELOW (SEE EXPECTED OUTPUT FOR EXAMPLE)\n\n# NOTE: THERE IS A TYPO IN THE THIRD PLOT, IT SHOULD BE RECALL IN THE Y-AXIS LABEL NOT ACCURACY\n\ntest_results_np = np.array(test_results)\ntrain_results_np = np.array(train_results)\n\n# Plot the accuracy for training and test sets\nplt.figure(figsize=(10, 6))\nplt.plot(train_results_np[:, 0], train_results_np[:, 1], 'bo-', label='Training (blue)')\nplt.plot(test_results_np[:, 0], test_results_np[:, 1], 'ro-', label='Test (red)')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('Accuracy')\nplt.title('Accuracy vs. Number of Layers in Decision Tree')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n# Plot the recall for Y=1 for training and test sets\nplt.figure(figsize=(10, 6))\nplt.plot(train_results_np[:, 0], train_results_np[:, 3], 'bo-', label='Training (blue)')\nplt.plot(test_results_np[:, 0], test_results_np[:, 3], 'ro-', label='Test (red)')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('Recall (Y=1)')\nplt.title('Recall (Y=1) vs. Number of Layers in Decision Tree')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbest_tree = tree.DecisionTreeClassifier(max_depth=4)\nbest_tree = best_tree.fit(x_train,y_train)\n\nyp_train=best_tree.predict(x_train)\nyp_test=best_tree.predict(x_test)\n\n\n    \n#INSERT CODE TO WRITE A FUNCTION def confusion_plot(y_data,y_pred) WHICH GENERATES A CONFUSION MATRIX PLOT AND PRINTS THE INFORMATION ABOVE (see link above for example)\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, recall_score, precision_score\n\ndef confusion_plot(y_data, y_pred):\n    # Calculate confusion matrix\n    cm = confusion_matrix(y_data, y_pred)\n    \n    accuracy = accuracy_score(y_data, y_pred)\n    negative_recall = recall_score(y_data, y_pred, pos_label=0)\n    negative_precision = precision_score(y_data, y_pred, pos_label=0)\n    positive_recall = recall_score(y_data, y_pred, pos_label=1)\n    positive_precision = precision_score(y_data, y_pred, pos_label=1)\n    \n    # Print metrics\n    print(f'ACCURACY: {accuracy}')\n    print(f'NEGATIVE RECALL (Y=0): {negative_recall}')\n    print(f'NEGATIVE PRECISION (Y=0): {negative_precision}')\n    print(f'POSITIVE RECALL (Y=1): {positive_recall}')\n    print(f'POSITIVE PRECISION (Y=1): {positive_precision}')\n    print(cm)\n    \n    # Plot confusion matrix\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n    disp.plot()\n    plt.show()\n\n\nimport graphviz\ndef plot_tree(model,X,Y):\n    model_fitted = model.fit(X,Y)\n    fig = plt.figure(figsize=(25,20))\n    _ = tree.plot_tree(model_fitted, \n                   feature_names=X.columns.tolist(),  \n                   class_names=['0','1'],\n                   filled=True)\n\n\n# RUN THE FOLLOWING CODE TO EVALUATE YOUR MODEL\nprint(\"------TRAINING------\")\nconfusion_plot(y_train,yp_train)\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)\n\n\n------TRAINING------\nACCURACY: 0.72825\nNEGATIVE RECALL (Y=0): 0.7299871575342466\nNEGATIVE PRECISION (Y=0): 0.7279097894137735\nPOSITIVE RECALL (Y=1): 0.7265088672768879\nPOSITIVE PRECISION (Y=1): 0.7285929432013769\n[[20463  7569]\n [ 7649 20319]]\n------TEST------\nACCURACY: 0.7308571428571429\nNEGATIVE RECALL (Y=0): 0.7374445557304335\nNEGATIVE PRECISION (Y=0): 0.7272470721038521\nPOSITIVE RECALL (Y=1): 0.7242904007987448\nPOSITIVE PRECISION (Y=1): 0.7345580789816288\n[[5154 1835]\n [1933 5078]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_tree(best_tree,x_train,y_train)"
  },
  {
    "objectID": "DecisionTrees/decisiontrees.html",
    "href": "DecisionTrees/decisiontrees.html",
    "title": "Decision Trees",
    "section": "",
    "text": "I apply an array of tree-based algorithms—Decision Trees (DT) and Random Forests—to unravel complex patterns within the cardiovascular dataset. Each method brings unique strengths: DTs for their clarity and ease of interpretation, Random Forests for their robustness and accuracy. Together, they form a comprehensive analytical approach that enhances the project’s predictive capabilities and provides insightful, data-driven decisions.\nApplying a Decision Tree to cardiovascular datasets involves constructing a model that uses the data’s features to infer simple rules in a hierarchical structure, akin to a flowchart. This model excels in its interpretability, allowing medical practitioners to follow the logical progression of decisions leading to a diagnosis or prediction. It is particularly useful when the goal is to understand the decision-making process, such as identifying the risk factors that lead to heart disease.\nExtending this approach, Random Forests aggregate the insights of numerous decision trees, thereby creating a ‘forest’ that is more robust and accurate. This ensemble method is especially powerful in cardiovascular studies due to its ability to handle large datasets with numerous variables, each contributing to the risk and progression of heart-related ailments. By harnessing the collective decision-making of multiple trees, Random Forests mitigate the overfitting issues prevalent in individual Decision Trees and offer a more nuanced understanding of the data.",
    "crumbs": [
      "Decision Trees"
    ]
  },
  {
    "objectID": "DecisionTrees/decisiontrees.html#introduction",
    "href": "DecisionTrees/decisiontrees.html#introduction",
    "title": "Decision Trees",
    "section": "",
    "text": "I apply an array of tree-based algorithms—Decision Trees (DT) and Random Forests—to unravel complex patterns within the cardiovascular dataset. Each method brings unique strengths: DTs for their clarity and ease of interpretation, Random Forests for their robustness and accuracy. Together, they form a comprehensive analytical approach that enhances the project’s predictive capabilities and provides insightful, data-driven decisions.\nApplying a Decision Tree to cardiovascular datasets involves constructing a model that uses the data’s features to infer simple rules in a hierarchical structure, akin to a flowchart. This model excels in its interpretability, allowing medical practitioners to follow the logical progression of decisions leading to a diagnosis or prediction. It is particularly useful when the goal is to understand the decision-making process, such as identifying the risk factors that lead to heart disease.\nExtending this approach, Random Forests aggregate the insights of numerous decision trees, thereby creating a ‘forest’ that is more robust and accurate. This ensemble method is especially powerful in cardiovascular studies due to its ability to handle large datasets with numerous variables, each contributing to the risk and progression of heart-related ailments. By harnessing the collective decision-making of multiple trees, Random Forests mitigate the overfitting issues prevalent in individual Decision Trees and offer a more nuanced understanding of the data.",
    "crumbs": [
      "Decision Trees"
    ]
  },
  {
    "objectID": "DecisionTrees/decisiontrees.html#methods",
    "href": "DecisionTrees/decisiontrees.html#methods",
    "title": "Decision Trees",
    "section": "Methods",
    "text": "Methods\nDecision Trees (DTs) stand out as a non-parametric supervised learning method, adept at both classification and regression. They aim to forge a model that predicts the value of a target variable through the learning of simple decision rules drawn from the features within the data. The method initiates at the root node, from where a multitude of paths diverge, each charting a course dictated by conditional rules. These paths culminate in the leaf nodes, which house the outcomes—the model’s predicted values—crafted through this insightful and systematic approach.\nUsing Decision Trees on the cardiovascular dataset is advantageous due to several key aspects of the algorithm that align well with the nature of medical data and the requirements of clinical decision-making processes.\nInterpretability: Medical professionals often prefer models that provide transparent reasoning for their predictions. Decision Trees offer clear visualizations of the path taken to reach a conclusion, resembling clinical decision charts that practitioners are accustomed to, thus facilitating easier validation and trust in the model’s predictions.\nHandling Non-linear Relationships: Cardiovascular data often involve complex, non-linear relationships between features and outcomes. Decision Trees can capture these non-linearities without the need for transformation of variables, as they recursively partition the space in a way that maximizes the separation of the classes or reduction of variance.\nFeature Importance: Decision Trees inherently perform feature selection by prioritizing splits on the most informative features. This can be particularly useful in a clinical setting to identify and rank risk factors that are most predictive of cardiovascular events.\n\n\n\n\n\n\nWhy Decision Trees?\n\n\n\n\nInterpretable: They mimic human decision-making better than many other algorithms, making their decisions easy to understand.\nVersatile: Capable of performing both classification and regression, they can handle a variety of data types.\nNon-Parametric: They make no assumptions about the distribution of data, making them a robust option for real-world data.\n\n\n\n\n\n\n\n\n\nThe methodology for applying Decision Trees to a cardiovascular dataset involves several key steps:\n\n\n\n\n\n\nData Preprocessing: This includes cleaning the data, handling missing values, encoding categorical variables, and normalizing or standardizing numerical features if necessary.\nFeature Engineering: New features, such as Body Mass Index (BMI), could be derived from existing data to provide additional insight into the patient’s health status.\nModel Construction: A Decision Tree model is built using the preprocessed data, where the algorithm iteratively splits the data into subsets based on certain criteria, aiming to maximize the homogeneity of the target variable within each subset.\nModel Tuning: Parameters such as tree depth, minimum samples per leaf, and splitting criteria are tuned to optimize model performance and prevent overfitting.\nValidation: The model’s performance is assessed using appropriate metrics like accuracy, precision, recall, and the area under the ROC curve for classification tasks or mean squared error for regression.\nInterpretation: The resulting tree is analyzed to understand the decision paths and rules it has learned, which features are most important for predictions, and how different feature values affect the outcome.",
    "crumbs": [
      "Decision Trees"
    ]
  },
  {
    "objectID": "DecisionTrees/decisiontrees.html#class-distribution",
    "href": "DecisionTrees/decisiontrees.html#class-distribution",
    "title": "Decision Trees",
    "section": "Class Distribution",
    "text": "Class Distribution\n\n\nCode\nimport pandas as pd\ncardio=pd.read_csv('../Data/01-modified-data/cardiovascular_numeric_final.csv')\ncount0 = len(cardio[cardio['cardio'] == 0])\ncount1 = len(cardio[cardio['cardio'] == 1])\ntotal = count0  + count1  \nprop1 = count0  / total\nprop2 = count1 / total\n\nprint(\"Number of points with cardio=0:\", count0, prop1)\nprint(\"Number of points with cardio=1:\", count1, prop2)\n\n\nNumber of points with cardio=0: 33577 0.5181156065796377\nNumber of points with cardio=1: 31229 0.4818843934203623\n\n\nFrom the code above, there are 31,229 data records labeled as cardiovascular disease, and for non-cardiovascular disease, there are 33,557 records. The split is approximately 1:1, which guarantees a large number of data points for each class label. When splitting the dataset, attention must be paid to maintain a similar balance between the two classes to ensure an effective training and test data split.",
    "crumbs": [
      "Decision Trees"
    ]
  },
  {
    "objectID": "DecisionTrees/decisiontrees.html#baseline-model-for-comparison",
    "href": "DecisionTrees/decisiontrees.html#baseline-model-for-comparison",
    "title": "Decision Trees",
    "section": "Baseline Model for Comparison",
    "text": "Baseline Model for Comparison\n\n\nCode\n#Baseline Random Classifier\nimport numpy as np\nimport pandas as pd\nimport random\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\ndf=pd.read_csv('../Data/01-modified-data/cardiovascular_numeric_final.csv')\nY=df[['cardio']]\nnp.random.seed(12)\n# RANDOM CLASSIFIER \ndef random_classifier(y_data):\n    ypred=[];\n    max_label=np.max(y_data); #print(max_label)\n    for i in range(0,len(y_data)):\n        ypred.append(int(np.floor((max_label+1)*np.random.uniform(0,1))))\n\n    print(\"-----RANDOM CLASSIFIER-----\")\n    print(\"count of prediction:\",Counter(ypred).values()) # counts the elements' frequency\n    print(\"probability of prediction:\",np.fromiter(Counter(ypred).values(), dtype=float)/len(y_data)) # counts the elements' frequency\n    print(\"accuracy\",accuracy_score(y_data, ypred))\n    print(\"percision, recall, fscore,\",precision_recall_fscore_support(y_data, ypred))\n\nprint(\"\\nBINARY CLASS: UNIFORM LOAD\")\ny=Y\nrandom_classifier(y)\n\nprint(\"\\nBINARY CLASS: NON UNIFORM LOAD\")\ny=Y\nrandom_classifier(y)\n\n\n\nBINARY CLASS: UNIFORM LOAD\n-----RANDOM CLASSIFIER-----\ncount of prediction: dict_values([32479, 32327])\nprobability of prediction: [0.50117273 0.49882727]\naccuracy 0.49902786779001945\npercision, recall, fscore, (array([0.51710336, 0.48086739]), array([0.50019358, 0.4977745 ]), array([0.50850793, 0.4891749 ]), array([33577, 31229]))\n\nBINARY CLASS: NON UNIFORM LOAD\n-----RANDOM CLASSIFIER-----\ncount of prediction: dict_values([32371, 32435])\nprobability of prediction: [0.49950622 0.50049378]\naccuracy 0.5042742955899145\npercision, recall, fscore, (array([0.52241204, 0.48617234]), array([0.50364833, 0.50494732]), array([0.51285862, 0.49538201]), array([33577, 31229]))\n\n\nA random classifier randomly picks either 1 or 0 as the predicted number, resulting in an accuracy of around 50%, as expected, due to the binary outcome. It could serve as the baseline model because all machine learning models should surpass random guessing. The machine learning model must discern patterns from the training data to predict outcomes for the test data. Random guessing does not incorporate any information from the training data and thus performs the worst among all models. When comparing tree models and random forest models, metrics such as accuracy, recall, and precision should all outperform the baseline model.",
    "crumbs": [
      "Decision Trees"
    ]
  },
  {
    "objectID": "DecisionTrees/decisiontrees.html#features-selection-optional",
    "href": "DecisionTrees/decisiontrees.html#features-selection-optional",
    "title": "Decision Trees",
    "section": "Features Selection (optional)",
    "text": "Features Selection (optional)\nFor feature selection, as there are only 12 feature variables in the dataset, all features have been retained to train the tree model and preserve the maximum amount of information.",
    "crumbs": [
      "Decision Trees"
    ]
  },
  {
    "objectID": "DecisionTrees/decisiontrees.html#model-tuning",
    "href": "DecisionTrees/decisiontrees.html#model-tuning",
    "title": "Decision Trees",
    "section": "Model Tuning",
    "text": "Model Tuning\nOne of the most importan parameters for the tree model is the ‘max_depth’ parameter. It controls how many splits a tree model could make. If ‘max_depth’ is set too high, the training tree model will make as many splits as possible and try to perfectly fit the training data. It will potentially lead to overfitting whch is a notable issue of using tree model. Therefore, hyperparameter tuning process is conducted on this ‘max_depth’ variable for constructing the tree model.\n\n\n\nAccuracy\n\n\n\n\n\nRecall\n\n\nFrom the above graphs, the best test accuracies are achieved when ‘max_depth’ is between 4 and 7. This range is reasonable as it not only attains high test accuracy but also maintains a decent level of interpretability for the tree model. Given the nature of this research, which prioritizes the correct prediction of all cardiovascular diseases, it is crucial to minimize false negatives and, hence, maximize recall. The recall graph indicates that the best recall is obtained when ‘max_depth’ is set to 4. Therefore, as a result of hyperparameter tuning, the ‘max_depth’ for the final tree model is established at 4.",
    "crumbs": [
      "Decision Trees"
    ]
  },
  {
    "objectID": "DecisionTrees/decisiontrees.html#final-results",
    "href": "DecisionTrees/decisiontrees.html#final-results",
    "title": "Decision Trees",
    "section": "Final Results",
    "text": "Final Results\nThe final tree model is constructed with ‘max_depth’ set to 4. The statistics for the training and test data are shown as follows.\n\n\n\n\n\n\n\n\n\nTraining\n\n\n\n\n\n\n\nTest\n\n\n\n\n\n\n\n\n\nTraining\n\n\n\n\n\n\n\nTest\n\n\n\n\n\nFor training data, the accuracy is at 0.7285 and the recall for Y = 1 is 0.7265 while for test data, the accuracy is at 0.7309 and the recall for Y = 1 is 0.7243. Overall, the performance is relatively strong, with an accuracy rate of over 70% for both the training and test data. Since the training and test data perform similarly, no overfitting issue is identified. From a recall perspective, the scores are also above 0.7, indicating that 70% of the true cardiovascular disease records are predicted correctly. This is a positive outcome, as the research goal focuses more on maximizing recall than precision.\n\n\n\nTree Diagram\n\n\nThe tree diagram illustrates that the primary attributes influencing the decision-making process include ‘ap_hi’, ‘age’, ‘cholesterol’, ‘gluc’, ‘bmi’, and ‘ap_lo’. At the very top, the root node is represented by ‘ap_hi’, which suggests that it is perhaps the most significant factor in this particular dataset. This node splits the dataset into two distinct groups, forming the basis for further analysis. Following this, ‘ap_hi’ branches out into two paths leading to ‘age’ and another instance of ‘ap_hi’, which are then further divided into sub-nodes. These sub-nodes, which result from subsequent splits, are referred to as decision nodes. As the process continues, each decision node evaluates additional attributes, further refining the categorization until reaching a conclusion at the leaf nodes. Tree models are relatively easy to interpret. For instance, to interpret the leftmost node: if ‘ap_hi’ is less than or equal to 129.5, ‘age’ is less than or equal to 54.5, ‘cholesterol’ is less than or equal to 2.5, and ‘age’ is again less than or equal to 45, then the class is predicted to be 0, indicating non-cardiovascular disease. This logic is reasonable because if an individual is younger with lower levels of cholesterol and blood pressure, it typically suggests that the person is healthier and in better shape, thus having a lower risk of developing cardiovascular disease.",
    "crumbs": [
      "Decision Trees"
    ]
  },
  {
    "objectID": "DecisionTrees/decisiontrees.html#methods-1",
    "href": "DecisionTrees/decisiontrees.html#methods-1",
    "title": "Decision Trees",
    "section": "Methods",
    "text": "Methods\nThe Random Forest method is a type of ensemble learning technique, particularly useful for classification and regression tasks. The method operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random Forests belong to the broader class of ensemble methods which operate by building multiple models and aggregating their predictions.\n\n\n\n\n\n\nKey Features of Random Forest:\n\n\n\n\n\n\nEnsemble of Decision Trees: It combines the predictions from multiple decision tree models to produce a more accurate and stable prediction than any individual tree.\nRandomness: When building trees, each tree in a Random Forest is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set. Moreover, when splitting nodes, the selected features are chosen from a random subset of the features. This ensures diversity among the trees, which adds to the robustness of the model.\nReduction of Overfitting: Unlike single decision trees, which can be prone to overfitting, Random Forests mitigate this by averaging or ‘voting’ across the forest of trees, which tends to cancel out biases and variances.\nVariable Importance: Random Forests have the ability to provide a measure of feature importance by observing how much prediction errors increase when data for that feature is permuted while all others are left unchanged.\nVersatility: They can be used for both categorical and continuous inputs and outputs, and they inherently perform multiclass classification. Non-parametric Method: The Random Forest algorithm makes no underlying assumptions about the distribution of data.\n\n\n\n\n\n\n\n\n\n\nRandom Forest Algorithm Steps:\n\n\n\n\n\n\nBootstrap Sampling: Select N random samples from the dataset with replacement to create a bootstrap dataset.\nTree Building:\n\n\nGrow a decision tree from the bootstrap dataset. At each node:\nSelect a random subset of features.\nChoose the best split among those features to partition the data. Split the node into child nodes.\n\n\nRepeat: Repeat steps 1 and 2 to create a predetermined number of trees, adding each new tree to the forest.\nAggregation:\n\n\nFor classification tasks, use majority voting from each decision tree.\nFor regression tasks, take the average prediction from all decision trees.\n\n\nPrediction: To predict new data, run the data through each tree in the forest and use the majority vote or average as the final prediction.",
    "crumbs": [
      "Decision Trees"
    ]
  },
  {
    "objectID": "DecisionTrees/decisiontrees.html#class-distribution-1",
    "href": "DecisionTrees/decisiontrees.html#class-distribution-1",
    "title": "Decision Trees",
    "section": "Class Distribution",
    "text": "Class Distribution\nRefer to the class distribution in the Decision Tree section for additional details.",
    "crumbs": [
      "Decision Trees"
    ]
  },
  {
    "objectID": "DecisionTrees/decisiontrees.html#baseline-model-for-comparison-1",
    "href": "DecisionTrees/decisiontrees.html#baseline-model-for-comparison-1",
    "title": "Decision Trees",
    "section": "Baseline Model for Comparison",
    "text": "Baseline Model for Comparison\nRefer to the same baseline model in the Decision Tree section for additional details.",
    "crumbs": [
      "Decision Trees"
    ]
  },
  {
    "objectID": "DecisionTrees/decisiontrees.html#features-selection-optional-1",
    "href": "DecisionTrees/decisiontrees.html#features-selection-optional-1",
    "title": "Decision Trees",
    "section": "Features Selection (optional)",
    "text": "Features Selection (optional)\nIn the context of Random Forest, a subset of features is selected when building each tree within the forest. Therefore, additional feature selection will not be conducted during the Random Forest model building process.",
    "crumbs": [
      "Decision Trees"
    ]
  },
  {
    "objectID": "DecisionTrees/decisiontrees.html#model-tuning-1",
    "href": "DecisionTrees/decisiontrees.html#model-tuning-1",
    "title": "Decision Trees",
    "section": "Model Tuning",
    "text": "Model Tuning\nDuring the model tuning phase for the Random Forest algorithm, I opted to adjust the parameters ‘max_depth’, ‘max_features’, and ‘n_estimators’. The parameter ‘max_depth’, also adjusted in the Decision Tree section, establishes the tree’s maximum depth. ‘max_features’ determines the number of features evaluated when identifying the optimal split during the tree’s training process. In a Random Forest, this parameter introduces an element of randomness to the model. Rather than selecting the best feature for each node split, it chooses from a random subset of features, which results in a variety of trees. The ‘n_estimators’ parameter indicates the number of trees within the Random Forest. This represents the ensemble’s size; typically, more trees enhance performance but extend the training duration.\nThe outcomes of the hyperparameter tuning are documented as follows.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the aforementioned results, the parameters that yield the highest accuracies are {‘max_depth’: 8, ‘max_features’: 5, ‘n_estimators’: 200}. These parameters will be employed to construct the final Random Forest model.",
    "crumbs": [
      "Decision Trees"
    ]
  },
  {
    "objectID": "DecisionTrees/decisiontrees.html#final-results-1",
    "href": "DecisionTrees/decisiontrees.html#final-results-1",
    "title": "Decision Trees",
    "section": "Final Results",
    "text": "Final Results\n\n\n\n\n\n\n\n\n\nTrain\n\n\n\n\n\n\n\nTest\n\n\n\n\n\n\n\n\n\nTrain\n\n\n\n\n\n\n\nTest\n\n\n\n\n\nFor training data, the accuracy is at 0.7427 and the recall for Y = 1 is 0.6687 while for test data, the accuracy is at 0.7316 and the recall for Y = 1 is 0.6523. Overall, the performance is moderatly strong, with an accuracy rate of over 70% for both the training and test data. Since the training and test data perform similarly, no overfitting issue is identified. From a recall perspective, the scores are also above 0.65, indicating that 65% of the true cardiovascular disease records are predicted correctly. This is a positive outcome, as the research goal focuses more on maximizing recall than precision.\n\n\n\nVariable\n\n\nFrom the variable importance graph displayed above, ‘bmi’, ‘age’, ‘cholesterol’, ‘ap_lo’, and ‘ap_hi’ emerge as the top five most important variables in the Random Forest model building process. These results align with those obtained using other machine learning methods and also mirror the findings of the feature selection section discussed in previous segments. These key variables are crucial in relation to cardiovascular disease during the model training process for Random Forest and should receive more focus in future research endeavors.",
    "crumbs": [
      "Decision Trees"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Portfolio Project: A Narrative Website",
    "section": "",
    "text": "Projct Summary: This website is about a data science research project on cardiovascular disease. The goal of this project is to identify key characteristics that have a high impact on cardiovascular health.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "DataGathering/datagathering.html",
    "href": "DataGathering/datagathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "Data gathering is a fundamental aspect of any research or analysis project. The primary purpose of collecting data is to acquire accurate and relevant information that can answer specific research questions or test hypotheses. This process enables researchers and analysts to gain insights into patterns, trends, and relationships within the subject matter. Without proper data gathering, any analysis would be based on assumptions or incomplete information, leading to potentially flawed or inconclusive results. Moreover, in fields like science, healthcare, economics, and social research, data gathering is crucial for drawing evidence-based conclusions that can inform policy, drive innovation, or guide decision-making. By carefully designing and executing the data collection process, we ensure the validity, reliability, and representativeness of the data, which are essential for the integrity and credibility of the analysis.",
    "crumbs": [
      "Data Gathering"
    ]
  },
  {
    "objectID": "DataGathering/datagathering.html#why-is-data-gathering-important",
    "href": "DataGathering/datagathering.html#why-is-data-gathering-important",
    "title": "Data Gathering",
    "section": "",
    "text": "Data gathering is a fundamental aspect of any research or analysis project. The primary purpose of collecting data is to acquire accurate and relevant information that can answer specific research questions or test hypotheses. This process enables researchers and analysts to gain insights into patterns, trends, and relationships within the subject matter. Without proper data gathering, any analysis would be based on assumptions or incomplete information, leading to potentially flawed or inconclusive results. Moreover, in fields like science, healthcare, economics, and social research, data gathering is crucial for drawing evidence-based conclusions that can inform policy, drive innovation, or guide decision-making. By carefully designing and executing the data collection process, we ensure the validity, reliability, and representativeness of the data, which are essential for the integrity and credibility of the analysis.",
    "crumbs": [
      "Data Gathering"
    ]
  },
  {
    "objectID": "DataGathering/datagathering.html#record-data-cardio_train.xlsx",
    "href": "DataGathering/datagathering.html#record-data-cardio_train.xlsx",
    "title": "Data Gathering",
    "section": "1. Record Data: cardio_train.xlsx",
    "text": "1. Record Data: cardio_train.xlsx\nData Description The dataset in focus is designed to study various features associated with the presence or absence of cardiovascular disease. These features are categorized into three main types based on their nature and source.\nLink to the dataset: https://www.kaggle.com/datasets/sulianova/cardiovascular-disease-dataset/data.\n\nObjective Features: These features represent factual information. Age: Measured in days and stored as an integer. Height: Measured in centimeters and stored as an integer. Weight: Measured in kilograms and stored as a float. Gender: This is a categorical code.\nExamination Features: These are results derived from medical examinations. Systolic Blood Pressure (ap_hi): An integer value representing the pressure in arteries during the contraction of the heart muscle. Diastolic Blood Pressure (ap_lo): An integer value representing the pressure in the arteries when the heart muscle is resting between beats. Cholesterol: Categorized into three levels: 1: Normal 2: Above Normal 3: Well Above Normal Glucose (gluc): Similarly, categorized into three levels: 1: Normal 2: Above Normal 3: Well Above Normal\nSubjective Features: These are based on information provided by the patient. Smoking (smoke): A binary value where 1 indicates a smoker and 0 indicates a non-smoker. Alcohol Intake (alco): A binary value where 1 represents consumption and 0 indicates otherwise. Physical Activity (active): A binary value, where 1 indicates that the patient is physically active and 0 indicates they are not. Target Variable:\n\nPresence or Absence of Cardiovascular Disease (cardio): This binary variable serves as the outcome of interest, where 1 indicates the presence of cardiovascular disease and 0 indicates its absence. All the values in this dataset were gathered at the time of the medical examination, ensuring the contemporaneity of the information.\n\nDataset Dimension This dataset has 70,000 records with 13 variables. All 13 variables are continuous variables related to the biological features of individuals. See below tables for detailed information\n\n\n\nCode\nlibrary(skimr)\nlibrary(knitr)\nlibrary(tidyverse)\nlibrary(\"readxl\")\ncardio &lt;- read_excel('../Data/00-raw-data/cardio_train.xlsx')\nskim(cardio)\n\n\n\nData summary\n\n\nName\ncardio\n\n\nNumber of rows\n70000\n\n\nNumber of columns\n13\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n13\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nid\n0\n1\n49972.42\n28851.30\n0\n25006.75\n50001.5\n74889.25\n99999\n▇▇▇▇▇\n\n\nage\n0\n1\n19468.87\n2467.25\n10798\n17664.00\n19703.0\n21327.00\n23713\n▁▂▆▇▇\n\n\ngender\n0\n1\n1.35\n0.48\n1\n1.00\n1.0\n2.00\n2\n▇▁▁▁▅\n\n\nheight\n0\n1\n164.36\n8.21\n55\n159.00\n165.0\n170.00\n250\n▁▁▇▂▁\n\n\nweight\n0\n1\n74.21\n14.40\n10\n65.00\n72.0\n82.00\n200\n▁▇▂▁▁\n\n\nap_hi\n0\n1\n128.82\n154.01\n-150\n120.00\n120.0\n140.00\n16020\n▇▁▁▁▁\n\n\nap_lo\n0\n1\n96.63\n188.47\n-70\n80.00\n80.0\n90.00\n11000\n▇▁▁▁▁\n\n\ncholesterol\n0\n1\n1.37\n0.68\n1\n1.00\n1.0\n2.00\n3\n▇▁▂▁▁\n\n\ngluc\n0\n1\n1.23\n0.57\n1\n1.00\n1.0\n1.00\n3\n▇▁▁▁▁\n\n\nsmoke\n0\n1\n0.09\n0.28\n0\n0.00\n0.0\n0.00\n1\n▇▁▁▁▁\n\n\nalco\n0\n1\n0.05\n0.23\n0\n0.00\n0.0\n0.00\n1\n▇▁▁▁▁\n\n\nactive\n0\n1\n0.80\n0.40\n0\n1.00\n1.0\n1.00\n1\n▂▁▁▁▇\n\n\ncardio\n0\n1\n0.50\n0.50\n0\n0.00\n0.0\n1.00\n1\n▇▁▁▁▇",
    "crumbs": [
      "Data Gathering"
    ]
  },
  {
    "objectID": "DataGathering/datagathering.html#text-data-news-api-using-python",
    "href": "DataGathering/datagathering.html#text-data-news-api-using-python",
    "title": "Data Gathering",
    "section": "2. Text Data: News API using Python",
    "text": "2. Text Data: News API using Python\n\nData Description\nIn the scope of my research on cardiovascular topics, I leveraged Python to streamline the data collection and processing. Beginning with the Wikipedia-API, I queried it to obtain text data relevant to my subject. The results from this query were systematically organized into three distinct dictionaries. From these, I extracted essential textual elements, specifically the title and description.\nSubsequent to the extraction, I embarked on a thorough text cleaning process to ensure the data’s reliability and coherence. With this cleaned dataset, I then visualized the frequency of terms by generating a word cloud. This visualization not only underscored the primary terms associated with the cardiovascular domain but also highlighted words that potentially share a relationship with my core topic. This approach allowed for an intuitive understanding of the conceptual landscape surrounding cardiovascular research.\nThe subsequent chart illustrates the frequency with which each source was referenced in the data pull. Notably, terms such as ‘risk’, ‘heart’, ‘new’, and ‘health’ stand out. These words are particularly relevant and resonate strongly with my primary focus: cardiovascular.\nLink to the API code: https://github.com/anly501/dsan-5000-project-Sel272/blob/main/DSAN5000project/DataGathering/datagatheringAPI_python.ipynb\n\n\n\nWord Cloud",
    "crumbs": [
      "Data Gathering"
    ]
  },
  {
    "objectID": "DataGathering/datagathering.html#text-data-new-york-times-api-using-r",
    "href": "DataGathering/datagathering.html#text-data-new-york-times-api-using-r",
    "title": "Data Gathering",
    "section": "3. Text Data: New York Times API using R",
    "text": "3. Text Data: New York Times API using R\n\nData Description\nLeveraging the New York Times (NYT) API with the capabilities of R programming presents a powerful resource for researchers focused on cardiovascular risk prediction. By securing an API key from the NYT Developer’s portal, spotlighting “Heart” and “Disease” to find the related key words. With these articles in hand, R’s comprehensive analytical toolbox facilitates profound text examination, enabling researchers to discern trends, identify patterns, and uncover pivotal insights from a globally recognized news outlet. This synergy significantly augments the depth and scope of their exploration into heart health. The following chart shows the frequency of sources for the text pulled.\nLink to the API code: https://github.com/anly501/dsan-5000-project-Sel272/blob/main/DSAN5000project/DataGathering/datagatheringAPI_R.Rmd.\n\n\n\nFrequency",
    "crumbs": [
      "Data Gathering"
    ]
  },
  {
    "objectID": "DataGathering/datagatheringAPI_python_text.html",
    "href": "DataGathering/datagatheringAPI_python_text.html",
    "title": "Research Project",
    "section": "",
    "text": "# Import\nimport requests\nimport json\nimport re\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n#Set credentials \nbaseURL = \"https://newsapi.org/v2/everything?\"\ntotal_requests=2\nverbose=True\n\n# THIS CODE WILL NOT WORK UNLESS YOU INSERT YOUR API KEY IN THE NEXT LINE\nAPI_KEY='9cf6d393469e41738d84303a9c840fd1'\n\n\nTOPICS = ['Cardiovascular', 'Cancer','Stroke','Diabetes']\n\n\ndef extract (x):\n\n    URLpost = {'apiKey': API_KEY,\n                'q': '+'+ TOPICS[x],\n                'sortBy': 'relevancy',\n                'totalRequests': 1}\n\n    # print(baseURL)\n    # print(URLpost)\n\n    #GET DATA FROM API\n    response = requests.get(baseURL, URLpost) #request data from the server\n    # print(response.url);  \n    response = response.json() #extract txt data from request into json\n\n    # PRETTY PRINT\n    # https://www.digitalocean.com/community/tutorials/python-pretty-print-json\n\n    # print(json.dumps(response, indent=2))\n\n    # #GET TIMESTAMP FOR PULL REQUEST\n    from datetime import datetime\n    timestamp = datetime.now().strftime(\"%Y-%m-%d-H%H-M%M-S%S\")\n\n    # SAVE TO FILE \n    with open(timestamp+'-newapi-raw-data.json', 'w') as outfile:\n        json.dump(response, outfile, indent=4)\n        \n    return response\n\n\n\nresponse1 = extract(0)\nresponse2 = extract(1)\nresponse3 = extract(2)\nresponse4 = extract(3)\n\n\n# Utility function\n# Function to clean strings\ndef string_cleaner(input_string):\n    try: \n        out=re.sub(r\"\"\"\n                    [,.;@#?!&$-]+  # Accept one or more copies of punctuation\n                    \\ *           # plus zero or more copies of a space,\n                    \"\"\",\n                    \" \",          # and replace it with a single space\n                    input_string, flags=re.VERBOSE)\n\n        #REPLACE SELECT CHARACTERS WITH NOTHING\n        out = re.sub('[’.]+', '', input_string)\n\n        #ELIMINATE DUPLICATE WHITESPACES USING WILDCARDS\n        out = re.sub(r'\\s+', ' ', out)\n\n        #CONVERT TO LOWER CASE\n        out=out.lower()\n    except:\n        print(\"ERROR\")\n        out=''\n    return out\n\n\ndef clean (response):\n    article_list=response['articles']   #list of dictionaries for each article\n    article_keys=article_list[0].keys()\n    # print(\"AVAILABLE KEYS:\")\n    # print(article_keys)\n    index=0\n    cleaned_data=[];  \n    for article in article_list:\n        tmp=[]\n        # if(verbose):\n        #     print(\"#------------------------------------------\")\n        #     print(\"#\",index)\n        #     print(\"#------------------------------------------\")\n\n        for key in article_keys:\n            # if(verbose):\n            #     print(\"----------------\")\n            #     print(key)\n            #     print(article[key])\n            #     print(\"----------------\")\n\n            # if(key=='source'):\n            #     src=string_cleaner(article[key]['name'])\n            #     tmp.append(src) \n\n            # if(key=='author'):\n            #     author=string_cleaner(article[key])\n            #     #ERROR CHECK (SOMETIMES AUTHOR IS SAME AS PUBLICATION)\n            #     if(src in author): \n            #         print(\" AUTHOR ERROR:\",author);author='NA'\n            #     tmp.append(author)\n\n            # if(key=='title'):\n            #     tmp.append(string_cleaner(article[key]))\n                \n            if(key=='description'):\n                tmp.append(string_cleaner(article[key]))\n\n            # if(key=='content'):\n            #     tmp.append(string_cleaner(article[key]))\n\n            # if(key=='publishedAt'):\n            #     #DEFINE DATA PATERN FOR RE TO CHECK  .* --&gt; wildcard\n            #     ref = re.compile('.*-.*-.*T.*:.*:.*Z')\n            #     date=article[key]\n            #     if(not ref.match(date)):\n            #         print(\" DATE ERROR:\",date); date=\"NA\"\n            #     tmp.append(date)\n\n        cleaned_data.append(tmp)\n        index+=1\n\n    return cleaned_data\n\n\n# Create DataFrame\ncleaned_data1 = clean(response1)\ndf1 = pd.DataFrame(cleaned_data1)\ncleaned_data2 = clean(response2)\ndf2 = pd.DataFrame(cleaned_data2)\ncleaned_data3 = clean(response3)\ndf3 = pd.DataFrame(cleaned_data3)\ncleaned_data4 = clean(response4)\ndf4 = pd.DataFrame(cleaned_data4)\ndf1['label'] = 1\ndf2['label'] = 0\ndf3['label'] = 0\ndf4['label'] = 0\n\n\n\ncardiovascular_text = pd.concat([df1, df2, df3, df4])\ncardiovascular_text = cardiovascular_text.rename(columns={0: 'text'})\ncardiovascular_text.head()\n\n\n\n\n\n\n\n\ntext\nlabel\n\n\n\n\n0\nresearchers increasingly find that the effects...\n1\n\n\n1\ntogether, ai imaging and ai genetic analysis m...\n1\n\n\n2\nlas enfermedades relacionadas con nuestro sist...\n1\n\n\n3\na monthly dose of the vitamin was found to low...\n1\n\n\n4\nauthor cat bohannon says there's a \"male norm\"...\n1\n\n\n\n\n\n\n\n\ncardiovascular_text.shape\n\n(400, 2)\n\n\n\ncardiovascular_text.to_csv('../data/01-modified-data/cardiovascular_text_final.csv')"
  },
  {
    "objectID": "Data/00-raw-data/data.html",
    "href": "Data/00-raw-data/data.html",
    "title": "Data",
    "section": "",
    "text": "Data is stored in the github repository：https://github.com/anly501/dsan-5000-project-Sel272.git"
  },
  {
    "objectID": "DimensionalityReduction/dimensionalityreduction.html",
    "href": "DimensionalityReduction/dimensionalityreduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "The objective is to investigate and illustrate the efficacy of PCA and t-SNE for dimensionality reduction in complex, multimodal datasets, ensuring key information is retained and visualization is improved. By applying PCA and t-SNE to practical situations, we seek to enrich our experience and understanding in simplifying data structures and enhancing their graphical representation.\nData Selection:\nDimensionality reduction techniques can only be performed on numeric features. Therefore, a subset of numeric variables has been selected for conducting PCA and t-SNE. The following data has been chosen:\n\n\nCode\nimport pandas as pd\n\ndata=pd.read_csv('../Data/01-modified-data/cardiovascular_numeric_final.csv')\ndata = data.drop(data.columns[0], axis=1)\n\nnumerical_cols = [\"age\", \"height\", \"weight\", \"ap_hi\", \"ap_lo\", \"bmi\"]\nX = data[numerical_cols]\nY = data['cardio']\nX.head()\n\n\n\n\n\n\n\n\n\nage\nheight\nweight\nap_hi\nap_lo\nbmi\n\n\n\n\n0\n50\n168\n62\n110\n80\n21.97\n\n\n1\n55\n156\n85\n140\n90\n34.93\n\n\n2\n52\n165\n64\n130\n70\n23.51\n\n\n3\n48\n169\n82\n150\n100\n28.71\n\n\n4\n48\n156\n56\n100\n60\n23.01\n\n\n\n\n\n\n\nTools or Libraries:\nCommon Python packages have been installed for this approach. The libraries used are as follows:",
    "crumbs": [
      "Dimensionality Reduction"
    ]
  },
  {
    "objectID": "DimensionalityReduction/dimensionalityreduction.html#a-brief-proposal",
    "href": "DimensionalityReduction/dimensionalityreduction.html#a-brief-proposal",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "The objective is to investigate and illustrate the efficacy of PCA and t-SNE for dimensionality reduction in complex, multimodal datasets, ensuring key information is retained and visualization is improved. By applying PCA and t-SNE to practical situations, we seek to enrich our experience and understanding in simplifying data structures and enhancing their graphical representation.\nData Selection:\nDimensionality reduction techniques can only be performed on numeric features. Therefore, a subset of numeric variables has been selected for conducting PCA and t-SNE. The following data has been chosen:\n\n\nCode\nimport pandas as pd\n\ndata=pd.read_csv('../Data/01-modified-data/cardiovascular_numeric_final.csv')\ndata = data.drop(data.columns[0], axis=1)\n\nnumerical_cols = [\"age\", \"height\", \"weight\", \"ap_hi\", \"ap_lo\", \"bmi\"]\nX = data[numerical_cols]\nY = data['cardio']\nX.head()\n\n\n\n\n\n\n\n\n\nage\nheight\nweight\nap_hi\nap_lo\nbmi\n\n\n\n\n0\n50\n168\n62\n110\n80\n21.97\n\n\n1\n55\n156\n85\n140\n90\n34.93\n\n\n2\n52\n165\n64\n130\n70\n23.51\n\n\n3\n48\n169\n82\n150\n100\n28.71\n\n\n4\n48\n156\n56\n100\n60\n23.01\n\n\n\n\n\n\n\nTools or Libraries:\nCommon Python packages have been installed for this approach. The libraries used are as follows:",
    "crumbs": [
      "Dimensionality Reduction"
    ]
  },
  {
    "objectID": "DimensionalityReduction/dimensionalityreduction.html#methods",
    "href": "DimensionalityReduction/dimensionalityreduction.html#methods",
    "title": "Dimensionality Reduction",
    "section": "Methods",
    "text": "Methods\n\nPrincipal Component Analysis\n\nPCA, or Principal Component Analysis, is a prevalent technique in statistical data analysis for reducing the complexity of high-dimensional data while preserving essential trends and patterns. The method works by calculating the first principal component to account for the largest variance in the data, with each subsequent component designed to have the highest variance possible while remaining orthogonal to the previous components. It is essential to standardize the selected data before performing PCA, particularly when the variables have different scales. This approach is commonly employed for dimensionality reduction, pattern recognition, and data preparation for predictive modeling.\n\nT-distributed Stochastic Neighbor Embedding\n\nT-SNE, or t-distributed Stochastic Neighbor Embedding, is a non-linear technique that excels in visualizing complex high-dimensional data within a low-dimensional space by preserving the original structure and identifying clusters. It achieves this by mapping similar points closely together and separating dissimilar ones based on probability distributions, minimizing the divergence between the data’s high-dimensional and low-dimensional representations. However, due to its non-convex cost function, t-SNE can yield different visualizations upon varying initializations. While powerful for visual exploration, it’s advisable to pre-process data using methods like PCA for dense datasets or TruncatedSVD for sparse datasets, especially when dealing with more than 50 features.",
    "crumbs": [
      "Dimensionality Reduction"
    ]
  },
  {
    "objectID": "DimensionalityReduction/dimensionalityreduction.html#results-and-implementations",
    "href": "DimensionalityReduction/dimensionalityreduction.html#results-and-implementations",
    "title": "Dimensionality Reduction",
    "section": "Results and Implementations",
    "text": "Results and Implementations\n\nPrincipal Components Analysis\nThe first step after applying the PCA technique to the dataset is to determine how many principal components to select. To facilitate this, a graph of the variance explained by the principal components is created, which visualizes the importance of each principal component. The results are presented as follows:\n\nFrom the graph, it can be concluded that using the first four components explains 90% of the variance. By employing these four components, the dimensionality of the dataset can be effectively reduced from six to four while preserving most of the information from the original dataset. Therefore, four PCA components have been selected as a result of the PCA analysis.\nAfter applying the PCA technique and selecting the first four components, visualizations using the PCA axes are created to display the effectiveness of using the PCA components to separate datasets based on the response variable. Since higher dimension visualization is challenging, 2-D graphs are generated to illustrate the differences. The combinations of the 1st and 2nd, 2nd and 3rd, and 3rd and 4th components are displayed as follows.\n\n\n\n\n\n\n\n\n\n1st & 2nd\n\n\n\n\n\n\n\n2nd & 3rd\n\n\n\n\n\n\n\n\n\n3rd & 4th\n\n\n\n\n\n\n\n1st & 3rd\n\n\n\n\n\nBased on the above visualizations, it is clear that the 1st, 2nd, and 3rd components effectively distinguish the different classes of the response variable “cardio”. The presence of cardiovascular diseases and non-cardiovascular diseases is clearly separated into two groups in the graphs. This indicates the strength of using the first four PCA components for further supervised model training processes to predict the response variable.\n\n\n\nthe Explained Variance by Each Component & the Optimal Number of Components\n\n\n\n\nT-distributed Stochastic Neighbor Embedding\nTo perform t-SNE on the dataset, begin by choosing the target number of dimensions for the output, commonly set to two for visualization purposes. It’s important to preprocess the dataset, standardizing it if necessary, especially if the features have different scales.\nThe second step is to choose the value for one of the most important hyperparameters; perplexity. Perplexity in t-SNE, ranging typically from 5 to 50, acts as a knob controlling the emphasis on local versus global data structure in the resulting visualization. The right perplexity value, often found through experimentation, is crucial for meaningful low-dimensional embeddings, balancing the capture of global trends and local clusters within the data.\nThe range of perplexity values tested includes 1, 5, 10, 30, and 50. Due to the large volume of the dataset, only these five candidate values are tested to reduce the extensive processing time. Below are the results for each perplexity value after mapping the dataset into a 2-dimensional space.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen perplexity is set to 1, it captures more local structures. Perplexity reflects the number of close neighbors each point considers. With low perplexity, fewer points are considered neighbors, resulting in a scattered plot with no distinct groups. As perplexity increases, points begin to separate out, preserving more global structure. At a perplexity of 30, there is a distinct separation between the two ‘cardio’ classes. Therefore, for optimal visualization of the global structure of the data, a perplexity value of 30 is chosen.",
    "crumbs": [
      "Dimensionality Reduction"
    ]
  },
  {
    "objectID": "DimensionalityReduction/dimensionalityreduction.html#evaluation-and-comparison",
    "href": "DimensionalityReduction/dimensionalityreduction.html#evaluation-and-comparison",
    "title": "Dimensionality Reduction",
    "section": "Evaluation and Comparison",
    "text": "Evaluation and Comparison\nFrom a data structure perspective, PCA excels at capturing the linear relationships between features by creating new dimensions that encapsulate the maximum variance from the original dataset. However, some information is inevitably lost during the PCA process since only a few principal components are selected for further analysis. In contrast, t-SNE is adept at capturing non-linear relationships. An optimal perplexity value must be chosen to balance the preservation of global and local structures of the original data.\nVisually, both PCA and t-SNE can map high-dimensional data into lower-dimensional spaces, such as 2-D or 3-D, facilitating pattern detection through different components, thereby comparing the effectiveness of the two techniques.\nThe choice between PCA and t-SNE is largely dictated by the research objective. PCA is preferable when there’s a linear relationship between the response and feature variables. Conversely, t-SNE may be superior for more complex, non-linear relationships. Moreover, the number of PCA components and the perplexity value for t-SNE are crucial in deciding between the two, especially when considering the trade-off between preserving local and global structures.\nFor this research project, PCA is favored over t-SNE based on visualization results. The first three components distinctly separate data points across different “cardio” response variable levels. In subsequent supervised learning model training, PCA components could replace the original features to reduce dimensions and enhance processing speed.",
    "crumbs": [
      "Dimensionality Reduction"
    ]
  }
]