summarize(max_balance = max(balance), min_balance = min(balance))
transactions3 <- transactions1 %>% group_by(account_id) %>%
summarize(max_withdrawal = max(amount), min_withdrawal = min(amount))
transactions4 <- transactions1 %>% group_by(account_id) %>%
summarize(cc_payments = n())
df6 <- left_join(df5, transactions2, by = "account_id")
df7 <- left_join(df6, transactions3, by = "account_id")
df8 <- left_join(df7, transactions4, by = "account_id")
df8$cc_payments[is.na(df8$cc_payments)] <- 0
df8$account_id <- as.integer(df8$account_id)
df8$cc_payments <- as.integer(df8$cc_payments)
# Reorder columns
desired_order <- c('account_id', 'district_name', 'open_date', 'statement_frequency', 'num_customers', 'credit_cards', 'loan', 'loan_amount', 'loan_payments', 'loan_term', 'loan_status', 'loan_default', 'max_withdrawal', 'min_withdrawal', 'cc_payments', 'max_balance', 'min_balance')
df_reordered <- df8 %>% select(all_of(desired_order))
write.csv(df_reordered, 'analytical_R.csv', row.names = FALSE)
View(df8)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
# Read the data
loans <- read_csv("../data/loans.csv")
# Rename 'id' to 'loan_id'
loans <- loans %>% rename(loan_id = id)
# Melt the data
loans1 <- loans %>%
gather(loan_info, loan_value, -loan_id, -account_id, -date, -amount, -payments) %>%
filter(loan_value == "X") %>%
select(-loan_value)
# Extract loan term
loan_term <- function(loan_info) {
as.integer(substr(loan_info, 1, 2))
}
# Determine loan status
loan_status <- function(loan_info) {
if (substr(loan_info, nchar(loan_info), nchar(loan_info)) %in% c("A", "B")) {
return("expired")
} else {
return("current")
}
}
# Check if there's a default
loan_default <- function(loan_info) {
if (substr(loan_info, nchar(loan_info), nchar(loan_info)) %in% c("D", "B")) {
return(TRUE)
} else {
return(FALSE)
}
}
# Apply functions and create new columns
loans1 <- loans1 %>%
mutate(
loan_term = map_int(loan_info, loan_term),
loan_status = map_chr(loan_info, loan_status),
loan_default = map_lgl(loan_info, loan_default)
) %>%
select(-loan_info)
library(tidyverse)
# Read the data
district <- read.csv("../data/districts.csv")
# Rename 'id' to 'districts_id'
district <- district %>% rename(districts_id = id)
# Extracting values from the unemployment_rate and commited_crimes columns
district$unemployment_rate <- str_replace_all(district$unemployment_rate, "\\[|\\]", "") %>% str_split(",")
district$commited_crimes <- str_replace_all(district$commited_crimes, "\\[|\\]", "") %>% str_split(",")
# Splitting the lists into separate columns
district <- district %>%
mutate(
unemployment_rate95 = sapply(unemployment_rate, `[[`, 1),
unemployment_rate96 = sapply(unemployment_rate, `[[`, 2),
commited_crimes95 = sapply(commited_crimes, `[[`, 1),
commited_crimes96 = sapply(commited_crimes, `[[`, 2)
) %>%
select(-unemployment_rate, -commited_crimes)
library(dplyr)
library(tidyr)
library(readr)
# Read data
accounts <- read.csv('../data/accounts.csv')
cards <- read.csv('../data/cards.csv')
clients <- read.csv('../data/clients.csv')
links <- read.csv('../data/links.csv')
transactions <- read.csv('../data/transactions.csv')
# Rename columns
colnames(accounts)[1] <- 'account_id'
colnames(cards)[1] <- 'card_id'
colnames(clients)[1] <- 'client_id'
colnames(links)[1] <- 'link_id'
colnames(transactions)[1] <- 'id'
# Filter links
links1 <- filter(links, type == 'owner')
# Merging
df <- left_join(links1, cards, by = "link_id")
df1 <- left_join(df, clients, by = "client_id")
df2 <- left_join(df1, accounts, by = "account_id")
# Drop rows where account_id is NA
df2 <- df2 %>% filter(!is.na(account_id))
# Merge with district and further transformations
df3 <- left_join(df2, district, by = c('district_id.x' = 'districts_id')) %>%
select(link_id, client_id, account_id, card_id, date, statement_frequency, name) %>%
rename(open_date = date, district_name = name)
df4 <- left_join(df3, loans1, by = "account_id")
df4$loan <- !is.na(df4$loan_id)
df4 <- df4 %>%
select(-date, -loan_id) %>%
rename(loan_amount = amount, loan_payments = payments)
links2 <- links %>% group_by(account_id) %>% summarize(num_customers = n())
df5 <- left_join(df4, links2, by = "account_id")
df5$credit_cards <- !is.na(df5$card_id)
df5 <- df5 %>% select(-card_id)
transactions1 <- transactions %>% filter(method == 'credit card' & type == 'debit')
transactions1b <- transactions %>% filter(type == 'debit')
transactions2 <- transactions %>% group_by(account_id) %>%
summarize(max_balance = max(balance), min_balance = min(balance))
transactions3 <- transactions1b %>% group_by(account_id) %>%
summarize(max_withdrawal = max(amount), min_withdrawal = min(amount))
transactions4 <- transactions1 %>% group_by(account_id) %>%
summarize(cc_payments = n())
df6 <- left_join(df5, transactions2, by = "account_id")
df7 <- left_join(df6, transactions3, by = "account_id")
df8 <- left_join(df7, transactions4, by = "account_id")
df8$cc_payments[is.na(df8$cc_payments)] <- 0
df8$account_id <- as.integer(df8$account_id)
df8$cc_payments <- as.integer(df8$cc_payments)
# Reorder columns
desired_order <- c('account_id', 'district_name', 'open_date', 'statement_frequency', 'num_customers', 'credit_cards', 'loan', 'loan_amount', 'loan_payments', 'loan_term', 'loan_status', 'loan_default', 'max_withdrawal', 'min_withdrawal', 'cc_payments', 'max_balance', 'min_balance')
df_reordered <- df8 %>% select(all_of(desired_order))
df_reordered
View(df_reordered)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
# Read the data
loans <- read_csv("../data/loans.csv")
# Rename 'id' to 'loan_id'
loans <- loans %>% rename(loan_id = id)
# Melt the data
loans1 <- loans %>%
gather(loan_info, loan_value, -loan_id, -account_id, -date, -amount, -payments) %>%
filter(loan_value == "X") %>%
select(-loan_value)
# Extract loan term
loan_term <- function(loan_info) {
as.integer(substr(loan_info, 1, 2))
}
# Determine loan status
loan_status <- function(loan_info) {
if (substr(loan_info, nchar(loan_info), nchar(loan_info)) %in% c("A", "B")) {
return("expired")
} else {
return("current")
}
}
# Check if there's a default
loan_default <- function(loan_info) {
if (substr(loan_info, nchar(loan_info), nchar(loan_info)) %in% c("D", "B")) {
return(TRUE)
} else {
return(FALSE)
}
}
# Apply functions and create new columns
loans1 <- loans1 %>%
mutate(
loan_term = map_int(loan_info, loan_term),
loan_status = map_chr(loan_info, loan_status),
loan_default = map_lgl(loan_info, loan_default)
) %>%
select(-loan_info)
library(tidyverse)
# Read the data
district <- read.csv("../data/districts.csv")
# Rename 'id' to 'districts_id'
district <- district %>% rename(districts_id = id)
# Extracting values from the unemployment_rate and commited_crimes columns
district$unemployment_rate <- str_replace_all(district$unemployment_rate, "\\[|\\]", "") %>% str_split(",")
district$commited_crimes <- str_replace_all(district$commited_crimes, "\\[|\\]", "") %>% str_split(",")
# Splitting the lists into separate columns
district <- district %>%
mutate(
unemployment_rate95 = sapply(unemployment_rate, `[[`, 1),
unemployment_rate96 = sapply(unemployment_rate, `[[`, 2),
commited_crimes95 = sapply(commited_crimes, `[[`, 1),
commited_crimes96 = sapply(commited_crimes, `[[`, 2)
) %>%
select(-unemployment_rate, -commited_crimes)
library(dplyr)
library(tidyr)
library(readr)
# Read data
accounts <- read.csv('../data/accounts.csv')
cards <- read.csv('../data/cards.csv')
clients <- read.csv('../data/clients.csv')
links <- read.csv('../data/links.csv')
transactions <- read.csv('../data/transactions.csv')
# Rename columns
colnames(accounts)[1] <- 'account_id'
colnames(cards)[1] <- 'card_id'
colnames(clients)[1] <- 'client_id'
colnames(links)[1] <- 'link_id'
colnames(transactions)[1] <- 'id'
# Filter links
links1 <- filter(links, type == 'owner')
# Merging
df <- left_join(links1, cards, by = "link_id")
df1 <- left_join(df, clients, by = "client_id")
df2 <- left_join(df1, accounts, by = "account_id")
# Drop rows where account_id is NA
df2 <- df2 %>% filter(!is.na(account_id))
# Merge with district and further transformations
df3 <- left_join(df2, district, by = c('district_id.x' = 'districts_id')) %>%
select(link_id, client_id, account_id, card_id, date, statement_frequency, name) %>%
rename(open_date = date, district_name = name)
df4 <- left_join(df3, loans1, by = "account_id")
df4$loan <- !is.na(df4$loan_id)
df4 <- df4 %>%
select(-date, -loan_id) %>%
rename(loan_amount = amount, loan_payments = payments)
links2 <- links %>% group_by(account_id) %>% summarize(num_customers = n())
df5 <- left_join(df4, links2, by = "account_id")
df5$credit_cards <- !is.na(df5$card_id)
df5 <- df5 %>% select(-card_id)
transactions1 <- transactions %>% filter(method == 'credit card' & type == 'debit')
transactions1b <- transactions %>% filter(type == 'debit')
transactions2 <- transactions %>% group_by(account_id) %>%
summarize(max_balance = max(balance), min_balance = min(balance))
transactions3 <- transactions1b %>% group_by(account_id) %>%
summarize(max_withdrawal = max(amount), min_withdrawal = min(amount))
transactions4 <- transactions1 %>% group_by(account_id) %>%
summarize(cc_payments = n())
df6 <- left_join(df5, transactions2, by = "account_id")
df7 <- left_join(df6, transactions3, by = "account_id")
df8 <- left_join(df7, transactions4, by = "account_id")
df8$cc_payments[is.na(df8$cc_payments)] <- 0
df8$account_id <- as.integer(df8$account_id)
df8$cc_payments <- as.integer(df8$cc_payments)
df8$credit_cards <- as.integer(df8$credit_cards)
# Reorder columns
desired_order <- c('account_id', 'district_name', 'open_date', 'statement_frequency', 'num_customers', 'credit_cards', 'loan', 'loan_amount', 'loan_payments', 'loan_term', 'loan_status', 'loan_default', 'max_withdrawal', 'min_withdrawal', 'cc_payments', 'max_balance', 'min_balance')
df_reordered <- df8 %>% select(all_of(desired_order))
df_reordered
cardio <- read_csv("cardio_train.csv")
cardio <- read_csv("../Data/cardio_train.csv")
getwd()
setwd("/Users/xueningyang/DSAN5000Project/DSAN5000project/Data")
getwd()
setwd("/Users/xueningyang/Desktop/DSAN5000Project/DSAN5000project/Data")
cardio <- read_csv("cardio_train.csv")
cardio <- read_csv("cardio_train.csv")
cardio <- read.csv("cardio_train.csv")
head(cardio)
cardio <- read.csv("cardio_train.csv")
cardio
cardio <- read.csv("cardio_train.csv")
cardio
#Analyzing the CSV File
print(is.data.frame(cardio))
print(ncol(cardio))
print(nrow(cardio))
cardio <- read.csv("cardio_train.csv", header = TRUE, sep = ",", quote = "\"",
dec = ".", fill = TRUE, comment.char = "", ...)
cardio <- read.csv("cardio_train.csv", header = TRUE, sep = ",", quote = "\"",
dec = ".", fill = TRUE, comment.char = "", ...)
cardio <- read.csv("cardio_train.csv", header = TRUE, sep = ",")
cardio
#Analyzing the CSV File
print(is.data.frame(cardio))
print(ncol(cardio))
print(nrow(cardio))
install.packages("writexl")
cardio_r <- read.table("cardio_train.csv")
cardio_r
cardio_r <- read.table("cardio_train.csv")
cardio_r
View(cardio_r)
install.packages("readxl")
cardio_r <- read_excel("cardio_train.xlsx")
library(readxl)
install.packages("readxl")
install.packages("readxl")
cardio <- read_excel("cardio_train.xlsx")
cardio <- read_xlsx("cardio_train.xlsx")
cardio <- read_excel("cardio_train.xlsx")
install.packages("readxl")
install.packages("readxl")
cardio <- read_excel("cardio_train.xlsx")
library(readxl)
cardio <- read_excel("cardio_train.xlsx")
head(cardio)
summary(cardio)n
cardio <- read_excel("cardio_train.xlsx")
head(cardio)
summary(cardio)
#Analyzing the excel File
print(is.data.frame(cardio))
print(ncol(cardio))
print(nrow(cardio))
#checking for any NA value
colSums(is.na(cardio))
names(cardio)
View(cardio)
# Compute the correlation matrix
cor_matrix <- cor(cardio)  # I'm using the built-in mtcars dataset as an example
# Create a heatmap
heatmap(cor_matrix, main="Correlation Heatmap", symm=TRUE, trace="none")
# Install and load the corrplot package
install.packages("corrplot")
library(corrplot)
# Compute the correlation matrix
cor_matrix <- cor(cardio)  # Again using mtcars dataset
# Create a correlation plot
corrplot(cor_matrix, method="circle")
cardio$age <- cardio$age / 365.25
cardio$age
cardio$age <- cardio$age / 365
cardio$age
install.packages("readxl")
cardio <- read_excel("cardio_train.xlsx")
head(cardio)
summary(cardio)
cardio$age <- cardio$age / 365.
cardio$age
cardio$age <- cardio$age / 365
cardio$age
cardio$age <- cardio$age / 365.25
cardio$age
cardio$age <- cardio$age / 365
round(cardio$age)
cardio$age1 <- cardio$age / 365
round(cardio$age1)
cardio <- read_excel("cardio_train.xlsx")
head(cardio)
summary(cardio)
cardio$age <- cardio$age / 365
round(cardio$age)
cardio <- read_excel("cardio_train.xlsx")
cardio$age <- cardio$age / 365
round(cardio$age)
head(cardio)
summary(cardio)
cardio$age <- cardio$age / 365
round(cardio$age)
write.csv(cardio, file = "cardio.csv", row.names = FALSE)
cardio <- read_excel("cardio_train.xlsx")
cardio$age <- round(cardio$age / 365)
head(cardio)
summary(cardio)
#Analyzing the excel File
print(is.data.frame(cardio))
print(ncol(cardio))
print(nrow(cardio))
#70000 obs, 13 variables
#checking for any NA value
colSums(is.na(cardio)) #No NA
# Compute the correlation matrix
cor_matrix <- cor(cardio)  # I'm using the built-in mtcars dataset as an example
# Create a heatmap
heatmap(cor_matrix, main="Correlation Heatmap", symm=TRUE, trace="none")
install.packages("corrplot")
library(corrplot)
#compute the correlation matrix
cor_matrix <- cor(cardio)
#correlation plot
corrplot(cor_matrix, method="circle")
write.csv(cardio, file = "cardio.csv", row.names = FALSE)
library("readxl")
cardio <- read_excel("cardio_train.xlsx")
cardio$age <- round(cardio$age / 365)
head(cardio)
library("readxl")
cardio <- read_excel("./cardio_train.xlsx")
cardio$age <- round(cardio$age / 365)
head(cardio)
summary(cardio)
#Analyzing the excel File
print(is.data.frame(cardio))
print(ncol(cardio))
print(nrow(cardio))
#70000 obs, 13 variables
#checking for any NA value
colSums(is.na(cardio)) #No NA
# Compute the correlation matrix
cor_matrix <- cor(cardio)  # I'm using the built-in mtcars dataset as an example
# Create a heatmap
heatmap(cor_matrix, main="Correlation Heatmap", symm=TRUE, trace="none")
install.packages("corrplot")
library("readxl")
cardio <- read_excel("./cardio_train.xlsx")
cardio$age <- round(cardio$age / 365)
head(cardio)
summary(cardio)
#Analyzing the excel File
print(is.data.frame(cardio))
print(ncol(cardio))
print(nrow(cardio))
#70000 obs, 13 variables
#checking for any NA value
colSums(is.na(cardio)) #No NA
# Compute the correlation matrix
cor_matrix <- cor(cardio)  # I'm using the built-in mtcars dataset as an example
# Create a heatmap
heatmap(cor_matrix, main="Correlation Heatmap", symm=TRUE, trace="none")
library(corrplot)
#compute the correlation matrix
cor_matrix <- cor(cardio)
#correlation plot
corrplot(cor_matrix, method="circle")
write.csv(cardio, file = "cardio.csv", row.names = FALSE)
install.packages(skimr)
library(skimr)
library("readxl")
library(skimr)
NYTIMES_KEY <- "Hl58Rk7ILb2Zsy2S8nEe3EaUhCiqw4tV"
term <- "cardiovascular+risk+prediction" # Need to use + to string together separate words
begin_date <- "20000101"
end_date <- "20201231"
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
"&begin_date=",begin_date,"&end_date=",end_date,
"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
initialQuery <- fromJSON(baseurl)
library(jsonlite)
NYTIMES_KEY <- "Hl58Rk7ILb2Zsy2S8nEe3EaUhCiqw4tV"
term <- "cardiovascular+risk+prediction" # Need to use + to string together separate words
begin_date <- "20000101"
end_date <- "20201231"
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
"&begin_date=",begin_date,"&end_date=",end_date,
"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
initialQuery <- fromJSON(baseurl)
maxPages <- round((initialQuery$response$meta$hits[1] / 10)-1)
pages <- list()
for(i in 0:9){
nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame()
message("Retrieving page ", i)
pages[[i+1]] <- nytSearch
Sys.sleep(5)
}
View(nytSearch)
term <- "cardiovascular" # Need to use + to string together separate words
begin_date <- "20000101"
end_date <- "20201231"
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
"&begin_date=",begin_date,"&end_date=",end_date,
"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
initialQuery <- fromJSON(baseurl)
maxPages <- round((initialQuery$response$meta$hits[1] / 10)-1)
pages <- list()
for(i in 0:9){
nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame()
message("Retrieving page ", i)
pages[[i+1]] <- nytSearch
Sys.sleep(5)
}
View(nytSearch)
term <- "Heart+diseases" # Need to use + to string together separate words
begin_date <- "20000101"
end_date <- "20201231"
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
"&begin_date=",begin_date,"&end_date=",end_date,
"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
initialQuery <- fromJSON(baseurl)
library(jsonlite)
NYTIMES_KEY <- "Hl58Rk7ILb2Zsy2S8nEe3EaUhCiqw4tV"
term <- "Heart+diseases" # Need to use + to string together separate words
begin_date <- "20000101"
end_date <- "20201231"
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
"&begin_date=",begin_date,"&end_date=",end_date,
"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
initialQuery <- fromJSON(baseurl)
maxPages <- round((initialQuery$response$meta$hits[1] / 10)-1)
pages <- list()
for(i in 0:9){
nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame()
message("Retrieving page ", i)
pages[[i+1]] <- nytSearch
Sys.sleep(5)
}
library(jsonlite)
library(tidyverse)
NYTIMES_KEY <- "Hl58Rk7ILb2Zsy2S8nEe3EaUhCiqw4tV"
term <- "Heart+diseases" # Need to use + to string together separate words
begin_date <- "20000101"
end_date <- "20201231"
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
"&begin_date=",begin_date,"&end_date=",end_date,
"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
initialQuery <- fromJSON(baseurl)
maxPages <- round((initialQuery$response$meta$hits[1] / 10)-1)
pages <- list()
for(i in 0:9){
nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame()
message("Retrieving page ", i)
pages[[i+1]] <- nytSearch
Sys.sleep(5)
}
allNYTSearch <- rbind_pages(pages)
# Visualize coverage by section
allNYTSearch %>%
group_by(response.docs.type_of_material) %>%
summarize(count=n()) %>%
mutate(percent = (count / sum(count))*100) %>%
ggplot() +
geom_bar(aes(y=percent, x=response.docs.type_of_material, fill=response.docs.type_of_material), stat = "identity") + coord_flip() + labs(x="Sources",y="Percentage",title="Records by Sources")+theme(legend.position = "none")
allNYTSearch1 = subset(allNYTSearch, select = -c(`response.docs.multimedia`, `response.docs.keywords`,`response.docs.byline.person`))
file_path <- "../../data/00-raw-data/NewYorkTimesAPI.csv"
write.csv(allNYTSearch1, file = file_path, row.names = FALSE)
allNYTSearch <- rbind_pages(pages)
# Visualize coverage by section
allNYTSearch %>%
group_by(response.docs.type_of_material) %>%
summarize(count=n()) %>%
mutate(percent = (count / sum(count))*100) %>%
ggplot() +
geom_bar(aes(y=percent, x=response.docs.type_of_material, fill=response.docs.type_of_material), stat = "identity") + coord_flip() + labs(x="Sources",y="Percentage",title="Records by Sources")+theme(legend.position = "none")
